{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TzYm2OX4Az7X"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Triton vector add kernel\n",
        "# y = x + z\n",
        "# -----------------------------\n",
        "@triton.jit\n",
        "def vector_add_kernel(x_ptr, y_ptr, z_ptr, N, BLOCK_SIZE: tl.constexpr):\n",
        "    pid = tl.program_id(axis=0)\n",
        "\n",
        "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "\n",
        "    mask = offsets < N\n",
        "\n",
        "    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n",
        "    y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n",
        "\n",
        "    z = x + y\n",
        "\n",
        "    tl.store(z_ptr + offsets, z, mask=mask)\n",
        "\n",
        "def triton_vector_add(x: torch.Tensor, z: torch.Tensor, block: int = 1024):\n",
        "    assert x.is_cuda and z.is_cuda, \"Triton example requires CUDA tensors.\"\n",
        "    assert x.dtype == torch.float32 and z.dtype == torch.float32, \"Use float32 for this demo.\"\n",
        "    assert x.is_contiguous() and z.is_contiguous(), \"Please use contiguous tensors.\"\n",
        "\n",
        "    N = x.numel()\n",
        "    y = torch.empty_like(x)\n",
        "    grid = (triton.cdiv(N, block),)\n",
        "    vector_add_kernel[grid](x, z, y, N=N, BLOCK_SIZE=block)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Benchmark utils\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def bench_cuda(fn, iters=200, warmup=50):\n",
        "    # Warmup\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    starter = torch.cuda.Event(enable_timing=True)\n",
        "    ender = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    starter.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    ender.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    ms = starter.elapsed_time(ender) / iters\n",
        "    return ms  # per-iter milliseconds\n",
        "\n",
        "def gbps_for_vector_add(N, dtype_bytes=4, ms=1.0):\n",
        "    # vector add: read x + read z + write y => 3 * N * bytes\n",
        "    bytes_moved = 3 * N * dtype_bytes\n",
        "    seconds = ms / 1e3\n",
        "    return (bytes_moved / seconds) / 1e9  # GB/s"
      ],
      "metadata": {
        "id": "bHE3C9q_FwuK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Main\n",
        "# -----------------------------\n",
        "def main():\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA is not available. Please run on a GPU machine.\")\n",
        "        return\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    device = \"cuda\"\n",
        "    dtype = torch.float32\n",
        "\n",
        "    # Try a few sizes\n",
        "    sizes = [1_000_000, 10_000_000, 50_000_000]  # you can change\n",
        "    blocks = [256, 512, 1024]  # try different BLOCK sizes\n",
        "\n",
        "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"dtype: {dtype}\\n\")\n",
        "\n",
        "    rows = []\n",
        "    for N in sizes:\n",
        "        x = torch.randn(N, device=device, dtype=dtype)\n",
        "        z = torch.randn(N, device=device, dtype=dtype)\n",
        "\n",
        "        # PyTorch baseline\n",
        "        def torch_fn():\n",
        "            return torch.add(x, z)\n",
        "\n",
        "        torch_ms = bench_cuda(torch_fn)\n",
        "        torch_gbs = gbps_for_vector_add(N, dtype_bytes=4, ms=torch_ms)\n",
        "\n",
        "        # Triton: pick the best among blocks (simple manual \"tuning\")\n",
        "        best = None\n",
        "        for b in blocks:\n",
        "            def triton_fn():\n",
        "                return triton_vector_add(x, z, block=b)\n",
        "            ms = bench_cuda(triton_fn)\n",
        "            gbs = gbps_for_vector_add(N, dtype_bytes=4, ms=ms)\n",
        "            if (best is None) or (ms < best[\"ms\"]):\n",
        "                best = {\"block\": b, \"ms\": ms, \"gbs\": gbs}\n",
        "\n",
        "        # Correctness check (one run)\n",
        "        y_triton = triton_vector_add(x, z, block=best[\"block\"])\n",
        "        y_torch = torch.add(x, z)\n",
        "        max_err = (y_triton - y_torch).abs().max().item()\n",
        "\n",
        "        rows.append({\n",
        "            \"N\": N,\n",
        "            \"torch_ms\": torch_ms,\n",
        "            \"torch_GBs\": torch_gbs,\n",
        "            \"triton_block\": best[\"block\"],\n",
        "            \"triton_ms\": best[\"ms\"],\n",
        "            \"triton_GBs\": best[\"gbs\"],\n",
        "            \"speedup\": torch_ms / best[\"ms\"],\n",
        "            \"max_abs_err\": max_err,\n",
        "        })\n",
        "\n",
        "    # Print table\n",
        "    header = (\n",
        "        f\"{'N':>12} | {'torch (ms)':>10} | {'torch GB/s':>10} | \"\n",
        "        f\"{'triton BLOCK':>11} | {'triton (ms)':>11} | {'triton GB/s':>11} | \"\n",
        "        f\"{'speedup':>8} | {'max_err':>9}\"\n",
        "    )\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['N']:>12} | {r['torch_ms']:>10.4f} | {r['torch_GBs']:>10.2f} | \"\n",
        "            f\"{r['triton_block']:>11} | {r['triton_ms']:>11.4f} | {r['triton_GBs']:>11.2f} | \"\n",
        "            f\"{r['speedup']:>8.2f} | {r['max_abs_err']:>9.2e}\"\n",
        "        )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "zFdEi0vjGAm1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "185e8135-fd41-49a5-a3e1-6d20703ae32a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: Tesla T4\n",
            "dtype: torch.float32\n",
            "\n",
            "           N | torch (ms) | torch GB/s | triton BLOCK | triton (ms) | triton GB/s |  speedup |   max_err\n",
            "--------------------------------------------------------------------------------------------------------\n",
            "     1000000 |     0.0526 |     228.34 |         512 |      0.0521 |      230.14 |     1.01 |  0.00e+00\n",
            "    10000000 |     0.4915 |     244.15 |         256 |      0.4672 |      256.87 |     1.05 |  0.00e+00\n",
            "    50000000 |     2.4857 |     241.38 |         256 |      2.3667 |      253.51 |     1.05 |  0.00e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Benchmark helper (CUDA events)\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def bench_ms(fn, iters=200, warmup=50):\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    s = torch.cuda.Event(enable_timing=True)\n",
        "    e = torch.cuda.Event(enable_timing=True)\n",
        "    s.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    e.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return s.elapsed_time(e) / iters  # ms/iter\n",
        "\n",
        "def eff_gbs(N, ms, dtype_bytes=4):\n",
        "    # vector add: read x + read z + write y => 3 * N * bytes\n",
        "    bytes_moved = 3 * N * dtype_bytes\n",
        "    return (bytes_moved / (ms / 1e3)) / 1e9\n",
        "\n"
      ],
      "metadata": {
        "id": "sZADs48cIDe8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Run + print a small table\n",
        "# -----------------------------\n",
        "def main():\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA not available.\")\n",
        "        return\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    device = \"cuda\"\n",
        "    dtype = torch.float32\n",
        "\n",
        "    # Choose a size\n",
        "    N = 10_000_000\n",
        "    x = torch.randn(N, device=device, dtype=dtype)\n",
        "    z = torch.randn(N, device=device, dtype=dtype)\n",
        "\n",
        "    # PyTorch baseline\n",
        "    torch_fn = lambda: torch.add(x, z)\n",
        "    torch_ms = bench_ms(torch_fn)\n",
        "    torch_gbs = eff_gbs(N, torch_ms)\n",
        "\n",
        "    # Triton (try a few blocks, pick best)\n",
        "    best = None\n",
        "    for bs in [256, 512, 1024, 2048]:\n",
        "        tri_fn = lambda bs=bs: triton_vector_add(x, z, block=bs)\n",
        "        ms = bench_ms(tri_fn)\n",
        "        gbs = eff_gbs(N, ms)\n",
        "        if best is None or ms < best[\"ms\"]:\n",
        "            best = {\"bs\": bs, \"ms\": ms, \"gbs\": gbs}\n",
        "\n",
        "    # Correctness\n",
        "    y_torch = torch.add(x, z)\n",
        "    y_triton = triton_vector_add(x, z, block=best[\"bs\"])\n",
        "    max_err = (y_torch - y_triton).abs().max().item()\n",
        "\n",
        "    # Print table\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)} | dtype: {dtype} | N={N}\\n\")\n",
        "    header = f\"{'Impl':<10} | {'ms/iter':>10} | {'GB/s':>10} | {'speedup':>8} | {'note':<18}\"\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "    print(f\"{'PyTorch':<10} | {torch_ms:>10.4f} | {torch_gbs:>10.2f} | {1.00:>8.2f} | {'torch.add':<18}\")\n",
        "    print(f\"{'Triton':<10} | {best['ms']:>10.4f} | {best['gbs']:>10.2f} | {(torch_ms/best['ms']):>8.2f} | {('BLOCK='+str(best['bs'])):<18}\")\n",
        "    print(f\"\\nmax_abs_err = {max_err:.3e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObpVfcpIIdyB",
        "outputId": "2535515e-72ae-4409-ed7d-56896d12ad8e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4 | dtype: torch.float32 | N=10000000\n",
            "\n",
            "Impl       |    ms/iter |       GB/s |  speedup | note              \n",
            "--------------------------------------------------------------------\n",
            "PyTorch    |     0.4914 |     244.20 |     1.00 | torch.add         \n",
            "Triton     |     0.4630 |     259.17 |     1.06 | BLOCK=256         \n",
            "\n",
            "max_abs_err = 0.000e+00\n"
          ]
        }
      ]
    }
  ]
}