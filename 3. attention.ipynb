{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs_bXqDJWwhs"
      },
      "source": [
        "# Triton Attention Systems: Naive ‚Üí Page ‚Üí Flash\n",
        "\n",
        "üéØ **Weekly Goal**  \n",
        "Implement attention from scratch in Triton, profile performance bottlenecks,  \n",
        "understand KV cache memory layouts (PagedAttention), and build a mini FlashAttention kernel  \n",
        "to develop intuition for **IO-awareness, tiling, SRAM reuse, and kernel fusion**.\n",
        "\n",
        "---\n",
        "\n",
        "# Day 2 ‚Äî Naive Triton Attention\n",
        "\n",
        "## Objective\n",
        "\n",
        "Implement the most straightforward attention pipeline:\n",
        "\n",
        "attn = softmax(QK·µÄ) @ V\n",
        "\n",
        "Each stage must be implemented as an independent Triton kernel.  \n",
        "‚ö†Ô∏è No fusion. No tiling optimization. No IO reduction tricks.\n",
        "\n",
        "---\n",
        "\n",
        "## Tasks\n",
        "\n",
        "- [ ] Implement QK·µÄ kernel\n",
        "- [ ] Implement row-wise softmax kernel\n",
        "- [ ] Implement P @ V kernel\n",
        "- [ ] Add optional mask support (causal / padding)\n",
        "- [ ] Validate correctness vs PyTorch reference\n",
        "- [ ] Measure max / mean absolute error\n",
        "- [ ] Test small and large sequence lengths\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "- Attention compute complexity: O(n¬≤d)\n",
        "- Memory traffic complexity: O(n¬≤)\n",
        "- Materializing the attention matrix is expensive\n",
        "- Softmax requires multiple passes:\n",
        "  - max reduction\n",
        "  - exp + sum\n",
        "  - normalization\n",
        "\n",
        "---\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "- triton_naive_attention.py\n",
        "- Correctness validation script\n",
        "- Basic latency benchmark (ms)\n",
        "\n",
        "---\n",
        "\n",
        "# Day 3 ‚Äî Profiling & Bottleneck Analysis\n",
        "\n",
        "## Objective\n",
        "\n",
        "Diagnose why naive attention is slow using Nsight Compute.\n",
        "\n",
        "---\n",
        "\n",
        "## Tasks\n",
        "\n",
        "- [ ] Profile kernels with Nsight Compute\n",
        "- [ ] Collect:\n",
        "  - DRAM throughput\n",
        "  - SM efficiency\n",
        "  - Achieved occupancy\n",
        "  - Warp stall reasons\n",
        "- [ ] Identify whether bottleneck is:\n",
        "  - memory-bound\n",
        "  - reduction-bound\n",
        "  - compute-bound\n",
        "- [ ] Sweep:\n",
        "  - block sizes\n",
        "  - sequence length (512 ‚Üí 4k ‚Üí 8k)\n",
        "  - fp16 vs fp32\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "- Softmax is typically memory-bound\n",
        "- QK·µÄ behaves like GEMM (often compute-bound)\n",
        "- Writing and rereading n¬≤ matrices dominates IO\n",
        "- Arithmetic intensity determines roofline behavior\n",
        "\n",
        "---\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "### Performance Table\n",
        "\n",
        "| Impl  | ms | GB/s | TFLOPs | Speedup |\n",
        "|-------|----|------|--------|---------|\n",
        "| Torch |    |      |        | 1.0x    |\n",
        "| Naive |    |      |        |         |\n",
        "\n",
        "### Bottleneck Analysis Writeup\n",
        "\n",
        "Explain:\n",
        "\n",
        "- Why softmax is IO-heavy  \n",
        "- Why n¬≤ memory traffic dominates  \n",
        "- What stall reason dominates  \n",
        "- Whether QK·µÄ saturates compute units  \n",
        "\n",
        "---\n",
        "\n",
        "# Day 4 ‚Äî PageAttention (KV Cache Layout)\n",
        "\n",
        "## Objective\n",
        "\n",
        "Understand how vLLM reduces KV memory waste via block-based paging.\n",
        "\n",
        "---\n",
        "\n",
        "## Tasks\n",
        "\n",
        "- [ ] Study contiguous KV layout\n",
        "- [ ] Study paged KV layout\n",
        "- [ ] Design fixed-size KV blocks\n",
        "- [ ] Implement logical-to-physical block mapping\n",
        "- [ ] Write toy Triton PageAttention kernel\n",
        "- [ ] Validate correctness\n",
        "- [ ] Measure memory usage\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "- KV cache grows linearly with sequence length\n",
        "- Contiguous layout leads to fragmentation\n",
        "- Paged layout uses block tables\n",
        "- Improves memory utilization for long-context inference\n",
        "\n",
        "---\n",
        "\n",
        "## Memory Comparison\n",
        "\n",
        "| Mode        | KV Memory | Fragmentation | Best Use Case |\n",
        "|------------|------------|---------------|---------------|\n",
        "| Contiguous |            |               |               |\n",
        "| Paged      |            |               |               |\n",
        "\n",
        "---\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "- triton_page_attention.py\n",
        "- Memory usage comparison\n",
        "- Short explanation of when paging helps\n",
        "\n",
        "---\n",
        "\n",
        "# Day 5 ‚Äî FlashAttention Theory & Tiling Design\n",
        "\n",
        "## Objective\n",
        "\n",
        "Understand IO-aware attention and why FlashAttention is faster.\n",
        "\n",
        "---\n",
        "\n",
        "## Tasks\n",
        "\n",
        "- [ ] Study FlashAttention core ideas:\n",
        "  - SRAM reuse\n",
        "  - Block Q\n",
        "  - Block K\n",
        "  - Online softmax\n",
        "  - Avoid n¬≤ materialization\n",
        "- [ ] Derive why IO is reduced\n",
        "- [ ] Compute arithmetic intensity before vs after tiling\n",
        "- [ ] Design kernel parameters:\n",
        "  - BLOCK_M\n",
        "  - BLOCK_N\n",
        "  - BLOCK_D\n",
        "- [ ] Write kernel skeleton:\n",
        "  - for k_tile in K:\n",
        "  - compute qk_tile\n",
        "  - update running max\n",
        "  - update running sum\n",
        "  - accumulate output\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "- Avoid writing S (n √ó n) to DRAM\n",
        "- Online softmax enables single-pass normalization\n",
        "- FlashAttention reduces memory traffic from O(n¬≤) ‚Üí O(nd)\n",
        "- Kernel fusion increases arithmetic intensity\n",
        "\n",
        "---\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "- FlashAttention design document\n",
        "- Arithmetic intensity comparison\n",
        "- Kernel skeleton file\n",
        "\n",
        "---\n",
        "\n",
        "# Day 6 ‚Äî Triton FlashAttention (Mini Version)\n",
        "\n",
        "## Objective\n",
        "\n",
        "Implement a fused, tiled attention kernel in Triton.\n",
        "\n",
        "---\n",
        "\n",
        "## Tasks\n",
        "\n",
        "- [ ] Implement tiled QK·µÄ\n",
        "- [ ] Implement online softmax\n",
        "- [ ] Fuse V multiplication\n",
        "- [ ] Integrate into single kernel\n",
        "- [ ] Validate correctness\n",
        "- [ ] Benchmark vs naive implementation\n",
        "\n",
        "---\n",
        "\n",
        "## Final Comparison\n",
        "\n",
        "| Impl   | ms | GB/s | TFLOPs | Speedup |\n",
        "|--------|----|------|--------|---------|\n",
        "| Naive  |    |      |        | 1.0x    |\n",
        "| Flash  |    |      |        | 2.0x+   |\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "- SRAM reuse eliminates n¬≤ writes\n",
        "- Fusion reduces global memory traffic\n",
        "- FlashAttention shifts kernel toward compute-bound region\n",
        "- IO-awareness matters more than reducing FLOPs\n",
        "\n",
        "---\n",
        "\n",
        "# End-of-Week Takeaways\n",
        "\n",
        "- Attention performance is dominated by memory traffic\n",
        "- Softmax is more memory-bound than QK·µÄ\n",
        "- Kernel fusion drastically improves arithmetic intensity\n",
        "- FlashAttention works by reducing IO, not reducing math\n",
        "- Triton enables CUDA-level attention kernel design in Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fvND3OxsZRtf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c691691-41ef-4b6b-a6bf-78e885bb9cbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[check_correctness] N=128, D=64, dtype=torch.float16, use_mask=True\n",
            "  max_abs_err : 1.192093e-06\n",
            "  mean_abs_err: 6.595712e-08\n",
            "  rmse        : 1.132886e-07\n",
            "\n",
            "[quick_bench]\n",
            "N=1024, D=64, dtype=torch.float16, mask=False\n",
            "Triton: 4.320 ms\n",
            "Torch : 0.163 ms\n",
            "Speedup (Torch/Triton): 0.04x\n"
          ]
        }
      ],
      "source": [
        "# triton_naive_attention_skeleton.py\n",
        "# ============================================================\n",
        "# Day 2 ‚Äî Triton Naive Attention Kernel (NO SOLUTION)\n",
        "# Goal:\n",
        "#   Implement naive attention:\n",
        "#       attn = softmax(Q @ K.T) @ V\n",
        "#   - Separate kernels for each step (QK^T, softmax, PV)\n",
        "#   - Correctness validation vs PyTorch\n",
        "#   - Support mask (causal / padding via additive -inf)\n",
        "#   - Intentionally NOT optimized (no fusion, no FlashAttention tricks)\n",
        "#\n",
        "# Notes:\n",
        "#   - This is a skeleton with TODOs only. Fill in kernels + launcher code.\n",
        "#   - Keep correctness first; performance will be poor by design.\n",
        "# ============================================================\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Utilities\n",
        "# -----------------------------\n",
        "def _assert_cuda(x: torch.Tensor, name: str):\n",
        "    if not x.is_cuda:\n",
        "        raise ValueError(f\"{name} must be on CUDA, got {x.device}\")\n",
        "    if not x.is_contiguous():\n",
        "        raise ValueError(f\"{name} must be contiguous for this skeleton.\")\n",
        "\n",
        "\n",
        "def _make_additive_causal_mask(n: int, device, dtype):\n",
        "    \"\"\"\n",
        "    Returns additive mask M in shape [n, n]:\n",
        "      M[i, j] = 0 for j <= i\n",
        "      M[i, j] = -inf for j > i\n",
        "    Used as: scores = scores + M\n",
        "    \"\"\"\n",
        "    # TODO: implement causal mask creation\n",
        "    # raise NotImplementedError\n",
        "    if device is None:\n",
        "        device = \"cpu\"\n",
        "    if dtype is None:\n",
        "        dtype = torch.float32\n",
        "\n",
        "    # Use the minimum finite value for the dtype to represent \"-inf\" in practice.\n",
        "    # For fp16/bf16, true -inf exists, but using finfo.min is also common and safe.\n",
        "    # neg_inf = torch.finfo(dtype).min\n",
        "\n",
        "    # upper triangular (strictly above diagonal) => future positions\n",
        "    # shape [n, n], True where j > i\n",
        "    future = torch.triu(torch.ones((n, n), device=device, dtype=torch.bool), diagonal=1)\n",
        "\n",
        "    # start from zeros, fill future with neg_inf\n",
        "    mask = torch.zeros((n, n), device=device, dtype=dtype)\n",
        "    mask = mask.masked_fill(future, -float(\"inf\"))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def _make_additive_padding_mask(valid_lens: torch.Tensor, n: int, device, dtype):\n",
        "    \"\"\"\n",
        "    valid_lens: [B] or [n] style lengths; for this Day2 skeleton we keep it simple:\n",
        "      - Assume a single sequence length n, and valid_lens is optional.\n",
        "      - If you want per-row masking, expand to [n, n] additive mask.\n",
        "      scores = scores + padding_mask\n",
        "    \"\"\"\n",
        "    # TODO: implement padding mask (optional)\n",
        "    # raise NotImplementedError\n",
        "    if device is None:\n",
        "        device = \"cpu\"\n",
        "    if dtype is None:\n",
        "        dtype = torch.float32\n",
        "\n",
        "    neg_inf = torch.finfo(dtype).min\n",
        "\n",
        "    # assume single length\n",
        "    if valid_lens is None:\n",
        "        return torch.zeros((n, n), device=device, dtype=dtype)\n",
        "\n",
        "    L = int(valid_lens.item())\n",
        "\n",
        "    # shape [n]\n",
        "    key_positions = torch.arange(n, device=device)\n",
        "\n",
        "    # True where j >= L\n",
        "    invalid = key_positions >= L\n",
        "\n",
        "    # expand to [n, n] (each row same mask)\n",
        "    invalid = invalid.unsqueeze(0).expand(n, n)\n",
        "\n",
        "    mask = torch.zeros((n, n), device=device, dtype=dtype)\n",
        "    mask = mask.masked_fill(invalid, neg_inf)\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Kernel 1: Scores = Q @ K^T\n",
        "# Q: [N, D], K: [N, D]  => Scores: [N, N]\n",
        "# ============================================================\n",
        "@triton.jit\n",
        "def qk_t_kernel(\n",
        "    q_ptr, k_ptr, s_ptr,\n",
        "    N: tl.constexpr, D: tl.constexpr,\n",
        "    stride_qn: tl.constexpr, stride_qd: tl.constexpr,\n",
        "    stride_kn: tl.constexpr, stride_kd: tl.constexpr,\n",
        "    stride_sn: tl.constexpr, stride_sm: tl.constexpr,\n",
        "    # Tile sizes (intentionally simple / naive)\n",
        "    BLOCK_M: tl.constexpr,\n",
        "    BLOCK_N: tl.constexpr,\n",
        "    BLOCK_K: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute a tile of S = QK^T.\n",
        "    Program ids map over (rows, cols) tiles of S.\n",
        "\n",
        "    TODO:\n",
        "    - Compute pid_m, pid_n\n",
        "    - Compute row/col offsets\n",
        "    - Load Q tile [BLOCK_M, BLOCK_K]\n",
        "    - Load K tile [BLOCK_N, BLOCK_K] (note K^T => K rows act like cols)\n",
        "    - Accumulate dot products\n",
        "    - Store to S\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    tl.static_assert(BLOCK_K <= D,\n",
        "    \"BLOCK_K must be <= D for this skeleton\")\n",
        "    # raise NotImplementedError\n",
        "\n",
        "    # 2D program grid: each program handles a (BLOCK_M x BLOCK_N) tile of X\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "\n",
        "    m_mask = offs_m < N\n",
        "    n_mask = offs_n < N\n",
        "\n",
        "\n",
        "    for k0 in range(0, D, BLOCK_K):\n",
        "        d_offsets = k0 + offs_k\n",
        "        d_mask = d_offsets < D\n",
        "\n",
        "        q_ptrs = q_ptr + offs_m[:, None] * stride_qn + d_offsets[None, :] * stride_qd   # [BM, BK]\n",
        "        k_ptrs = k_ptr + offs_n[:, None] * stride_kn + d_offsets[None, :] * stride_kd  # [BN,BK]\n",
        "\n",
        "        # 2D masks for loads\n",
        "        q_load_mask = m_mask[:, None] & d_mask[None, :]    # [BM, BK]\n",
        "        k_load_mask = n_mask[:, None] & d_mask[None, :]    # [BK, BN]\n",
        "\n",
        "\n",
        "        q_tile = tl.load(q_ptrs, mask = q_load_mask, other = 0.0)\n",
        "        k_tile = tl.load(k_ptrs, mask = k_load_mask, other = 0.0)\n",
        "\n",
        "        acc += tl.dot(q_tile, tl.trans(k_tile))\n",
        "\n",
        "    s_ptrs = s_ptr + offs_m[:, None] * stride_sn + offs_n[None, :] * stride_sm\n",
        "    tl.store(s_ptrs, acc, mask = m_mask[:, None] & n_mask[None, :])\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Kernel 2: Softmax over each row of S (row-wise)\n",
        "# S: [N, N] -> P: [N, N]\n",
        "# Optional additive mask: M: [N, N] where invalid positions are -inf\n",
        "# ============================================================\n",
        "@triton.jit\n",
        "def softmax_row_kernel(\n",
        "    s_ptr, m_ptr, p_ptr,\n",
        "    N: tl.constexpr,\n",
        "    stride_sn: tl.constexpr, stride_sm: tl.constexpr,\n",
        "    stride_mn: tl.constexpr, stride_mm: tl.constexpr,\n",
        "    stride_pn: tl.constexpr, stride_pm: tl.constexpr,\n",
        "    BLOCK_N: tl.constexpr,\n",
        "    HAS_MASK: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Row-wise softmax:\n",
        "      p[i, :] = softmax(s[i, :] + mask[i, :])\n",
        "      p = s + m\n",
        "\n",
        "    TODO:\n",
        "    - Map program id to a row i\n",
        "    - Load a row block of scores\n",
        "    - If HAS_MASK, load mask and add\n",
        "    - Numerically stable softmax:\n",
        "        x = x - max(x)\n",
        "        exp = tl.exp(x)\n",
        "        denom = tl.sum(exp)\n",
        "        p = exp / denom\n",
        "    - Store p\n",
        "\n",
        "    Notes:\n",
        "    - This skeleton assumes N can be larger than BLOCK_N; you may loop over blocks\n",
        "      or restrict this Day2 to N <= BLOCK_N initially.\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    # raise NotImplementedError\n",
        "    # if N > BLOCK_N:\n",
        "    #     raise ValueError(\"Naive row-softmax requires N <= BLOCK_N\")\n",
        "    pid_row = tl.program_id(0)\n",
        "    offs = tl.arange(0, BLOCK_N)\n",
        "\n",
        "    s_row_ptr = s_ptr + pid_row * stride_sn + offs * stride_sm\n",
        "    p_row_ptr = p_ptr + pid_row * stride_pn + offs * stride_pm\n",
        "\n",
        "    mask = offs < N\n",
        "    s_row = tl.load(s_row_ptr, mask = mask, other = -float(\"inf\")).to(tl.float32)\n",
        "    if HAS_MASK:\n",
        "        m_row_ptr = m_ptr + pid_row * stride_mn + offs * stride_mm\n",
        "        m_row = tl.load(m_row_ptr, mask = mask, other = 0.0).to(tl.float32)\n",
        "        s_row = s_row + m_row\n",
        "\n",
        "    # stable softmax\n",
        "    s_max = tl.max(s_row, axis = 0)\n",
        "    s_row = s_row - s_max\n",
        "    s_exp = tl.exp(s_row)\n",
        "    s_sum = tl.sum(s_exp, axis=0)\n",
        "    p_row = s_exp / s_sum\n",
        "\n",
        "    tl.store(p_row_ptr, p_row, mask = mask)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Kernel 3: Out = P @ V\n",
        "# P: [N, N], V: [N, D] -> O: [N, D]\n",
        "# ============================================================\n",
        "@triton.jit\n",
        "def pv_kernel(\n",
        "    p_ptr, v_ptr, o_ptr,\n",
        "    N: tl.constexpr, D: tl.constexpr,\n",
        "    stride_pn: tl.constexpr, stride_pm: tl.constexpr,\n",
        "    stride_vn: tl.constexpr, stride_vd: tl.constexpr,\n",
        "    stride_on: tl.constexpr, stride_od: tl.constexpr,\n",
        "    BLOCK_M: tl.constexpr,\n",
        "    BLOCK_N: tl.constexpr,\n",
        "    BLOCK_K: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute a tile of O = P V.\n",
        "\n",
        "    TODO:\n",
        "    - Program ids over (rows of O, cols of O)\n",
        "    - Load P tile [BLOCK_M, BLOCK_K]\n",
        "    - Load V tile [BLOCK_K, BLOCK_N] (here K dimension is N of P / V rows)\n",
        "    - Accumulate\n",
        "    - Store to O\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    # raise NotImplementedError\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "    m_mask = offs_m < N\n",
        "    n_mask = offs_n < D\n",
        "\n",
        "    for k0 in tl.static_range(0, N, BLOCK_K):\n",
        "          # current K indices for this chunk\n",
        "          k_offsets = k0 + offs_k\n",
        "          k_mask = k_offsets < N\n",
        "\n",
        "          # build pointer grids for this chunk\n",
        "          p_ptrs = p_ptr + offs_m[:, None] * stride_pn + k_offsets[None, :] * stride_pm\n",
        "          v_ptrs = v_ptr + k_offsets[:, None] * stride_vn + offs_n[None, :] * stride_vd\n",
        "\n",
        "          # 2D masks for loads\n",
        "          p_load_mask = m_mask[:, None] & k_mask[None, :]\n",
        "          v_load_mask = k_mask[:, None] & n_mask[None, :]\n",
        "\n",
        "          # masked loads: out-of-bounds => 0\n",
        "          p_tile = tl.load(p_ptrs, mask=p_load_mask, other=0).to(tl.float32)\n",
        "          v_tile = tl.load(v_ptrs, mask=v_load_mask, other=0).to(tl.float32)\n",
        "\n",
        "          # accumulate (fp32)\n",
        "          # tl.dot will typically accumulate in fp32 when acc is fp32\n",
        "          acc += tl.dot(p_tile, v_tile)\n",
        "\n",
        "    o_tile = acc\n",
        "    o_ptrs = o_ptr + offs_m[:, None] * stride_on + offs_n[None, :] * stride_od\n",
        "    tl.store(o_ptrs, o_tile, mask=m_mask[:,None] & n_mask[None,:])\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Launchers (NO SOLUTION)\n",
        "# ============================================================\n",
        "def qk_t_triton(Q: torch.Tensor, K: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute S = Q @ K^T\n",
        "    Q, K: [N, D] contiguous CUDA tensors\n",
        "    Returns:\n",
        "      S: [N, N]\n",
        "    \"\"\"\n",
        "    _assert_cuda(Q, \"Q\")\n",
        "    _assert_cuda(K, \"K\")\n",
        "    assert Q.shape == K.shape\n",
        "    N, D = Q.shape\n",
        "\n",
        "    S = torch.empty((N, N), device=Q.device, dtype=torch.float32)  # scores typically fp32\n",
        "\n",
        "    # TODO:\n",
        "    # - Choose BLOCK_M/BLOCK_N/BLOCK_K (naive defaults)\n",
        "    # - Define grid mapping over tiles\n",
        "    # - Call qk_t_kernel[grid](...)\n",
        "    # raise NotImplementedError\n",
        "    BLOCK_M = 128\n",
        "    BLOCK_N = 128\n",
        "    BLOCK_K = 64\n",
        "    # if N > BLOCK_N:\n",
        "    #     raise ValueError(\"Naive row-softmax requires N <= BLOCK_N\")\n",
        "\n",
        "    N, D = Q.shape\n",
        "    N2, D2 = K.shape\n",
        "    assert N == N2, \"Q and K must have the same sequence length\"\n",
        "    assert D == D2, \"Q and K must have the same head dimension\"\n",
        "\n",
        "    grid = (\n",
        "        triton.cdiv(N, BLOCK_M),\n",
        "        triton.cdiv(N, BLOCK_N),\n",
        "    )\n",
        "\n",
        "    qk_t_kernel[grid](\n",
        "        Q, K, S,\n",
        "        N = N, D = D,\n",
        "        stride_qn=Q.stride(0), stride_qd=Q.stride(1),\n",
        "        stride_kn=K.stride(0), stride_kd=K.stride(1),\n",
        "        stride_sn=S.stride(0), stride_sm=S.stride(1),\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K)\n",
        "\n",
        "    return S\n",
        "\n",
        "\n",
        "def softmax_triton(S: torch.Tensor, mask: torch.Tensor | None) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute P = softmax(S + mask) row-wise.\n",
        "    S: [N, N]\n",
        "    mask: [N, N] additive mask (0 or -inf). If None, no mask.\n",
        "    Returns:\n",
        "      P: [N, N] (same dtype as S or fp16/fp32 choice)\n",
        "    \"\"\"\n",
        "    _assert_cuda(S, \"S\")\n",
        "    N, N2 = S.shape\n",
        "    assert N == N2\n",
        "\n",
        "    if mask is not None:\n",
        "        _assert_cuda(mask, \"mask\")\n",
        "        assert mask.shape == (N, N)\n",
        "\n",
        "    P = torch.empty_like(S)\n",
        "\n",
        "    # TODO:\n",
        "    # - Choose BLOCK_N\n",
        "    # - grid = (N,) one program per row (or per row-block)\n",
        "    # - HAS_MASK constexpr\n",
        "    # - Call softmax_row_kernel[grid](...)\n",
        "    # raise NotImplementedError\n",
        "    if N <= 128: BLOCK_N=128\n",
        "    elif N <= 256: BLOCK_N=256\n",
        "    elif N <= 512: BLOCK_N=512\n",
        "    elif N <= 1024: BLOCK_N=1024\n",
        "    else:\n",
        "        raise ValueError(f\"Naive row-softmax requires N <= 1024, got N={N}\")\n",
        "    grid = (N,)\n",
        "\n",
        "    HAS_MASK = mask is not None\n",
        "    m_ptr = mask if HAS_MASK else S\n",
        "    stride_mn = mask.stride(0) if HAS_MASK else 0\n",
        "    stride_mm = mask.stride(1) if HAS_MASK else 0\n",
        "\n",
        "\n",
        "    softmax_row_kernel[grid](\n",
        "    S, m_ptr, P,\n",
        "    N,\n",
        "    S.stride(0), S.stride(1),\n",
        "    stride_mn, stride_mm,\n",
        "    P.stride(0), P.stride(1),\n",
        "    BLOCK_N=BLOCK_N,\n",
        "    HAS_MASK=HAS_MASK,\n",
        ")\n",
        "\n",
        "\n",
        "    return P\n",
        "\n",
        "\n",
        "def pv_triton(P: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute O = P @ V\n",
        "    P: [N, N]\n",
        "    V: [N, D]\n",
        "    Returns:\n",
        "      O: [N, D]\n",
        "    \"\"\"\n",
        "    _assert_cuda(P, \"P\")\n",
        "    _assert_cuda(V, \"V\")\n",
        "    N, N2 = P.shape\n",
        "    assert N == N2\n",
        "    assert V.shape[0] == N\n",
        "    D = V.shape[1]\n",
        "\n",
        "    O = torch.empty((N, D), device=V.device, dtype=torch.float32)\n",
        "\n",
        "    # TODO:\n",
        "    # - Choose BLOCK_M/BLOCK_N/BLOCK_K\n",
        "    # - Define grid over O tiles\n",
        "    # - Call pv_kernel[grid](...)\n",
        "    # raise NotImplementedError\n",
        "    BLOCK_M = 128\n",
        "    BLOCK_N = 128\n",
        "    BLOCK_K = 64\n",
        "\n",
        "    grid = (\n",
        "        triton.cdiv(N, BLOCK_M),  # pid_m\n",
        "        triton.cdiv(D, BLOCK_N),  # pid_n\n",
        "    )\n",
        "\n",
        "    pv_kernel[grid](\n",
        "        P, V, O,\n",
        "        N=N, D=D,\n",
        "        stride_pn=P.stride(0), stride_pm=P.stride(1),\n",
        "        stride_vn=V.stride(0), stride_vd=V.stride(1),\n",
        "        stride_on=O.stride(0), stride_od=O.stride(1),\n",
        "        BLOCK_M=BLOCK_M,\n",
        "        BLOCK_N=BLOCK_N,\n",
        "        BLOCK_K=BLOCK_K)\n",
        "\n",
        "    return O\n",
        "\n",
        "def naive_attention_triton(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask: torch.Tensor | None = None):\n",
        "    \"\"\"\n",
        "    Full naive attention:\n",
        "      S = QK^T\n",
        "      P = softmax(S + mask)\n",
        "      O = P V\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # - Call qk_t_triton\n",
        "    # - Call softmax_triton\n",
        "    # - Call pv_triton\n",
        "    # raise NotImplementedError\n",
        "    _assert_cuda(Q, \"Q\")\n",
        "    _assert_cuda(K, \"K\")\n",
        "    _assert_cuda(V, \"V\")\n",
        "    assert Q.shape == K.shape, \"Q and K must have shape [N, D]\"\n",
        "    assert Q.shape == V.shape, \"For this toy naive version, assume V has shape [N, D]\"\n",
        "    N, D = Q.shape\n",
        "\n",
        "    if mask is not None:\n",
        "        _assert_cuda(mask, \"mask\")\n",
        "        assert mask.shape == (N, N), \"mask must be [N, N] additive mask (0 / -inf)\"\n",
        "\n",
        "    # 1) Scores: S = Q @ K^T   -> [N, N] (often fp32)\n",
        "    S = qk_t_triton(Q, K)\n",
        "\n",
        "    # 2) Probabilities: P = softmax(S + mask)  -> [N, N]\n",
        "    P = softmax_triton(S, mask)\n",
        "\n",
        "    # 3) Output: O = P @ V     -> [N, D]\n",
        "    O = pv_triton(P, V)\n",
        "\n",
        "    return O\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PyTorch reference & correctness checks (NO SOLUTION)\n",
        "# ============================================================\n",
        "def naive_attention_torch(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask: torch.Tensor | None = None):\n",
        "    \"\"\"\n",
        "    Reference implementation in PyTorch:\n",
        "      attn = softmax(Q @ K.T + mask) @ V\n",
        "    \"\"\"\n",
        "    # TODO: implement torch reference (use float32 accumulation if needed)\n",
        "    # raise NotImplementedError\n",
        "    assert Q.shape == K.shape == V.shape, \"This toy reference assumes Q,K,V are all [N, D]\"\n",
        "    N, D = Q.shape\n",
        "\n",
        "    # Use fp32 for scores/softmax stability, regardless of input dtype\n",
        "    Qf = Q.to(torch.float32)\n",
        "    Kf = K.to(torch.float32)\n",
        "    Vf = V.to(torch.float32)\n",
        "\n",
        "    scores = Qf @ Kf.transpose(0, 1)  # [N, N]\n",
        "\n",
        "    if mask is not None:\n",
        "        assert mask.shape == (N, N), f\"mask must be [N, N], got {mask.shape}\"\n",
        "        scores = scores + mask.to(torch.float32)\n",
        "\n",
        "    P = torch.softmax(scores, dim=-1)  # row-wise softmax\n",
        "    O = P @ Vf  # [N, D]\n",
        "\n",
        "    return O\n",
        "\n",
        "@torch.no_grad()\n",
        "def check_correctness(device=\"cuda\", dtype=torch.float16, N=256, D=64, use_mask=True):\n",
        "    torch.manual_seed(0)\n",
        "    Q = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    K = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    V = torch.randn((N, D), device=device, dtype=dtype)\n",
        "\n",
        "    mask = None\n",
        "    if use_mask:\n",
        "        # TODO: create a causal mask (or padding mask)\n",
        "        mask = _make_additive_causal_mask(N, device=device, dtype=torch.float32)\n",
        "\n",
        "    # TODO:\n",
        "    # - Run torch reference\n",
        "    # - Run triton naive attention\n",
        "    # - Compare max/mean error\n",
        "    # raise NotImplementedError\n",
        "    # Reference (PyTorch)\n",
        "    out_ref = naive_attention_torch(Q, K, V, mask=mask)\n",
        "\n",
        "    # Triton naive\n",
        "    out_tri = naive_attention_triton(Q, K, V, mask=mask)\n",
        "\n",
        "    # Compare (cast both to fp32 for fair error)\n",
        "    diff = (out_tri.to(torch.float32) - out_ref.to(torch.float32)).abs()\n",
        "    max_err = diff.max().item()\n",
        "    mean_err = diff.mean().item()\n",
        "    rmse = torch.sqrt((diff * diff).mean()).item()\n",
        "\n",
        "    print(f\"[check_correctness] N={N}, D={D}, dtype={dtype}, use_mask={use_mask}\")\n",
        "    print(f\"  max_abs_err : {max_err:.6e}\")\n",
        "    print(f\"  mean_abs_err: {mean_err:.6e}\")\n",
        "    print(f\"  rmse        : {rmse:.6e}\")\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def quick_bench(device=\"cuda\", dtype=torch.float16, N=1024, D=64, iters=50, warmup=10, use_mask=False):\n",
        "    \"\"\"\n",
        "    Simple benchmark harness (intentionally minimal).\n",
        "    \"\"\"\n",
        "    Q = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    K = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    V = torch.randn((N, D), device=device, dtype=dtype)\n",
        "\n",
        "    mask = None\n",
        "    if use_mask:\n",
        "        # TODO: create mask\n",
        "        mask = _make_additive_causal_mask(N, device=device, dtype=torch.float32)\n",
        "\n",
        "    # TODO:\n",
        "    # - Warmup runs\n",
        "    # - Time with CUDA events\n",
        "    # - Print ms/iter for torch vs triton\n",
        "    # raise NotImplementedError\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    Q = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    K = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    V = torch.randn((N, D), device=device, dtype=dtype)\n",
        "\n",
        "    mask = None\n",
        "    if use_mask:\n",
        "        # Additive causal mask: 0 or -inf\n",
        "        mask = _make_additive_causal_mask(N, device=device, dtype=torch.float32)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Warmup\n",
        "    # ----------------------------\n",
        "    for _ in range(warmup):\n",
        "        naive_attention_triton(Q, K, V, mask=mask)\n",
        "        naive_attention_torch(Q, K, V, mask=mask)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # ----------------------------\n",
        "    # Benchmark Triton\n",
        "    # ----------------------------\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    start.record()\n",
        "    for _ in range(iters):\n",
        "        naive_attention_triton(Q, K, V, mask=mask)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    triton_ms = start.elapsed_time(end) / iters\n",
        "\n",
        "    # ----------------------------\n",
        "    # Benchmark Torch\n",
        "    # ----------------------------\n",
        "    start.record()\n",
        "    for _ in range(iters):\n",
        "        naive_attention_torch(Q, K, V, mask=mask)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    torch_ms = start.elapsed_time(end) / iters\n",
        "\n",
        "    speedup = torch_ms / triton_ms\n",
        "\n",
        "    print(f\"\\n[quick_bench]\")\n",
        "    print(f\"N={N}, D={D}, dtype={dtype}, mask={use_mask}\")\n",
        "    print(f\"Triton: {triton_ms:.3f} ms\")\n",
        "    print(f\"Torch : {torch_ms:.3f} ms\")\n",
        "    print(f\"Speedup (Torch/Triton): {speedup:.2f}x\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: run correctness + small bench\n",
        "    check_correctness(N=128, D=64, use_mask=True)\n",
        "    quick_bench(N=1024, D=64, use_mask=False)\n",
        "    # pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCKUF3QM8pml"
      },
      "outputs": [],
      "source": [
        "# day3_profile_bottleneck_skeleton.py\n",
        "# ============================================================\n",
        "# Day 3 ‚Äî Profiling + Bottleneck Analysis (NO SOLUTION)\n",
        "#\n",
        "# Goal:\n",
        "#   Profile naive attention vs torch attention, identify bottlenecks.\n",
        "#\n",
        "# Tasks:\n",
        "#   - Nsight Compute metrics:\n",
        "#       * DRAM throughput\n",
        "#       * SM efficiency\n",
        "#       * Stall reasons\n",
        "#   - Decide bottleneck:\n",
        "#       * memory-bound?\n",
        "#       * reduction-bound?\n",
        "#   - Sweep:\n",
        "#       * different block sizes\n",
        "#       * different sequence lengths\n",
        "#\n",
        "# Outputs:\n",
        "#   - Markdown table comparing naive vs torch (printed)\n",
        "#   - Bottleneck analysis template (printed)\n",
        "#\n",
        "# Notes:\n",
        "#   - Plug in your Day2 implementations:\n",
        "#       naive_attention_triton(Q,K,V,mask,cfg)\n",
        "#       naive_attention_torch(Q,K,V,mask)\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import json\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, Dict, Any, List, Tuple\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TODO: import your Day2 implementations\n",
        "# ============================================================\n",
        "def naive_attention_triton(Q, K, V, mask=None, cfg=None):\n",
        "    # TODO: call your Triton naive attention implementation\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def naive_attention_torch(Q, K, V, mask=None):\n",
        "    # TODO: call your PyTorch reference implementation\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Config definition for block size sweep\n",
        "# ============================================================\n",
        "@dataclass(frozen=True)\n",
        "class TritonNaiveCfg:\n",
        "    BLOCK_M: int\n",
        "    BLOCK_N: int\n",
        "    BLOCK_K: int\n",
        "    SOFTMAX_BLOCK: int\n",
        "    num_warps: int = 4\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Benchmark utilities\n",
        "# ============================================================\n",
        "def cuda_time_ms(fn, iters=30, warmup=10) -> float:\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    start.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    return start.elapsed_time(end) / iters\n",
        "\n",
        "\n",
        "def gflops_qk_pv(N: int, D: int) -> float:\n",
        "    flops = 4.0 * N * N * D\n",
        "    return flops / 1e9\n",
        "\n",
        "\n",
        "def estimate_bytes(N: int, D: int, elem_bytes: int = 2) -> int:\n",
        "    # TODO: refine if using fp32 intermediate\n",
        "    qkv = 3 * N * D * elem_bytes\n",
        "    s_mat = 2 * N * N * elem_bytes\n",
        "    p_mat = 2 * N * N * elem_bytes\n",
        "    out = N * D * elem_bytes\n",
        "    return qkv + s_mat + p_mat + out\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Experiment execution\n",
        "# ============================================================\n",
        "@dataclass\n",
        "class ResultRow:\n",
        "    impl: str\n",
        "    N: int\n",
        "    D: int\n",
        "    cfg: Optional[Dict[str, Any]]\n",
        "    ms: float\n",
        "    gflops: float\n",
        "    est_gbs: float\n",
        "\n",
        "\n",
        "def run_case(N: int, D: int, cfg: TritonNaiveCfg):\n",
        "    device = \"cuda\"\n",
        "    dtype = torch.float16\n",
        "\n",
        "    Q = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    K = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    V = torch.randn((N, D), device=device, dtype=dtype)\n",
        "\n",
        "    # --- Torch baseline ---\n",
        "    def fn_torch():\n",
        "        return naive_attention_torch(Q, K, V, mask=None)\n",
        "\n",
        "    ms_torch = cuda_time_ms(fn_torch)\n",
        "    gflops = gflops_qk_pv(N, D)\n",
        "    bytes_est = estimate_bytes(N, D)\n",
        "\n",
        "    torch_row = ResultRow(\n",
        "        impl=\"torch\",\n",
        "        N=N,\n",
        "        D=D,\n",
        "        cfg=None,\n",
        "        ms=ms_torch,\n",
        "        gflops=gflops / (ms_torch / 1e3),\n",
        "        est_gbs=(bytes_est / (ms_torch / 1e3)) / 1e9,\n",
        "    )\n",
        "\n",
        "    # --- Triton naive ---\n",
        "    def fn_triton():\n",
        "        return naive_attention_triton(Q, K, V, mask=None, cfg=cfg)\n",
        "\n",
        "    ms_triton = cuda_time_ms(fn_triton)\n",
        "\n",
        "    triton_row = ResultRow(\n",
        "        impl=\"naive_triton\",\n",
        "        N=N,\n",
        "        D=D,\n",
        "        cfg=asdict(cfg),\n",
        "        ms=ms_triton,\n",
        "        gflops=gflops / (ms_triton / 1e3),\n",
        "        est_gbs=(bytes_est / (ms_triton / 1e3)) / 1e9,\n",
        "    )\n",
        "\n",
        "    return torch_row, triton_row\n",
        "\n",
        "\n",
        "def sweep(seq_lens: List[int], D: int, cfgs: List[TritonNaiveCfg]):\n",
        "    rows: List[ResultRow] = []\n",
        "    for N in seq_lens:\n",
        "        for cfg in cfgs:\n",
        "            torch_row, triton_row = run_case(N, D, cfg)\n",
        "            rows.append(torch_row)\n",
        "            rows.append(triton_row)\n",
        "    return rows\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Output formatting\n",
        "# ============================================================\n",
        "def print_markdown_table(rows: List[ResultRow]):\n",
        "    print(\"\\n# Day 3 Results (Naive Triton vs Torch)\\n\")\n",
        "    print(\"| impl | N | D | cfg | ms | GFLOP/s | est_GB/s | speedup_vs_torch |\")\n",
        "    print(\"|------|---|---|-----|----|---------|----------|------------------|\")\n",
        "\n",
        "    torch_map = {(r.N, r.D): r.ms for r in rows if r.impl == \"torch\"}\n",
        "\n",
        "    for r in rows:\n",
        "        base = torch_map.get((r.N, r.D), None)\n",
        "        speedup = base / r.ms if (base and r.impl != \"torch\") else 1.0\n",
        "\n",
        "        cfg_str = \"-\"\n",
        "        if r.cfg:\n",
        "            cfg_str = f\"BM={r.cfg['BLOCK_M']},BN={r.cfg['BLOCK_N']},BK={r.cfg['BLOCK_K']},SB={r.cfg['SOFTMAX_BLOCK']},w={r.cfg['num_warps']}\"\n",
        "\n",
        "        print(f\"| {r.impl} | {r.N} | {r.D} | {cfg_str} | \"\n",
        "              f\"{r.ms:.4f} | {r.gflops:.2f} | {r.est_gbs:.2f} | \"\n",
        "              f\"{speedup:.2f}x |\")\n",
        "\n",
        "\n",
        "def print_bottleneck_template():\n",
        "    print(\"\\n\\n# Bottleneck Analysis (Fill After Nsight Compute)\\n\")\n",
        "    print(\"## Nsight Compute Observations\")\n",
        "    print(\"- DRAM throughput (% peak): TODO\")\n",
        "    print(\"- SM throughput (% peak): TODO\")\n",
        "    print(\"- Dominant stall reasons:\")\n",
        "    print(\"  - long scoreboard: TODO\")\n",
        "    print(\"  - memory dependency: TODO\")\n",
        "    print(\"  - barrier: TODO\")\n",
        "    print(\"  - math pipe throttle: TODO\\n\")\n",
        "\n",
        "    print(\"## Bottleneck Classification\")\n",
        "    print(\"- [ ] Memory-bound\")\n",
        "    print(\"- [ ] Reduction-bound\")\n",
        "    print(\"- [ ] Compute-bound\\n\")\n",
        "\n",
        "    print(\"## Interpretation\")\n",
        "    print(\"- Naive attention materializes N√óN matrices.\")\n",
        "    print(\"- Softmax requires multiple passes (max/sum/normalize).\")\n",
        "    print(\"- Heavy DRAM traffic likely dominates performance.\\n\")\n",
        "\n",
        "    print(\"## Next Steps\")\n",
        "    print(\"- Tune block sizes.\")\n",
        "    print(\"- Increase arithmetic intensity.\")\n",
        "    print(\"- Consider fusion (FlashAttention).\\n\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Main\n",
        "# ============================================================\n",
        "def main():\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"CUDA required.\")\n",
        "\n",
        "    seq_lens = [256, 512, 1024]  # TODO: extend if desired\n",
        "    D = 64\n",
        "\n",
        "    cfgs = [\n",
        "        TritonNaiveCfg(64, 64, 32, 256, 4),\n",
        "        TritonNaiveCfg(128, 64, 32, 256, 4),\n",
        "        TritonNaiveCfg(64, 128, 32, 512, 8),\n",
        "    ]\n",
        "\n",
        "    rows = sweep(seq_lens, D, cfgs)\n",
        "\n",
        "    print_markdown_table(rows)\n",
        "    print_bottleneck_template()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmawQOy28pml"
      },
      "outputs": [],
      "source": [
        "# day4_paged_attention_toy_skeleton.py\n",
        "# ============================================================\n",
        "# Day 4 ‚Äî PageAttention (Toy) (NO SOLUTION)\n",
        "#\n",
        "# Goal:\n",
        "#   Understand vLLM-style KV cache paging by building a toy PageAttention in Triton.\n",
        "#\n",
        "# What you'll implement (toy scope):\n",
        "#   - Two KV cache modes:\n",
        "#       (1) Contiguous KV: K,V stored as [T, D] for each sequence (single sequence toy)\n",
        "#       (2) Paged KV: K,V stored in fixed-size blocks; a block table maps logical blocks to physical blocks\n",
        "#   - A toy attention computation that reads K,V via the selected layout:\n",
        "#       out = softmax(Q @ K^T + mask) @ V\n",
        "#\n",
        "# Tasks:\n",
        "#   - Study: contiguous KV vs paged KV\n",
        "#   - Design: KV block layout + block table\n",
        "#   - Implement: Triton PageAttention (toy)\n",
        "#   - Validate correctness vs torch reference\n",
        "#   - Memory usage stats (allocated bytes, fragmentation estimate)\n",
        "#\n",
        "# Notes:\n",
        "#   - This is a skeleton with TODOs only.\n",
        "#   - Keep it SIMPLE: single-head, single sequence, fp16 inputs, fp32 accum.\n",
        "#   - You can extend later to multi-head/batch.\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import math\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Data structures\n",
        "# ============================================================\n",
        "@dataclass(frozen=True)\n",
        "class PageCfg:\n",
        "    # page/block size in tokens\n",
        "    BLOCK_T: int\n",
        "    # head dim\n",
        "    D: int\n",
        "    # number of physical blocks allocated in the KV pool\n",
        "    NUM_PHYS_BLOCKS: int\n",
        "\n",
        "    # toy kernel tiling knobs (optional)\n",
        "    BLOCK_M: int = 64      # query rows (here usually 1 query, but keep generic)\n",
        "    BLOCK_N: int = 128     # keys columns tile\n",
        "    num_warps: int = 4\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class MemStats:\n",
        "    mode: str\n",
        "    logical_T: int\n",
        "    D: int\n",
        "    block_T: int\n",
        "    num_logical_blocks: int\n",
        "    num_phys_blocks: int\n",
        "    kv_bytes_allocated: int\n",
        "    kv_bytes_used: int\n",
        "    fragmentation_bytes: int\n",
        "    fragmentation_ratio: float\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Helper: build additive causal mask (optional)\n",
        "# ============================================================\n",
        "def make_additive_causal_mask(T: int, device, dtype=torch.float32) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Returns [T, T] additive causal mask:\n",
        "      0 for j <= i\n",
        "      -inf for j > i\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Contiguous KV layout (toy)\n",
        "# ============================================================\n",
        "def alloc_contiguous_kv(T: int, D: int, device=\"cuda\", dtype=torch.float16) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Allocate contiguous K,V as [T, D]\n",
        "    \"\"\"\n",
        "    K = torch.empty((T, D), device=device, dtype=dtype)\n",
        "    V = torch.empty((T, D), device=device, dtype=dtype)\n",
        "    return K, V\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Paged KV layout (toy)\n",
        "# ============================================================\n",
        "def alloc_paged_kv_pool(num_phys_blocks: int, block_T: int, D: int, device=\"cuda\", dtype=torch.float16):\n",
        "    \"\"\"\n",
        "    Allocate a KV pool with fixed blocks:\n",
        "      K_pool: [num_phys_blocks, block_T, D]\n",
        "      V_pool: [num_phys_blocks, block_T, D]\n",
        "    \"\"\"\n",
        "    K_pool = torch.empty((num_phys_blocks, block_T, D), device=device, dtype=dtype)\n",
        "    V_pool = torch.empty((num_phys_blocks, block_T, D), device=device, dtype=dtype)\n",
        "    return K_pool, V_pool\n",
        "\n",
        "\n",
        "def build_block_table(num_logical_blocks: int, num_phys_blocks: int, device=\"cuda\") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Block table maps logical block idx -> physical block idx:\n",
        "      block_table[lb] = pb\n",
        "\n",
        "    For toy:\n",
        "      - you can map 0..L-1 to some subset of physical blocks\n",
        "      - support non-contiguous placement to mimic fragmentation avoidance\n",
        "\n",
        "    Returns:\n",
        "      block_table: [num_logical_blocks] int32\n",
        "    \"\"\"\n",
        "    # TODO: implement mapping strategy (e.g., random perm, or identity)\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def write_tokens_to_paged_kv(\n",
        "    K_tokens: torch.Tensor, V_tokens: torch.Tensor,\n",
        "    K_pool: torch.Tensor, V_pool: torch.Tensor,\n",
        "    block_table: torch.Tensor, block_T: int\n",
        "):\n",
        "    \"\"\"\n",
        "    Scatter logical tokens [T, D] into paged pools using block_table.\n",
        "    This simulates how vLLM stores KV into pages.\n",
        "\n",
        "    Inputs:\n",
        "      K_tokens, V_tokens: [T, D]\n",
        "      K_pool, V_pool: [PB, block_T, D]\n",
        "      block_table: [LB] (logical blocks)\n",
        "      block_T: tokens per block\n",
        "\n",
        "    TODO:\n",
        "      - For each logical token index t:\n",
        "          lb = t // block_T\n",
        "          off = t % block_T\n",
        "          pb = block_table[lb]\n",
        "          write into K_pool[pb, off, :]\n",
        "    \"\"\"\n",
        "    # TODO: implement scatter\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Memory stats\n",
        "# ============================================================\n",
        "def mem_stats_contiguous(T: int, D: int, dtype=torch.float16) -> MemStats:\n",
        "    elem = torch.tensor([], dtype=dtype).element_size()\n",
        "    used = 2 * T * D * elem  # K and V\n",
        "    return MemStats(\n",
        "        mode=\"contiguous\",\n",
        "        logical_T=T,\n",
        "        D=D,\n",
        "        block_T=0,\n",
        "        num_logical_blocks=0,\n",
        "        num_phys_blocks=0,\n",
        "        kv_bytes_allocated=used,\n",
        "        kv_bytes_used=used,\n",
        "        fragmentation_bytes=0,\n",
        "        fragmentation_ratio=0.0,\n",
        "    )\n",
        "\n",
        "\n",
        "def mem_stats_paged(T: int, cfg: PageCfg, dtype=torch.float16) -> MemStats:\n",
        "    elem = torch.tensor([], dtype=dtype).element_size()\n",
        "    block_T = cfg.BLOCK_T\n",
        "    LB = (T + block_T - 1) // block_T\n",
        "    allocated = 2 * cfg.NUM_PHYS_BLOCKS * block_T * cfg.D * elem\n",
        "    used = 2 * T * cfg.D * elem\n",
        "    frag = allocated - used\n",
        "    return MemStats(\n",
        "        mode=\"paged\",\n",
        "        logical_T=T,\n",
        "        D=cfg.D,\n",
        "        block_T=block_T,\n",
        "        num_logical_blocks=LB,\n",
        "        num_phys_blocks=cfg.NUM_PHYS_BLOCKS,\n",
        "        kv_bytes_allocated=allocated,\n",
        "        kv_bytes_used=used,\n",
        "        fragmentation_bytes=max(0, frag),\n",
        "        fragmentation_ratio=max(0.0, frag / max(1, allocated)),\n",
        "    )\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Triton: toy \"paged gather\" helper (kernel-side addressing)\n",
        "# ============================================================\n",
        "@triton.jit\n",
        "def paged_kv_gather_kernel(\n",
        "    # pointers\n",
        "    k_pool_ptr, v_pool_ptr,\n",
        "    block_table_ptr,\n",
        "    # output contiguous buffers for debugging (optional)\n",
        "    k_out_ptr, v_out_ptr,\n",
        "    # sizes\n",
        "    T: tl.constexpr, D: tl.constexpr,\n",
        "    BLOCK_T: tl.constexpr,\n",
        "    # strides (pool is [PB, BLOCK_T, D])\n",
        "    stride_kpb: tl.constexpr, stride_kpt: tl.constexpr, stride_kd: tl.constexpr,\n",
        "    stride_vpb: tl.constexpr, stride_vpt: tl.constexpr, stride_vd: tl.constexpr,\n",
        "    stride_out_t: tl.constexpr, stride_out_d: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    OPTIONAL helper kernel:\n",
        "      Gather paged K/V into contiguous [T, D] buffers.\n",
        "    This is NOT how vLLM does it (they avoid materializing), but useful for debugging.\n",
        "\n",
        "    TODO:\n",
        "      - Map program id to a token block\n",
        "      - For each token t in the block:\n",
        "          lb = t // BLOCK_T\n",
        "          off = t % BLOCK_T\n",
        "          pb = block_table[lb]\n",
        "          load K_pool[pb, off, :]\n",
        "          store into k_out[t, :]\n",
        "      - Similarly for V\n",
        "    \"\"\"\n",
        "    # TODO: implement (optional)\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Triton PageAttention (toy)\n",
        "# ============================================================\n",
        "@triton.jit\n",
        "def page_attention_kernel(\n",
        "    q_ptr,               # [M, D] or [1, D] in toy\n",
        "    k_pool_ptr, v_pool_ptr,   # [PB, BLOCK_T, D]\n",
        "    block_table_ptr,     # [LB]\n",
        "    out_ptr,             # [M, D]\n",
        "    # sizes\n",
        "    M: tl.constexpr,     # number of queries (toy: 1 or small)\n",
        "    T: tl.constexpr,     # total context length (keys/values)\n",
        "    D: tl.constexpr,\n",
        "    BLOCK_T: tl.constexpr,\n",
        "    # strides for Q [M, D]\n",
        "    stride_qm: tl.constexpr, stride_qd: tl.constexpr,\n",
        "    # strides for pool [PB, BLOCK_T, D]\n",
        "    stride_kpb: tl.constexpr, stride_kpt: tl.constexpr, stride_kd: tl.constexpr,\n",
        "    stride_vpb: tl.constexpr, stride_vpt: tl.constexpr, stride_vd: tl.constexpr,\n",
        "    # strides for Out [M, D]\n",
        "    stride_om: tl.constexpr, stride_od: tl.constexpr,\n",
        "    # kernel tiling\n",
        "    BLOCK_N: tl.constexpr,\n",
        "    HAS_MASK: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Toy paged attention:\n",
        "      out[m, :] = softmax( q[m,:] @ K[:T,:]^T + mask ) @ V[:T,:]\n",
        "\n",
        "    Constraints / simplifying assumptions:\n",
        "      - Single head\n",
        "      - Uses block_table to locate K/V blocks\n",
        "      - Does NOT attempt FlashAttention fusion tricks (this is day4, not day6)\n",
        "      - You may implement:\n",
        "          (A) full materialization of scores for toy correctness\n",
        "          or\n",
        "          (B) streaming softmax (more advanced, optional)\n",
        "    Skeleton expects TODOs only.\n",
        "\n",
        "    TODO:\n",
        "      1) Load q vector for row m\n",
        "      2) Iterate over key tiles t0:t0+BLOCK_N\n",
        "          - For each token t in tile:\n",
        "              lb = t // BLOCK_T\n",
        "              off = t % BLOCK_T\n",
        "              pb = block_table[lb]\n",
        "              load k = K_pool[pb, off, :]\n",
        "              compute score = dot(q, k)\n",
        "              apply mask if HAS_MASK\n",
        "          - softmax over T tokens (requires reduction across tiles)\n",
        "      3) Weighted sum over V similarly:\n",
        "          out = sum_j p_j * v_j\n",
        "\n",
        "    Because softmax needs a global normalization across all T,\n",
        "    you will likely need:\n",
        "      - a two-pass approach (scores -> softmax -> PV), OR\n",
        "      - an online softmax approach.\n",
        "\n",
        "    For this Day4 toy, pick the simplest correct approach.\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Torch references\n",
        "# ============================================================\n",
        "def attention_torch_contiguous(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
        "    \"\"\"\n",
        "    Reference attention for contiguous KV:\n",
        "      out = softmax(Q @ K.T + mask) @ V\n",
        "    \"\"\"\n",
        "    # TODO: implement (use fp32 scores for stability)\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def attention_torch_from_paged(\n",
        "    Q: torch.Tensor,\n",
        "    K_pool: torch.Tensor, V_pool: torch.Tensor,\n",
        "    block_table: torch.Tensor, T: int, cfg: PageCfg,\n",
        "    mask: Optional[torch.Tensor] = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Reference attention by first gathering paged KV into contiguous K,V (for correctness only).\n",
        "    Then run standard torch attention.\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # - gather K,V into [T,D] using block_table\n",
        "    # - call attention_torch_contiguous\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Driver: build toy data, run correctness checks\n",
        "# ============================================================\n",
        "@torch.no_grad()\n",
        "def check_correctness(\n",
        "    T: int = 1024,\n",
        "    D: int = 64,\n",
        "    block_T: int = 16,\n",
        "    num_phys_blocks: int = 128,\n",
        "    M: int = 1,\n",
        "    dtype=torch.float16,\n",
        "    use_mask: bool = False,\n",
        "):\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    cfg = PageCfg(BLOCK_T=block_T, D=D, NUM_PHYS_BLOCKS=num_phys_blocks)\n",
        "\n",
        "    # Create a toy query (M queries)\n",
        "    Q = torch.randn((M, D), device=device, dtype=dtype)\n",
        "\n",
        "    # Create logical tokens for KV (as if appended over time)\n",
        "    K_tokens = torch.randn((T, D), device=device, dtype=dtype)\n",
        "    V_tokens = torch.randn((T, D), device=device, dtype=dtype)\n",
        "\n",
        "    # --- Contiguous baseline ---\n",
        "    K_contig, V_contig = alloc_contiguous_kv(T, D, device=device, dtype=dtype)\n",
        "    K_contig.copy_(K_tokens)\n",
        "    V_contig.copy_(V_tokens)\n",
        "\n",
        "    mask = None\n",
        "    if use_mask:\n",
        "        # TODO: define mask shape; for toy use [M, T] or [M, T] additive\n",
        "        # or full [M, T] if you compute scores row-wise.\n",
        "        # mask = ...\n",
        "        raise NotImplementedError(\"TODO: mask construction\")\n",
        "\n",
        "    # TODO: torch contiguous reference\n",
        "    # out_ref = attention_torch_contiguous(Q, K_contig, V_contig, mask=mask)\n",
        "    out_ref = None\n",
        "\n",
        "    # --- Paged layout ---\n",
        "    K_pool, V_pool = alloc_paged_kv_pool(num_phys_blocks, block_T, D, device=device, dtype=dtype)\n",
        "    LB = (T + block_T - 1) // block_T\n",
        "    block_table = build_block_table(LB, num_phys_blocks, device=device)\n",
        "\n",
        "    write_tokens_to_paged_kv(K_tokens, V_tokens, K_pool, V_pool, block_table, block_T)\n",
        "\n",
        "    # TODO: Triton paged attention\n",
        "    # out_paged = page_attention_triton(Q, K_pool, V_pool, block_table, T, cfg, mask=mask)\n",
        "    out_paged = None\n",
        "\n",
        "    # TODO: compare out_paged with out_ref\n",
        "    # max_err = (out_paged - out_ref).abs().max().item()\n",
        "    # mean_err = (out_paged - out_ref).abs().mean().item()\n",
        "    # print(...)\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def page_attention_triton(\n",
        "    Q: torch.Tensor,\n",
        "    K_pool: torch.Tensor,\n",
        "    V_pool: torch.Tensor,\n",
        "    block_table: torch.Tensor,\n",
        "    T: int,\n",
        "    cfg: PageCfg,\n",
        "    mask: Optional[torch.Tensor] = None,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Launcher for page_attention_kernel.\n",
        "    Q: [M, D]\n",
        "    K_pool/V_pool: [PB, BLOCK_T, D]\n",
        "    block_table: [LB]\n",
        "    Returns:\n",
        "      out: [M, D]\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # - Validate shapes/dtypes\n",
        "    # - Allocate out\n",
        "    # - Define grid (e.g., one program per query row m)\n",
        "    # - Pass strides and constexpr args\n",
        "    # - HAS_MASK toggle\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Memory statistics printing\n",
        "# ============================================================\n",
        "def print_mem_stats(T: int, D: int, cfg: PageCfg, dtype=torch.float16):\n",
        "    c = mem_stats_contiguous(T, D, dtype=dtype)\n",
        "    p = mem_stats_paged(T, cfg, dtype=dtype)\n",
        "\n",
        "    print(\"\\n=== Memory Stats ===\")\n",
        "    print(f\"Contiguous KV: allocated={c.kv_bytes_allocated/1e6:.3f} MB, used={c.kv_bytes_used/1e6:.3f} MB\")\n",
        "    print(f\"Paged KV     : allocated={p.kv_bytes_allocated/1e6:.3f} MB, used={p.kv_bytes_used/1e6:.3f} MB, \"\n",
        "          f\"frag={p.fragmentation_ratio*100:.2f}%\")\n",
        "    print(\"====================\\n\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Main\n",
        "# ============================================================\n",
        "def main():\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"CUDA required.\")\n",
        "\n",
        "    # TODO: adjust toy parameters\n",
        "    T = 1024\n",
        "    D = 64\n",
        "    block_T = 16\n",
        "    num_phys_blocks = 128\n",
        "    M = 1\n",
        "\n",
        "    cfg = PageCfg(BLOCK_T=block_T, D=D, NUM_PHYS_BLOCKS=num_phys_blocks)\n",
        "    print_mem_stats(T, D, cfg, dtype=torch.float16)\n",
        "\n",
        "    # TODO: run correctness\n",
        "    # check_correctness(T=T, D=D, block_T=block_T, num_phys_blocks=num_phys_blocks, M=M, use_mask=False)\n",
        "    raise NotImplementedError(\"TODO: wire up correctness once kernels are implemented\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN2WzkMz8pmm"
      },
      "outputs": [],
      "source": [
        "# day6_flashattention_mini_skeleton.py\n",
        "# ============================================================\n",
        "# Day 6 ‚Äî Triton FlashAttention (Mini) (NO SOLUTION)\n",
        "#\n",
        "# Goal:\n",
        "#   Implement a mini FlashAttention-style kernel in Triton:\n",
        "#     - tiled QK^T\n",
        "#     - online softmax (streaming max/sum)\n",
        "#     - fuse V multiplication\n",
        "#     - single kernel end-to-end\n",
        "#   Then:\n",
        "#     - validate correctness vs PyTorch\n",
        "#     - compare performance vs naive attention (Day2)\n",
        "#\n",
        "# Scope (toy but realistic):\n",
        "#   - Single head (extend later)\n",
        "#   - One batch (extend later)\n",
        "#   - Q,K,V: [N, D] (N = seq length, D = head dim)\n",
        "#   - Output O: [N, D]\n",
        "#   - Optional causal mask (recommended)\n",
        "#   - Inputs fp16/bf16, accumulate fp32\n",
        "#\n",
        "# Notes:\n",
        "#   - This is a skeleton with TODOs only (no solution).\n",
        "#   - You will need to choose tiling sizes that fit SRAM (shared memory/registers).\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import math\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, Dict, Any, Tuple, List\n",
        "\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TODO: import your Day2 naive attention for comparison\n",
        "# ============================================================\n",
        "def naive_attention_triton(Q, K, V, mask=None, cfg=None):\n",
        "    # TODO: import and call your Day2 implementation\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Config\n",
        "# ============================================================\n",
        "@dataclass(frozen=True)\n",
        "class FlashCfg:\n",
        "    BLOCK_M: int     # rows of Q processed per program\n",
        "    BLOCK_N: int     # cols of K/V per step (streaming over N)\n",
        "    BLOCK_D: int     # head dim tile (usually == D, but keep generic)\n",
        "    num_warps: int = 4\n",
        "    num_stages: int = 2  # optional pipelining\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Mask helper (optional)\n",
        "# ============================================================\n",
        "def make_additive_causal_mask(N: int, device=\"cuda\", dtype=torch.float32) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Returns additive causal mask [N, N]:\n",
        "      0 for j <= i, -inf for j > i\n",
        "    Used as: scores = scores + mask\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FlashAttention mini kernel (single kernel)\n",
        "# ============================================================\n",
        "@triton.jit\n",
        "def flashattn_mini_kernel(\n",
        "    q_ptr, k_ptr, v_ptr, o_ptr,\n",
        "    # optional mask pointer (additive), can be None by HAS_MASK flag\n",
        "    mask_ptr,\n",
        "    N: tl.constexpr, D: tl.constexpr,\n",
        "    stride_qn: tl.constexpr, stride_qd: tl.constexpr,\n",
        "    stride_kn: tl.constexpr, stride_kd: tl.constexpr,\n",
        "    stride_vn: tl.constexpr, stride_vd: tl.constexpr,\n",
        "    stride_on: tl.constexpr, stride_od: tl.constexpr,\n",
        "    # mask strides (if used): mask is [N, N] additive\n",
        "    stride_mn: tl.constexpr, stride_mm: tl.constexpr,\n",
        "    # tiling\n",
        "    BLOCK_M: tl.constexpr,\n",
        "    BLOCK_N: tl.constexpr,\n",
        "    BLOCK_D: tl.constexpr,\n",
        "    HAS_MASK: tl.constexpr,\n",
        "    IS_CAUSAL: tl.constexpr,\n",
        "    # scale (typically 1/sqrt(D))\n",
        "    SCALE: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute O = softmax(QK^T + mask) V using tiling + online softmax, fused with V.\n",
        "\n",
        "    Structure (conceptual):\n",
        "      For each block of queries (m tile):\n",
        "        - initialize:\n",
        "            m_i = -inf         # running max per query row\n",
        "            l_i = 0            # running sum(exp(scores - m_i))\n",
        "            acc = 0            # running output accumulator (fp32)\n",
        "        - for n_tile over keys/values:\n",
        "            scores = q_tile @ k_tile^T * SCALE + mask_tile\n",
        "            # online softmax update:\n",
        "            m_new = max(m_i, rowmax(scores))\n",
        "            alpha = exp(m_i - m_new)\n",
        "            p = exp(scores - m_new)\n",
        "            l_new = l_i * alpha + rowsum(p)\n",
        "            acc = acc * alpha[:,None] + p @ v_tile\n",
        "            m_i = m_new\n",
        "            l_i = l_new\n",
        "        - normalize:\n",
        "            out = acc / l_i[:,None]\n",
        "        - store out\n",
        "\n",
        "    TODOs:\n",
        "      - Map program_id to query block start\n",
        "      - Load Q tile [BLOCK_M, D] (or [BLOCK_M, BLOCK_D] with loop if needed)\n",
        "      - Loop over K/V tiles along N:\n",
        "          * Load K tile [BLOCK_N, D]\n",
        "          * Compute score tile [BLOCK_M, BLOCK_N] in fp32\n",
        "          * Apply causal masking if IS_CAUSAL (score for j>i = -inf)\n",
        "          * Apply additive mask if HAS_MASK (mask_ptr)\n",
        "          * Update online softmax stats (m_i, l_i)\n",
        "          * Fuse V multiplication: acc += p @ V_tile\n",
        "      - Final normalize acc by l_i\n",
        "      - Store O tile\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Launcher\n",
        "# ============================================================\n",
        "def flashattn_mini_triton(\n",
        "    Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,\n",
        "    mask: Optional[torch.Tensor] = None,\n",
        "    causal: bool = False,\n",
        "    cfg: FlashCfg = FlashCfg(BLOCK_M=64, BLOCK_N=128, BLOCK_D=64, num_warps=4),\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Q,K,V: [N, D]\n",
        "    mask: additive [N, N] or None\n",
        "    causal: if True, apply causal mask internally (avoid materializing full mask if you implement it that way)\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # - Validate shapes, dtypes, contiguity\n",
        "    # - Allocate output O [N, D]\n",
        "    # - Define grid (e.g., grid = (ceil_div(N, BLOCK_M),))\n",
        "    # - Set HAS_MASK / IS_CAUSAL flags\n",
        "    # - SCALE = 1/sqrt(D)\n",
        "    # - Call flashattn_mini_kernel[grid](...)\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PyTorch reference + correctness\n",
        "# ============================================================\n",
        "def flashattn_ref_torch(\n",
        "    Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,\n",
        "    mask: Optional[torch.Tensor] = None,\n",
        "    causal: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Reference attention in torch:\n",
        "      scores = Q @ K.T / sqrt(D)\n",
        "      if causal: apply causal mask\n",
        "      if mask: scores += mask\n",
        "      P = softmax(scores)\n",
        "      O = P @ V\n",
        "    \"\"\"\n",
        "    # TODO: implement reference (use fp32 for scores/softmax for stability)\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def check_correctness(\n",
        "    N=1024, D=64, dtype=torch.float16,\n",
        "    use_mask=False, causal=True,\n",
        "    cfg: FlashCfg = FlashCfg(BLOCK_M=64, BLOCK_N=128, BLOCK_D=64, num_warps=4),\n",
        "):\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    Q = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    K = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    V = torch.randn((N, D), device=device, dtype=dtype)\n",
        "\n",
        "    mask = None\n",
        "    if use_mask:\n",
        "        # TODO: create additive mask (e.g., padding or random -inf positions)\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # TODO:\n",
        "    # - out_ref = flashattn_ref_torch(...)\n",
        "    # - out_tri = flashattn_mini_triton(...)\n",
        "    # - print max/mean abs error\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Benchmark: compare naive vs flash\n",
        "# ============================================================\n",
        "def cuda_time_ms(fn, iters=30, warmup=10) -> float:\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    s = torch.cuda.Event(enable_timing=True)\n",
        "    e = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    s.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    e.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return s.elapsed_time(e) / iters\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def compare_perf(\n",
        "    N_list: List[int] = [256, 512, 1024, 2048],\n",
        "    D: int = 64,\n",
        "    dtype=torch.float16,\n",
        "    causal: bool = True,\n",
        "    cfg_flash: FlashCfg = FlashCfg(BLOCK_M=64, BLOCK_N=128, BLOCK_D=64, num_warps=4),\n",
        "    cfg_naive: Optional[Dict[str, Any]] = None,\n",
        "):\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    print(\"| N | Impl | ms | speedup_vs_naive |\")\n",
        "    print(\"|---|------|----|------------------|\")\n",
        "\n",
        "    for N in N_list:\n",
        "        Q = torch.randn((N, D), device=device, dtype=dtype)\n",
        "        K = torch.randn((N, D), device=device, dtype=dtype)\n",
        "        V = torch.randn((N, D), device=device, dtype=dtype)\n",
        "\n",
        "        mask = None\n",
        "        if causal:\n",
        "            # For naive attention you may need a materialized mask; for flash you might do internal causal.\n",
        "            # TODO: create mask for naive if required by your implementation.\n",
        "            pass\n",
        "\n",
        "        # --- naive ---\n",
        "        def fn_naive():\n",
        "            return naive_attention_triton(Q, K, V, mask=mask, cfg=cfg_naive)\n",
        "\n",
        "        # --- flash ---\n",
        "        def fn_flash():\n",
        "            return flashattn_mini_triton(Q, K, V, mask=None, causal=causal, cfg=cfg_flash)\n",
        "\n",
        "        # TODO: optionally benchmark torch reference too\n",
        "        ms_naive = cuda_time_ms(fn_naive)\n",
        "        ms_flash = cuda_time_ms(fn_flash)\n",
        "\n",
        "        speedup = ms_naive / ms_flash if ms_flash > 0 else float(\"inf\")\n",
        "\n",
        "        print(f\"| {N} | naive | {ms_naive:.4f} | 1.00x |\")\n",
        "        print(f\"| {N} | flash | {ms_flash:.4f} | {speedup:.2f}x |\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Main\n",
        "# ============================================================\n",
        "def main():\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"CUDA required.\")\n",
        "\n",
        "    # TODO: run correctness first on small N\n",
        "    # check_correctness(N=256, D=64, causal=True, use_mask=False)\n",
        "\n",
        "    # TODO: then benchmark scaling\n",
        "    # compare_perf(N_list=[256, 512, 1024, 2048], D=64)\n",
        "\n",
        "    raise NotImplementedError(\"TODO: wire up your kernels and run correctness/bench\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}