{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs_bXqDJWwhs"
      },
      "source": [
        "# Triton Attention Systems: Naive ‚Üí Page ‚Üí Flash\n",
        "\n",
        "üéØ **Weekly Goal**  \n",
        "Implement attention from scratch in Triton, profile performance bottlenecks,  \n",
        "understand KV cache memory layouts (PagedAttention), and build a mini FlashAttention kernel  \n",
        "to develop intuition for **IO-awareness, tiling, SRAM reuse, and kernel fusion**.\n",
        "\n",
        "---\n",
        "\n",
        "# Day 2 ‚Äî Naive Triton Attention\n",
        "\n",
        "## Objective\n",
        "\n",
        "Implement the most straightforward attention pipeline:\n",
        "\n",
        "attn = softmax(QK·µÄ) @ V\n",
        "\n",
        "Each stage must be implemented as an independent Triton kernel.  \n",
        "‚ö†Ô∏è No fusion. No tiling optimization. No IO reduction tricks.\n",
        "\n",
        "---\n",
        "\n",
        "## Tasks\n",
        "\n",
        "- [ ] Implement QK·µÄ kernel\n",
        "- [ ] Implement row-wise softmax kernel\n",
        "- [ ] Implement P @ V kernel\n",
        "- [ ] Add optional mask support (causal / padding)\n",
        "- [ ] Validate correctness vs PyTorch reference\n",
        "- [ ] Measure max / mean absolute error\n",
        "- [ ] Test small and large sequence lengths\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "- Attention compute complexity: O(n¬≤d)\n",
        "- Memory traffic complexity: O(n¬≤)\n",
        "- Materializing the attention matrix is expensive\n",
        "- Softmax requires multiple passes:\n",
        "  - max reduction\n",
        "  - exp + sum\n",
        "  - normalization\n",
        "\n",
        "---\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "- triton_naive_attention.py\n",
        "- Correctness validation script\n",
        "- Basic latency benchmark (ms)\n",
        "\n",
        "---\n",
        "\n",
        "# Day 3 ‚Äî Profiling & Bottleneck Analysis\n",
        "\n",
        "## Objective\n",
        "\n",
        "Diagnose why naive attention is slow using Nsight Compute.\n",
        "\n",
        "---\n",
        "\n",
        "## Tasks\n",
        "\n",
        "- [ ] Profile kernels with Nsight Compute\n",
        "- [ ] Collect:\n",
        "  - DRAM throughput\n",
        "  - SM efficiency\n",
        "  - Achieved occupancy\n",
        "  - Warp stall reasons\n",
        "- [ ] Identify whether bottleneck is:\n",
        "  - memory-bound\n",
        "  - reduction-bound\n",
        "  - compute-bound\n",
        "- [ ] Sweep:\n",
        "  - block sizes\n",
        "  - sequence length (512 ‚Üí 4k ‚Üí 8k)\n",
        "  - fp16 vs fp32\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "- Softmax is typically memory-bound\n",
        "- QK·µÄ behaves like GEMM (often compute-bound)\n",
        "- Writing and rereading n¬≤ matrices dominates IO\n",
        "- Arithmetic intensity determines roofline behavior\n",
        "\n",
        "---\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "### Performance Table\n",
        "\n",
        "| Impl  | ms | GB/s | TFLOPs | Speedup |\n",
        "|-------|----|------|--------|---------|\n",
        "| Torch |    |      |        | 1.0x    |\n",
        "| Naive |    |      |        |         |\n",
        "\n",
        "### Bottleneck Analysis Writeup\n",
        "\n",
        "Explain:\n",
        "\n",
        "- Why softmax is IO-heavy  \n",
        "- Why n¬≤ memory traffic dominates  \n",
        "- What stall reason dominates  \n",
        "- Whether QK·µÄ saturates compute units  \n",
        "\n",
        "---\n",
        "\n",
        "# Day 4 ‚Äî PageAttention (KV Cache Layout)\n",
        "\n",
        "## Objective\n",
        "\n",
        "Understand how vLLM reduces KV memory waste via block-based paging.\n",
        "\n",
        "---\n",
        "\n",
        "## Tasks\n",
        "\n",
        "- [ ] Study contiguous KV layout\n",
        "- [ ] Study paged KV layout\n",
        "- [ ] Design fixed-size KV blocks\n",
        "- [ ] Implement logical-to-physical block mapping\n",
        "- [ ] Write toy Triton PageAttention kernel\n",
        "- [ ] Validate correctness\n",
        "- [ ] Measure memory usage\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "- KV cache grows linearly with sequence length\n",
        "- Contiguous layout leads to fragmentation\n",
        "- Paged layout uses block tables\n",
        "- Improves memory utilization for long-context inference\n",
        "\n",
        "---\n",
        "\n",
        "## Memory Comparison\n",
        "\n",
        "| Mode        | KV Memory | Fragmentation | Best Use Case |\n",
        "|------------|------------|---------------|---------------|\n",
        "| Contiguous |            |               |               |\n",
        "| Paged      |            |               |               |\n",
        "\n",
        "---\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "- triton_page_attention.py\n",
        "- Memory usage comparison\n",
        "- Short explanation of when paging helps\n",
        "\n",
        "---\n",
        "\n",
        "# Day 5 ‚Äî FlashAttention Theory & Tiling Design\n",
        "\n",
        "## Objective\n",
        "\n",
        "Understand IO-aware attention and why FlashAttention is faster.\n",
        "\n",
        "---\n",
        "\n",
        "## Tasks\n",
        "\n",
        "- [ ] Study FlashAttention core ideas:\n",
        "  - SRAM reuse\n",
        "  - Block Q\n",
        "  - Block K\n",
        "  - Online softmax\n",
        "  - Avoid n¬≤ materialization\n",
        "- [ ] Derive why IO is reduced\n",
        "- [ ] Compute arithmetic intensity before vs after tiling\n",
        "- [ ] Design kernel parameters:\n",
        "  - BLOCK_M\n",
        "  - BLOCK_N\n",
        "  - BLOCK_D\n",
        "- [ ] Write kernel skeleton:\n",
        "  - for k_tile in K:\n",
        "  - compute qk_tile\n",
        "  - update running max\n",
        "  - update running sum\n",
        "  - accumulate output\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "- Avoid writing S (n √ó n) to DRAM\n",
        "- Online softmax enables single-pass normalization\n",
        "- FlashAttention reduces memory traffic from O(n¬≤) ‚Üí O(nd)\n",
        "- Kernel fusion increases arithmetic intensity\n",
        "\n",
        "---\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "- FlashAttention design document\n",
        "- Arithmetic intensity comparison\n",
        "- Kernel skeleton file\n",
        "\n",
        "---\n",
        "\n",
        "# Day 6 ‚Äî Triton FlashAttention (Mini Version)\n",
        "\n",
        "## Objective\n",
        "\n",
        "Implement a fused, tiled attention kernel in Triton.\n",
        "\n",
        "---\n",
        "\n",
        "## Tasks\n",
        "\n",
        "- [ ] Implement tiled QK·µÄ\n",
        "- [ ] Implement online softmax\n",
        "- [ ] Fuse V multiplication\n",
        "- [ ] Integrate into single kernel\n",
        "- [ ] Validate correctness\n",
        "- [ ] Benchmark vs naive implementation\n",
        "\n",
        "---\n",
        "\n",
        "## Final Comparison\n",
        "\n",
        "| Impl   | ms | GB/s | TFLOPs | Speedup |\n",
        "|--------|----|------|--------|---------|\n",
        "| Naive  |    |      |        | 1.0x    |\n",
        "| Flash  |    |      |        | 2.0x+   |\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "- SRAM reuse eliminates n¬≤ writes\n",
        "- Fusion reduces global memory traffic\n",
        "- FlashAttention shifts kernel toward compute-bound region\n",
        "- IO-awareness matters more than reducing FLOPs\n",
        "\n",
        "---\n",
        "\n",
        "# End-of-Week Takeaways\n",
        "\n",
        "- Attention performance is dominated by memory traffic\n",
        "- Softmax is more memory-bound than QK·µÄ\n",
        "- Kernel fusion drastically improves arithmetic intensity\n",
        "- FlashAttention works by reducing IO, not reducing math\n",
        "- Triton enables CUDA-level attention kernel design in Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvND3OxsZRtf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b38dfa-0506-414a-8c5c-19fa1b4ef750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[check_correctness] N=128, D=64, dtype=torch.float16, use_mask=True\n",
            "  max_abs_err : 1.192093e-06\n",
            "  mean_abs_err: 6.595712e-08\n",
            "  rmse        : 1.132886e-07\n",
            "\n",
            "[quick_bench]\n",
            "N=1024, D=64, dtype=torch.float16, mask=False\n",
            "Triton: 4.277 ms\n",
            "Torch : 0.153 ms\n",
            "Speedup (Torch/Triton): 0.04x\n"
          ]
        }
      ],
      "source": [
        "# triton_naive_attention_skeleton.py\n",
        "# ============================================================\n",
        "# Day 2 ‚Äî Triton Naive Attention Kernel (NO SOLUTION)\n",
        "# Goal:\n",
        "#   Implement naive attention:\n",
        "#       attn = softmax(Q @ K.T) @ V\n",
        "#   - Separate kernels for each step (QK^T, softmax, PV)\n",
        "#   - Correctness validation vs PyTorch\n",
        "#   - Support mask (causal / padding via additive -inf)\n",
        "#   - Intentionally NOT optimized (no fusion, no FlashAttention tricks)\n",
        "#\n",
        "# Notes:\n",
        "#   - This is a skeleton with TODOs only. Fill in kernels + launcher code.\n",
        "#   - Keep correctness first; performance will be poor by design.\n",
        "# ============================================================\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Utilities\n",
        "# -----------------------------\n",
        "def _assert_cuda(x: torch.Tensor, name: str):\n",
        "    if not x.is_cuda:\n",
        "        raise ValueError(f\"{name} must be on CUDA, got {x.device}\")\n",
        "    if not x.is_contiguous():\n",
        "        raise ValueError(f\"{name} must be contiguous for this skeleton.\")\n",
        "\n",
        "\n",
        "def _make_additive_causal_mask(n: int, device, dtype):\n",
        "    \"\"\"\n",
        "    Returns additive mask M in shape [n, n]:\n",
        "      M[i, j] = 0 for j <= i\n",
        "      M[i, j] = -inf for j > i\n",
        "    Used as: scores = scores + M\n",
        "    \"\"\"\n",
        "    # TODO: implement causal mask creation\n",
        "    # raise NotImplementedError\n",
        "    if device is None:\n",
        "        device = \"cpu\"\n",
        "    if dtype is None:\n",
        "        dtype = torch.float32\n",
        "\n",
        "    # Use the minimum finite value for the dtype to represent \"-inf\" in practice.\n",
        "    # For fp16/bf16, true -inf exists, but using finfo.min is also common and safe.\n",
        "    # neg_inf = torch.finfo(dtype).min\n",
        "\n",
        "    # upper triangular (strictly above diagonal) => future positions\n",
        "    # shape [n, n], True where j > i\n",
        "    future = torch.triu(torch.ones((n, n), device=device, dtype=torch.bool), diagonal=1)\n",
        "\n",
        "    # start from zeros, fill future with neg_inf\n",
        "    mask = torch.zeros((n, n), device=device, dtype=dtype)\n",
        "    mask = mask.masked_fill(future, -float(\"inf\"))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def _make_additive_padding_mask(valid_lens: torch.Tensor, n: int, device, dtype):\n",
        "    \"\"\"\n",
        "    valid_lens: [B] or [n] style lengths; for this Day2 skeleton we keep it simple:\n",
        "      - Assume a single sequence length n, and valid_lens is optional.\n",
        "      - If you want per-row masking, expand to [n, n] additive mask.\n",
        "      scores = scores + padding_mask\n",
        "    \"\"\"\n",
        "    # TODO: implement padding mask (optional)\n",
        "    # raise NotImplementedError\n",
        "    if device is None:\n",
        "        device = \"cpu\"\n",
        "    if dtype is None:\n",
        "        dtype = torch.float32\n",
        "\n",
        "    neg_inf = torch.finfo(dtype).min\n",
        "\n",
        "    # assume single length\n",
        "    if valid_lens is None:\n",
        "        return torch.zeros((n, n), device=device, dtype=dtype)\n",
        "\n",
        "    L = int(valid_lens.item())\n",
        "\n",
        "    # shape [n]\n",
        "    key_positions = torch.arange(n, device=device)\n",
        "\n",
        "    # True where j >= L\n",
        "    invalid = key_positions >= L\n",
        "\n",
        "    # expand to [n, n] (each row same mask)\n",
        "    invalid = invalid.unsqueeze(0).expand(n, n)\n",
        "\n",
        "    mask = torch.zeros((n, n), device=device, dtype=dtype)\n",
        "    mask = mask.masked_fill(invalid, neg_inf)\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Kernel 1: Scores = Q @ K^T\n",
        "# Q: [N, D], K: [N, D]  => Scores: [N, N]\n",
        "# ============================================================\n",
        "@triton.jit\n",
        "def qk_t_kernel(\n",
        "    q_ptr, k_ptr, s_ptr,\n",
        "    N: tl.constexpr, D: tl.constexpr,\n",
        "    stride_qn: tl.constexpr, stride_qd: tl.constexpr,\n",
        "    stride_kn: tl.constexpr, stride_kd: tl.constexpr,\n",
        "    stride_sn: tl.constexpr, stride_sm: tl.constexpr,\n",
        "    # Tile sizes (intentionally simple / naive)\n",
        "    BLOCK_M: tl.constexpr,\n",
        "    BLOCK_N: tl.constexpr,\n",
        "    BLOCK_K: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute a tile of S = QK^T.\n",
        "    Program ids map over (rows, cols) tiles of S.\n",
        "\n",
        "    TODO:\n",
        "    - Compute pid_m, pid_n\n",
        "    - Compute row/col offsets\n",
        "    - Load Q tile [BLOCK_M, BLOCK_K]\n",
        "    - Load K tile [BLOCK_N, BLOCK_K] (note K^T => K rows act like cols)\n",
        "    - Accumulate dot products\n",
        "    - Store to S\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    tl.static_assert(BLOCK_K <= D,\n",
        "    \"BLOCK_K must be <= D for this skeleton\")\n",
        "    # raise NotImplementedError\n",
        "\n",
        "    # 2D program grid: each program handles a (BLOCK_M x BLOCK_N) tile of X\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "\n",
        "    m_mask = offs_m < N\n",
        "    n_mask = offs_n < N\n",
        "\n",
        "\n",
        "    for k0 in range(0, D, BLOCK_K):\n",
        "        d_offsets = k0 + offs_k\n",
        "        d_mask = d_offsets < D\n",
        "\n",
        "        q_ptrs = q_ptr + offs_m[:, None] * stride_qn + d_offsets[None, :] * stride_qd   # [BM, BK]\n",
        "        k_ptrs = k_ptr + offs_n[:, None] * stride_kn + d_offsets[None, :] * stride_kd  # [BN,BK]\n",
        "\n",
        "        # 2D masks for loads\n",
        "        q_load_mask = m_mask[:, None] & d_mask[None, :]    # [BM, BK]\n",
        "        k_load_mask = n_mask[:, None] & d_mask[None, :]    # [BK, BN]\n",
        "\n",
        "\n",
        "        q_tile = tl.load(q_ptrs, mask = q_load_mask, other = 0.0)\n",
        "        k_tile = tl.load(k_ptrs, mask = k_load_mask, other = 0.0)\n",
        "\n",
        "        acc += tl.dot(q_tile, tl.trans(k_tile))\n",
        "\n",
        "    s_ptrs = s_ptr + offs_m[:, None] * stride_sn + offs_n[None, :] * stride_sm\n",
        "    tl.store(s_ptrs, acc, mask = m_mask[:, None] & n_mask[None, :])\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Kernel 2: Softmax over each row of S (row-wise)\n",
        "# S: [N, N] -> P: [N, N]\n",
        "# Optional additive mask: M: [N, N] where invalid positions are -inf\n",
        "# ============================================================\n",
        "@triton.jit\n",
        "def softmax_row_kernel(\n",
        "    s_ptr, m_ptr, p_ptr,\n",
        "    N: tl.constexpr,\n",
        "    stride_sn: tl.constexpr, stride_sm: tl.constexpr,\n",
        "    stride_mn: tl.constexpr, stride_mm: tl.constexpr,\n",
        "    stride_pn: tl.constexpr, stride_pm: tl.constexpr,\n",
        "    BLOCK_N: tl.constexpr,\n",
        "    HAS_MASK: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Row-wise softmax:\n",
        "      p[i, :] = softmax(s[i, :] + mask[i, :])\n",
        "      p = s + m\n",
        "\n",
        "    TODO:\n",
        "    - Map program id to a row i\n",
        "    - Load a row block of scores\n",
        "    - If HAS_MASK, load mask and add\n",
        "    - Numerically stable softmax:\n",
        "        x = x - max(x)\n",
        "        exp = tl.exp(x)\n",
        "        denom = tl.sum(exp)\n",
        "        p = exp / denom\n",
        "    - Store p\n",
        "\n",
        "    Notes:\n",
        "    - This skeleton assumes N can be larger than BLOCK_N; you may loop over blocks\n",
        "      or restrict this Day2 to N <= BLOCK_N initially.\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    # raise NotImplementedError\n",
        "    # if N > BLOCK_N:\n",
        "    #     raise ValueError(\"Naive row-softmax requires N <= BLOCK_N\")\n",
        "    pid_row = tl.program_id(0)\n",
        "    offs = tl.arange(0, BLOCK_N)\n",
        "\n",
        "    s_row_ptr = s_ptr + pid_row * stride_sn + offs * stride_sm\n",
        "    p_row_ptr = p_ptr + pid_row * stride_pn + offs * stride_pm\n",
        "\n",
        "    mask = offs < N\n",
        "    s_row = tl.load(s_row_ptr, mask = mask, other = -float(\"inf\")).to(tl.float32)\n",
        "    if HAS_MASK:\n",
        "        m_row_ptr = m_ptr + pid_row * stride_mn + offs * stride_mm\n",
        "        m_row = tl.load(m_row_ptr, mask = mask, other = 0.0).to(tl.float32)\n",
        "        s_row = s_row + m_row\n",
        "\n",
        "    # stable softmax\n",
        "    s_max = tl.max(s_row, axis = 0)\n",
        "    s_row = s_row - s_max\n",
        "    s_exp = tl.exp(s_row)\n",
        "    s_sum = tl.sum(s_exp, axis=0)\n",
        "    p_row = s_exp / s_sum\n",
        "\n",
        "    tl.store(p_row_ptr, p_row, mask = mask)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Kernel 3: Out = P @ V\n",
        "# P: [N, N], V: [N, D] -> O: [N, D]\n",
        "# ============================================================\n",
        "@triton.jit\n",
        "def pv_kernel(\n",
        "    p_ptr, v_ptr, o_ptr,\n",
        "    N: tl.constexpr, D: tl.constexpr,\n",
        "    stride_pn: tl.constexpr, stride_pm: tl.constexpr,\n",
        "    stride_vn: tl.constexpr, stride_vd: tl.constexpr,\n",
        "    stride_on: tl.constexpr, stride_od: tl.constexpr,\n",
        "    BLOCK_M: tl.constexpr,\n",
        "    BLOCK_N: tl.constexpr,\n",
        "    BLOCK_K: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute a tile of O = P V.\n",
        "\n",
        "    TODO:\n",
        "    - Program ids over (rows of O, cols of O)\n",
        "    - Load P tile [BLOCK_M, BLOCK_K]\n",
        "    - Load V tile [BLOCK_K, BLOCK_N] (here K dimension is N of P / V rows)\n",
        "    - Accumulate\n",
        "    - Store to O\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    # raise NotImplementedError\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "    m_mask = offs_m < N\n",
        "    n_mask = offs_n < D\n",
        "\n",
        "    for k0 in tl.static_range(0, N, BLOCK_K):\n",
        "          # current K indices for this chunk\n",
        "          k_offsets = k0 + offs_k\n",
        "          k_mask = k_offsets < N\n",
        "\n",
        "          # build pointer grids for this chunk\n",
        "          p_ptrs = p_ptr + offs_m[:, None] * stride_pn + k_offsets[None, :] * stride_pm\n",
        "          v_ptrs = v_ptr + k_offsets[:, None] * stride_vn + offs_n[None, :] * stride_vd\n",
        "\n",
        "          # 2D masks for loads\n",
        "          p_load_mask = m_mask[:, None] & k_mask[None, :]\n",
        "          v_load_mask = k_mask[:, None] & n_mask[None, :]\n",
        "\n",
        "          # masked loads: out-of-bounds => 0\n",
        "          p_tile = tl.load(p_ptrs, mask=p_load_mask, other=0).to(tl.float32)\n",
        "          v_tile = tl.load(v_ptrs, mask=v_load_mask, other=0).to(tl.float32)\n",
        "\n",
        "          # accumulate (fp32)\n",
        "          # tl.dot will typically accumulate in fp32 when acc is fp32\n",
        "          acc += tl.dot(p_tile, v_tile)\n",
        "\n",
        "    o_tile = acc\n",
        "    o_ptrs = o_ptr + offs_m[:, None] * stride_on + offs_n[None, :] * stride_od\n",
        "    tl.store(o_ptrs, o_tile, mask=m_mask[:,None] & n_mask[None,:])\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Launchers (NO SOLUTION)\n",
        "# ============================================================\n",
        "def qk_t_triton(Q: torch.Tensor, K: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute S = Q @ K^T\n",
        "    Q, K: [N, D] contiguous CUDA tensors\n",
        "    Returns:\n",
        "      S: [N, N]\n",
        "    \"\"\"\n",
        "    _assert_cuda(Q, \"Q\")\n",
        "    _assert_cuda(K, \"K\")\n",
        "    assert Q.shape == K.shape\n",
        "    N, D = Q.shape\n",
        "\n",
        "    S = torch.empty((N, N), device=Q.device, dtype=torch.float32)  # scores typically fp32\n",
        "\n",
        "    # TODO:\n",
        "    # - Choose BLOCK_M/BLOCK_N/BLOCK_K (naive defaults)\n",
        "    # - Define grid mapping over tiles\n",
        "    # - Call qk_t_kernel[grid](...)\n",
        "    # raise NotImplementedError\n",
        "    BLOCK_M = 128\n",
        "    BLOCK_N = 128\n",
        "    BLOCK_K = 64\n",
        "    # if N > BLOCK_N:\n",
        "    #     raise ValueError(\"Naive row-softmax requires N <= BLOCK_N\")\n",
        "\n",
        "    N, D = Q.shape\n",
        "    N2, D2 = K.shape\n",
        "    assert N == N2, \"Q and K must have the same sequence length\"\n",
        "    assert D == D2, \"Q and K must have the same head dimension\"\n",
        "\n",
        "    grid = (\n",
        "        triton.cdiv(N, BLOCK_M),\n",
        "        triton.cdiv(N, BLOCK_N),\n",
        "    )\n",
        "\n",
        "    qk_t_kernel[grid](\n",
        "        Q, K, S,\n",
        "        N = N, D = D,\n",
        "        stride_qn=Q.stride(0), stride_qd=Q.stride(1),\n",
        "        stride_kn=K.stride(0), stride_kd=K.stride(1),\n",
        "        stride_sn=S.stride(0), stride_sm=S.stride(1),\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K)\n",
        "\n",
        "    return S\n",
        "\n",
        "\n",
        "def softmax_triton(S: torch.Tensor, mask: torch.Tensor | None) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute P = softmax(S + mask) row-wise.\n",
        "    S: [N, N]\n",
        "    mask: [N, N] additive mask (0 or -inf). If None, no mask.\n",
        "    Returns:\n",
        "      P: [N, N] (same dtype as S or fp16/fp32 choice)\n",
        "    \"\"\"\n",
        "    _assert_cuda(S, \"S\")\n",
        "    N, N2 = S.shape\n",
        "    assert N == N2\n",
        "\n",
        "    if mask is not None:\n",
        "        _assert_cuda(mask, \"mask\")\n",
        "        assert mask.shape == (N, N)\n",
        "\n",
        "    P = torch.empty_like(S)\n",
        "\n",
        "    # TODO:\n",
        "    # - Choose BLOCK_N\n",
        "    # - grid = (N,) one program per row (or per row-block)\n",
        "    # - HAS_MASK constexpr\n",
        "    # - Call softmax_row_kernel[grid](...)\n",
        "    # raise NotImplementedError\n",
        "    if N <= 128: BLOCK_N=128\n",
        "    elif N <= 256: BLOCK_N=256\n",
        "    elif N <= 512: BLOCK_N=512\n",
        "    elif N <= 1024: BLOCK_N=1024\n",
        "    else:\n",
        "        raise ValueError(f\"Naive row-softmax requires N <= 1024, got N={N}\")\n",
        "    grid = (N,)\n",
        "\n",
        "    HAS_MASK = mask is not None\n",
        "    m_ptr = mask if HAS_MASK else S\n",
        "    stride_mn = mask.stride(0) if HAS_MASK else 0\n",
        "    stride_mm = mask.stride(1) if HAS_MASK else 0\n",
        "\n",
        "\n",
        "    softmax_row_kernel[grid](\n",
        "    S, m_ptr, P,\n",
        "    N,\n",
        "    S.stride(0), S.stride(1),\n",
        "    stride_mn, stride_mm,\n",
        "    P.stride(0), P.stride(1),\n",
        "    BLOCK_N=BLOCK_N,\n",
        "    HAS_MASK=HAS_MASK,\n",
        ")\n",
        "\n",
        "\n",
        "    return P\n",
        "\n",
        "\n",
        "def pv_triton(P: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute O = P @ V\n",
        "    P: [N, N]\n",
        "    V: [N, D]\n",
        "    Returns:\n",
        "      O: [N, D]\n",
        "    \"\"\"\n",
        "    _assert_cuda(P, \"P\")\n",
        "    _assert_cuda(V, \"V\")\n",
        "    N, N2 = P.shape\n",
        "    assert N == N2\n",
        "    assert V.shape[0] == N\n",
        "    D = V.shape[1]\n",
        "\n",
        "    O = torch.empty((N, D), device=V.device, dtype=torch.float32)\n",
        "\n",
        "    # TODO:\n",
        "    # - Choose BLOCK_M/BLOCK_N/BLOCK_K\n",
        "    # - Define grid over O tiles\n",
        "    # - Call pv_kernel[grid](...)\n",
        "    # raise NotImplementedError\n",
        "    BLOCK_M = 128\n",
        "    BLOCK_N = 128\n",
        "    BLOCK_K = 64\n",
        "\n",
        "    grid = (\n",
        "        triton.cdiv(N, BLOCK_M),  # pid_m\n",
        "        triton.cdiv(D, BLOCK_N),  # pid_n\n",
        "    )\n",
        "\n",
        "    pv_kernel[grid](\n",
        "        P, V, O,\n",
        "        N=N, D=D,\n",
        "        stride_pn=P.stride(0), stride_pm=P.stride(1),\n",
        "        stride_vn=V.stride(0), stride_vd=V.stride(1),\n",
        "        stride_on=O.stride(0), stride_od=O.stride(1),\n",
        "        BLOCK_M=BLOCK_M,\n",
        "        BLOCK_N=BLOCK_N,\n",
        "        BLOCK_K=BLOCK_K)\n",
        "\n",
        "    return O\n",
        "\n",
        "def naive_attention_triton(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask: torch.Tensor | None = None):\n",
        "    \"\"\"\n",
        "    Full naive attention:\n",
        "      S = QK^T\n",
        "      P = softmax(S + mask)\n",
        "      O = P V\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # - Call qk_t_triton\n",
        "    # - Call softmax_triton\n",
        "    # - Call pv_triton\n",
        "    # raise NotImplementedError\n",
        "    _assert_cuda(Q, \"Q\")\n",
        "    _assert_cuda(K, \"K\")\n",
        "    _assert_cuda(V, \"V\")\n",
        "    assert Q.shape == K.shape, \"Q and K must have shape [N, D]\"\n",
        "    assert Q.shape == V.shape, \"For this toy naive version, assume V has shape [N, D]\"\n",
        "    N, D = Q.shape\n",
        "\n",
        "    if mask is not None:\n",
        "        _assert_cuda(mask, \"mask\")\n",
        "        assert mask.shape == (N, N), \"mask must be [N, N] additive mask (0 / -inf)\"\n",
        "\n",
        "    # 1) Scores: S = Q @ K^T   -> [N, N] (often fp32)\n",
        "    S = qk_t_triton(Q, K)\n",
        "\n",
        "    # 2) Probabilities: P = softmax(S + mask)  -> [N, N]\n",
        "    P = softmax_triton(S, mask)\n",
        "\n",
        "    # 3) Output: O = P @ V     -> [N, D]\n",
        "    O = pv_triton(P, V)\n",
        "\n",
        "    return O\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PyTorch reference & correctness checks (NO SOLUTION)\n",
        "# ============================================================\n",
        "def naive_attention_torch(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask: torch.Tensor | None = None):\n",
        "    \"\"\"\n",
        "    Reference implementation in PyTorch:\n",
        "      attn = softmax(Q @ K.T + mask) @ V\n",
        "    \"\"\"\n",
        "    # TODO: implement torch reference (use float32 accumulation if needed)\n",
        "    # raise NotImplementedError\n",
        "    assert Q.shape == K.shape == V.shape, \"This toy reference assumes Q,K,V are all [N, D]\"\n",
        "    N, D = Q.shape\n",
        "\n",
        "    # Use fp32 for scores/softmax stability, regardless of input dtype\n",
        "    Qf = Q.to(torch.float32)\n",
        "    Kf = K.to(torch.float32)\n",
        "    Vf = V.to(torch.float32)\n",
        "\n",
        "    scores = Qf @ Kf.transpose(0, 1)  # [N, N]\n",
        "\n",
        "    if mask is not None:\n",
        "        assert mask.shape == (N, N), f\"mask must be [N, N], got {mask.shape}\"\n",
        "        scores = scores + mask.to(torch.float32)\n",
        "\n",
        "    P = torch.softmax(scores, dim=-1)  # row-wise softmax\n",
        "    O = P @ Vf  # [N, D]\n",
        "\n",
        "    return O\n",
        "\n",
        "@torch.no_grad()\n",
        "def check_correctness(device=\"cuda\", dtype=torch.float16, N=256, D=64, use_mask=True):\n",
        "    torch.manual_seed(0)\n",
        "    Q = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    K = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    V = torch.randn((N, D), device=device, dtype=dtype)\n",
        "\n",
        "    mask = None\n",
        "    if use_mask:\n",
        "        # TODO: create a causal mask (or padding mask)\n",
        "        mask = _make_additive_causal_mask(N, device=device, dtype=torch.float32)\n",
        "\n",
        "    # TODO:\n",
        "    # - Run torch reference\n",
        "    # - Run triton naive attention\n",
        "    # - Compare max/mean error\n",
        "    # raise NotImplementedError\n",
        "    # Reference (PyTorch)\n",
        "    out_ref = naive_attention_torch(Q, K, V, mask=mask)\n",
        "\n",
        "    # Triton naive\n",
        "    out_tri = naive_attention_triton(Q, K, V, mask=mask)\n",
        "\n",
        "    # Compare (cast both to fp32 for fair error)\n",
        "    diff = (out_tri.to(torch.float32) - out_ref.to(torch.float32)).abs()\n",
        "    max_err = diff.max().item()\n",
        "    mean_err = diff.mean().item()\n",
        "    rmse = torch.sqrt((diff * diff).mean()).item()\n",
        "\n",
        "    print(f\"[check_correctness] N={N}, D={D}, dtype={dtype}, use_mask={use_mask}\")\n",
        "    print(f\"  max_abs_err : {max_err:.6e}\")\n",
        "    print(f\"  mean_abs_err: {mean_err:.6e}\")\n",
        "    print(f\"  rmse        : {rmse:.6e}\")\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def quick_bench(device=\"cuda\", dtype=torch.float16, N=1024, D=64, iters=50, warmup=10, use_mask=False):\n",
        "    \"\"\"\n",
        "    Simple benchmark harness (intentionally minimal).\n",
        "    \"\"\"\n",
        "    Q = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    K = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    V = torch.randn((N, D), device=device, dtype=dtype)\n",
        "\n",
        "    mask = None\n",
        "    if use_mask:\n",
        "        # TODO: create mask\n",
        "        mask = _make_additive_causal_mask(N, device=device, dtype=torch.float32)\n",
        "\n",
        "    # TODO:\n",
        "    # - Warmup runs\n",
        "    # - Time with CUDA events\n",
        "    # - Print ms/iter for torch vs triton\n",
        "    # raise NotImplementedError\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    Q = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    K = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    V = torch.randn((N, D), device=device, dtype=dtype)\n",
        "\n",
        "    mask = None\n",
        "    if use_mask:\n",
        "        # Additive causal mask: 0 or -inf\n",
        "        mask = _make_additive_causal_mask(N, device=device, dtype=torch.float32)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Warmup\n",
        "    # ----------------------------\n",
        "    for _ in range(warmup):\n",
        "        naive_attention_triton(Q, K, V, mask=mask)\n",
        "        naive_attention_torch(Q, K, V, mask=mask)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # ----------------------------\n",
        "    # Benchmark Triton\n",
        "    # ----------------------------\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    start.record()\n",
        "    for _ in range(iters):\n",
        "        naive_attention_triton(Q, K, V, mask=mask)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    triton_ms = start.elapsed_time(end) / iters\n",
        "\n",
        "    # ----------------------------\n",
        "    # Benchmark Torch\n",
        "    # ----------------------------\n",
        "    start.record()\n",
        "    for _ in range(iters):\n",
        "        naive_attention_torch(Q, K, V, mask=mask)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    torch_ms = start.elapsed_time(end) / iters\n",
        "\n",
        "    speedup = torch_ms / triton_ms\n",
        "\n",
        "    print(f\"\\n[quick_bench]\")\n",
        "    print(f\"N={N}, D={D}, dtype={dtype}, mask={use_mask}\")\n",
        "    print(f\"Triton: {triton_ms:.3f} ms\")\n",
        "    print(f\"Torch : {torch_ms:.3f} ms\")\n",
        "    print(f\"Speedup (Torch/Triton): {speedup:.2f}x\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: run correctness + small bench\n",
        "    check_correctness(N=128, D=64, use_mask=True)\n",
        "    quick_bench(N=1024, D=64, use_mask=False)\n",
        "    # pass\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# triton_naive_attention_skeleton.py\n",
        "# ============================================================\n",
        "# Day 2 ‚Äî Triton Naive Attention Kernel (NO SOLUTION)\n",
        "# Goal:\n",
        "  #improvement:1. change qk_t_kernel to uncoalscing reading on K demension(lower performance)\n",
        "  # 2. softmax change the accuracy to bf16 0.04x to 0.14x\n",
        "  # 3. softmax 2 pass kernel Pass1: row maxÔºõPass2: sum(exp) + store(lower performance)\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Utilities\n",
        "# -----------------------------\n",
        "def _assert_cuda(x: torch.Tensor, name: str):\n",
        "    if not x.is_cuda:\n",
        "        raise ValueError(f\"{name} must be on CUDA, got {x.device}\")\n",
        "    if not x.is_contiguous():\n",
        "        raise ValueError(f\"{name} must be contiguous for this skeleton.\")\n",
        "\n",
        "\n",
        "def _make_additive_causal_mask(n: int, device, dtype):\n",
        "    \"\"\"\n",
        "    Returns additive mask M in shape [n, n]:\n",
        "      M[i, j] = 0 for j <= i\n",
        "      M[i, j] = -inf for j > i\n",
        "    Used as: scores = scores + M\n",
        "    \"\"\"\n",
        "    # TODO: implement causal mask creation\n",
        "    # raise NotImplementedError\n",
        "    if device is None:\n",
        "        device = \"cpu\"\n",
        "    if dtype is None:\n",
        "        dtype = torch.float32\n",
        "\n",
        "    # Use the minimum finite value for the dtype to represent \"-inf\" in practice.\n",
        "    # For fp16/bf16, true -inf exists, but using finfo.min is also common and safe.\n",
        "    # neg_inf = torch.finfo(dtype).min\n",
        "\n",
        "    # upper triangular (strictly above diagonal) => future positions\n",
        "    # shape [n, n], True where j > i\n",
        "    future = torch.triu(torch.ones((n, n), device=device, dtype=torch.bool), diagonal=1)\n",
        "\n",
        "    # start from zeros, fill future with neg_inf\n",
        "    mask = torch.zeros((n, n), device=device, dtype=dtype)\n",
        "    mask = mask.masked_fill(future, -float(\"inf\"))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def _make_additive_padding_mask(valid_lens: torch.Tensor, n: int, device, dtype):\n",
        "    \"\"\"\n",
        "    valid_lens: [B] or [n] style lengths; for this Day2 skeleton we keep it simple:\n",
        "      - Assume a single sequence length n, and valid_lens is optional.\n",
        "      - If you want per-row masking, expand to [n, n] additive mask.\n",
        "      scores = scores + padding_mask\n",
        "    \"\"\"\n",
        "    # TODO: implement padding mask (optional)\n",
        "    # raise NotImplementedError\n",
        "    if device is None:\n",
        "        device = \"cpu\"\n",
        "    if dtype is None:\n",
        "        dtype = torch.float32\n",
        "\n",
        "    neg_inf = torch.finfo(dtype).min\n",
        "\n",
        "    # assume single length\n",
        "    if valid_lens is None:\n",
        "        return torch.zeros((n, n), device=device, dtype=dtype)\n",
        "\n",
        "    L = int(valid_lens.item())\n",
        "\n",
        "    # shape [n]\n",
        "    key_positions = torch.arange(n, device=device)\n",
        "\n",
        "    # True where j >= L\n",
        "    invalid = key_positions >= L\n",
        "\n",
        "    # expand to [n, n] (each row same mask)\n",
        "    invalid = invalid.unsqueeze(0).expand(n, n)\n",
        "\n",
        "    mask = torch.zeros((n, n), device=device, dtype=dtype)\n",
        "    mask = mask.masked_fill(invalid, neg_inf)\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Kernel 1: Scores = Q @ K^T\n",
        "# Q: [N, D], K: [N, D]  => Scores: [N, N]\n",
        "# ============================================================\n",
        "@triton.jit\n",
        "def qk_t_kernel(\n",
        "    q_ptr, k_ptr, s_ptr,\n",
        "    N: tl.constexpr, D: tl.constexpr,\n",
        "    stride_qn: tl.constexpr, stride_qd: tl.constexpr,\n",
        "    stride_kn: tl.constexpr, stride_kd: tl.constexpr,\n",
        "    stride_sn: tl.constexpr, stride_sm: tl.constexpr,\n",
        "    # Tile sizes (intentionally simple / naive)\n",
        "    BLOCK_M: tl.constexpr,\n",
        "    BLOCK_N: tl.constexpr,\n",
        "    BLOCK_K: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute a tile of S = QK^T.\n",
        "    Program ids map over (rows, cols) tiles of S.\n",
        "\n",
        "    TODO:\n",
        "    - Compute pid_m, pid_n\n",
        "    - Compute row/col offsets\n",
        "    - Load Q tile [BLOCK_M, BLOCK_K]\n",
        "    - Load K tile [BLOCK_N, BLOCK_K] (note K^T => K rows act like cols)\n",
        "    - Accumulate dot products\n",
        "    - Store to S\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    tl.static_assert(BLOCK_K <= D,\n",
        "    \"BLOCK_K must be <= D for this skeleton\")\n",
        "    # raise NotImplementedError\n",
        "\n",
        "    # 2D program grid: each program handles a (BLOCK_M x BLOCK_N) tile of X\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "\n",
        "    m_mask = offs_m < N\n",
        "    n_mask = offs_n < N\n",
        "\n",
        "\n",
        "    for k0 in range(0, D, BLOCK_K):\n",
        "        d_offsets = k0 + offs_k\n",
        "        d_mask = d_offsets < D\n",
        "\n",
        "        q_ptrs = q_ptr + offs_m[:, None] * stride_qn + d_offsets[None, :] * stride_qd   # [BM, BK]\n",
        "        k_ptrs = k_ptr + offs_n[:, None] * stride_kn + d_offsets[None, :] * stride_kd  # [BN,BK]\n",
        "\n",
        "        # 2D masks for loads\n",
        "        q_load_mask = m_mask[:, None] & d_mask[None, :]    # [BM, BK]\n",
        "        k_load_mask = n_mask[:, None] & d_mask[None, :]    # [BK, BN]\n",
        "\n",
        "\n",
        "        q_tile = tl.load(q_ptrs, mask = q_load_mask, other = 0.0)\n",
        "        k_tile = tl.load(k_ptrs, mask = k_load_mask, other = 0.0)\n",
        "\n",
        "        acc += tl.dot(q_tile, tl.trans(k_tile))\n",
        "\n",
        "    s_ptrs = s_ptr + offs_m[:, None] * stride_sn + offs_n[None, :] * stride_sm\n",
        "    tl.store(s_ptrs, acc, mask = m_mask[:, None] & n_mask[None, :])\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Kernel 2: Softmax over each row of S (row-wise)\n",
        "# S: [N, N] -> P: [N, N]\n",
        "# Optional additive mask: M: [N, N] where invalid positions are -inf\n",
        "# ============================================================\n",
        "@triton.jit\n",
        "def softmax_row_kernel(\n",
        "    s_ptr, m_ptr, p_ptr,\n",
        "    N: tl.constexpr,\n",
        "    stride_sn: tl.constexpr, stride_sm: tl.constexpr,\n",
        "    stride_mn: tl.constexpr, stride_mm: tl.constexpr,\n",
        "    stride_pn: tl.constexpr, stride_pm: tl.constexpr,\n",
        "    BLOCK_N: tl.constexpr,\n",
        "    HAS_MASK: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Row-wise softmax:\n",
        "      p[i, :] = softmax(s[i, :] + mask[i, :])\n",
        "      p = s + m\n",
        "\n",
        "    TODO:\n",
        "    - Map program id to a row i\n",
        "    - Load a row block of scores\n",
        "    - If HAS_MASK, load mask and add\n",
        "    - Numerically stable softmax:\n",
        "        x = x - max(x)\n",
        "        exp = tl.exp(x)\n",
        "        denom = tl.sum(exp)\n",
        "        p = exp / denom\n",
        "    - Store p\n",
        "\n",
        "    Notes:\n",
        "    - This skeleton assumes N can be larger than BLOCK_N; you may loop over blocks\n",
        "      or restrict this Day2 to N <= BLOCK_N initially.\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    # 2-pass row-wise softmax for a single row per program.\n",
        "\n",
        "    # Pass 1: row_max = max_j (s[row, j] + mask[row, j])\n",
        "    # Pass 2: row_sum = sum_j exp(s[row, j] + mask[row, j] - row_max)\n",
        "    #         write p[row, j] = exp(...) / row_sum\n",
        "    pid_row = tl.program_id(0)\n",
        "    # ----------------------------\n",
        "    # Pass 1: compute row max\n",
        "    # ----------------------------\n",
        "    row_max = tl.full((), -float(\"inf\"), tl.float32)\n",
        "\n",
        "    # loop over columns in blocks\n",
        "    for c0 in range(0, N, BLOCK_N):\n",
        "        offs = c0 + tl.arange(0, BLOCK_N)\n",
        "        col_mask = offs < N\n",
        "\n",
        "        s_row_ptr = s_ptr + pid_row * stride_sn + offs * stride_sm\n",
        "        x = tl.load(s_row_ptr, mask=col_mask, other=-float(\"inf\")).to(tl.float32)\n",
        "\n",
        "        if HAS_MASK:\n",
        "            m_row_ptr = m_ptr + pid_row * stride_mn + offs * stride_mm\n",
        "            m = tl.load(m_row_ptr, mask=col_mask, other=0.0).to(tl.float32)\n",
        "            x = x + m\n",
        "\n",
        "        block_max = tl.max(x, axis=0)\n",
        "        row_max = tl.maximum(row_max, block_max)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Pass 2: compute sum(exp) and write output\n",
        "    # ----------------------------\n",
        "    row_sum = tl.zeros((), dtype=tl.float32)\n",
        "\n",
        "    # 2a) sum\n",
        "    for c0 in range(0, N, BLOCK_N):\n",
        "        offs = c0 + tl.arange(0, BLOCK_N)\n",
        "        col_mask = offs < N\n",
        "\n",
        "        s_row_ptr = s_ptr + pid_row * stride_sn + offs * stride_sm\n",
        "        x = tl.load(s_row_ptr, mask=col_mask, other=-float(\"inf\")).to(tl.float32)\n",
        "\n",
        "        if HAS_MASK:\n",
        "            m_row_ptr = m_ptr + pid_row * stride_mn + offs * stride_mm\n",
        "            m = tl.load(m_row_ptr, mask=col_mask, other=0.0).to(tl.float32)\n",
        "            x = x + m\n",
        "\n",
        "        x = x - row_max\n",
        "        exp_x = tl.exp(x)\n",
        "        row_sum += tl.sum(exp_x, axis=0)\n",
        "\n",
        "    # 2b) write\n",
        "    inv_sum = 1.0 / row_sum\n",
        "    for c0 in range(0, N, BLOCK_N):\n",
        "        offs = c0 + tl.arange(0, BLOCK_N)\n",
        "        col_mask = offs < N\n",
        "\n",
        "        s_row_ptr = s_ptr + pid_row * stride_sn + offs * stride_sm\n",
        "        x = tl.load(s_row_ptr, mask=col_mask, other=-float(\"inf\")).to(tl.float32)\n",
        "\n",
        "        if HAS_MASK:\n",
        "            m_row_ptr = m_ptr + pid_row * stride_mn + offs * stride_mm\n",
        "            m = tl.load(m_row_ptr, mask=col_mask, other=0.0).to(tl.float32)\n",
        "            x = x + m\n",
        "\n",
        "        x = x - row_max\n",
        "        p = tl.exp(x) * inv_sum  # fp32\n",
        "\n",
        "        p_row_ptr = p_ptr + pid_row * stride_pn + offs * stride_pm\n",
        "        tl.store(p_row_ptr, p.to(tl.float16), mask=col_mask)\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Kernel 3: Out = P @ V\n",
        "# P: [N, N], V: [N, D] -> O: [N, D]\n",
        "# ============================================================\n",
        "@triton.jit\n",
        "def pv_kernel(\n",
        "    p_ptr, v_ptr, o_ptr,\n",
        "    N: tl.constexpr, D: tl.constexpr,\n",
        "    stride_pn: tl.constexpr, stride_pm: tl.constexpr,\n",
        "    stride_vn: tl.constexpr, stride_vd: tl.constexpr,\n",
        "    stride_on: tl.constexpr, stride_od: tl.constexpr,\n",
        "    BLOCK_M: tl.constexpr,\n",
        "    BLOCK_N: tl.constexpr,\n",
        "    BLOCK_K: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute a tile of O = P V.\n",
        "\n",
        "    TODO:\n",
        "    - Program ids over (rows of O, cols of O)\n",
        "    - Load P tile [BLOCK_M, BLOCK_K]\n",
        "    - Load V tile [BLOCK_K, BLOCK_N] (here K dimension is N of P / V rows)\n",
        "    - Accumulate\n",
        "    - Store to O\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    # raise NotImplementedError\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "    m_mask = offs_m < N\n",
        "    n_mask = offs_n < D\n",
        "\n",
        "    for k0 in tl.static_range(0, N, BLOCK_K):\n",
        "          # current K indices for this chunk\n",
        "          k_offsets = k0 + offs_k\n",
        "          k_mask = k_offsets < N\n",
        "\n",
        "          # build pointer grids for this chunk\n",
        "          p_ptrs = p_ptr + offs_m[:, None] * stride_pn + k_offsets[None, :] * stride_pm\n",
        "          v_ptrs = v_ptr + k_offsets[:, None] * stride_vn + offs_n[None, :] * stride_vd\n",
        "\n",
        "          # 2D masks for loads\n",
        "          p_load_mask = m_mask[:, None] & k_mask[None, :]\n",
        "          v_load_mask = k_mask[:, None] & n_mask[None, :]\n",
        "\n",
        "          # masked loads: out-of-bounds => 0\n",
        "          p_tile = tl.load(p_ptrs, mask=p_load_mask, other=0).to(tl.float32)\n",
        "          v_tile = tl.load(v_ptrs, mask=v_load_mask, other=0).to(tl.float32)\n",
        "\n",
        "          # accumulate (fp32)\n",
        "          # tl.dot will typically accumulate in fp32 when acc is fp32\n",
        "          acc += tl.dot(p_tile, v_tile)\n",
        "\n",
        "    o_tile = acc\n",
        "    o_ptrs = o_ptr + offs_m[:, None] * stride_on + offs_n[None, :] * stride_od\n",
        "    tl.store(o_ptrs, o_tile, mask=m_mask[:,None] & n_mask[None,:])\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Launchers (NO SOLUTION)\n",
        "# ============================================================\n",
        "def qk_t_triton(Q: torch.Tensor, K: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute S = Q @ K^T\n",
        "    Q, K: [N, D] contiguous CUDA tensors\n",
        "    Returns:\n",
        "      S: [N, N]\n",
        "    \"\"\"\n",
        "    _assert_cuda(Q, \"Q\")\n",
        "    _assert_cuda(K, \"K\")\n",
        "    assert Q.shape == K.shape\n",
        "    N, D = Q.shape\n",
        "\n",
        "    S = torch.empty((N, N), device=Q.device, dtype=torch.float32)  # scores typically fp32\n",
        "\n",
        "    # TODO:\n",
        "    # - Choose BLOCK_M/BLOCK_N/BLOCK_K (naive defaults)\n",
        "    # - Define grid mapping over tiles\n",
        "    # - Call qk_t_kernel[grid](...)\n",
        "    # raise NotImplementedError\n",
        "    BLOCK_M = 128\n",
        "    BLOCK_N = 128\n",
        "    BLOCK_K = 64\n",
        "    # if N > BLOCK_N:\n",
        "    #     raise ValueError(\"Naive row-softmax requires N <= BLOCK_N\")\n",
        "\n",
        "    N, D = Q.shape\n",
        "    N2, D2 = K.shape\n",
        "    assert N == N2, \"Q and K must have the same sequence length\"\n",
        "    assert D == D2, \"Q and K must have the same head dimension\"\n",
        "\n",
        "    grid = (\n",
        "        triton.cdiv(N, BLOCK_M),\n",
        "        triton.cdiv(N, BLOCK_N),\n",
        "    )\n",
        "\n",
        "    qk_t_kernel[grid](\n",
        "        Q, K, S,\n",
        "        N = N, D = D,\n",
        "        stride_qn=Q.stride(0), stride_qd=Q.stride(1),\n",
        "        stride_kn=K.stride(0), stride_kd=K.stride(1),\n",
        "        stride_sn=S.stride(0), stride_sm=S.stride(1),\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K)\n",
        "\n",
        "    return S\n",
        "\n",
        "\n",
        "def softmax_triton(S: torch.Tensor, mask: torch.Tensor | None) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute P = softmax(S + mask) row-wise.\n",
        "    S: [N, N]\n",
        "    mask: [N, N] additive mask (0 or -inf). If None, no mask.\n",
        "    Returns:\n",
        "      P: [N, N] (same dtype as S or fp16/fp32 choice)\n",
        "    \"\"\"\n",
        "    _assert_cuda(S, \"S\")\n",
        "    N, N2 = S.shape\n",
        "    assert N == N2\n",
        "\n",
        "    if mask is not None:\n",
        "        _assert_cuda(mask, \"mask\")\n",
        "        assert mask.shape == (N, N)\n",
        "\n",
        "    # P = torch.empty_like(S)\n",
        "    P = torch.empty((N, N), device=S.device, dtype=torch.float16)\n",
        "\n",
        "    # TODO:\n",
        "    # - Choose BLOCK_N\n",
        "    # - grid = (N,) one program per row (or per row-block)\n",
        "    # - HAS_MASK constexpr\n",
        "    # - Call softmax_row_kernel[grid](...)\n",
        "    # raise NotImplementedError\n",
        "    if N <= 128: BLOCK_N=128\n",
        "    elif N <= 256: BLOCK_N=256\n",
        "    elif N <= 512: BLOCK_N=512\n",
        "    elif N <= 1024: BLOCK_N=1024\n",
        "    else:\n",
        "        raise ValueError(f\"Naive row-softmax requires N <= 1024, got N={N}\")\n",
        "    grid = (N,)\n",
        "\n",
        "    HAS_MASK = mask is not None\n",
        "    m_ptr = mask if HAS_MASK else S\n",
        "    stride_mn = mask.stride(0) if HAS_MASK else 0\n",
        "    stride_mm = mask.stride(1) if HAS_MASK else 0\n",
        "\n",
        "\n",
        "    softmax_row_kernel[grid](\n",
        "    S, m_ptr, P,\n",
        "    N,\n",
        "    S.stride(0), S.stride(1),\n",
        "    stride_mn, stride_mm,\n",
        "    P.stride(0), P.stride(1),\n",
        "    BLOCK_N=BLOCK_N,\n",
        "    HAS_MASK=HAS_MASK,\n",
        ")\n",
        "\n",
        "\n",
        "    return P\n",
        "\n",
        "\n",
        "def pv_triton(P: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute O = P @ V\n",
        "    P: [N, N]\n",
        "    V: [N, D]\n",
        "    Returns:\n",
        "      O: [N, D]\n",
        "    \"\"\"\n",
        "    _assert_cuda(P, \"P\")\n",
        "    _assert_cuda(V, \"V\")\n",
        "    N, N2 = P.shape\n",
        "    assert N == N2\n",
        "    assert V.shape[0] == N\n",
        "    D = V.shape[1]\n",
        "\n",
        "    O = torch.empty((N, D), device=V.device, dtype=torch.float32)\n",
        "\n",
        "    # TODO:\n",
        "    # - Choose BLOCK_M/BLOCK_N/BLOCK_K\n",
        "    # - Define grid over O tiles\n",
        "    # - Call pv_kernel[grid](...)\n",
        "    # raise NotImplementedError\n",
        "    BLOCK_M = 128\n",
        "    BLOCK_N = 128\n",
        "    BLOCK_K = 64\n",
        "\n",
        "    grid = (\n",
        "        triton.cdiv(N, BLOCK_M),  # pid_m\n",
        "        triton.cdiv(D, BLOCK_N),  # pid_n\n",
        "    )\n",
        "\n",
        "    pv_kernel[grid](\n",
        "        P, V, O,\n",
        "        N=N, D=D,\n",
        "        stride_pn=P.stride(0), stride_pm=P.stride(1),\n",
        "        stride_vn=V.stride(0), stride_vd=V.stride(1),\n",
        "        stride_on=O.stride(0), stride_od=O.stride(1),\n",
        "        BLOCK_M=BLOCK_M,\n",
        "        BLOCK_N=BLOCK_N,\n",
        "        BLOCK_K=BLOCK_K)\n",
        "\n",
        "    return O\n",
        "\n",
        "def naive_attention_triton(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask: torch.Tensor | None = None):\n",
        "    \"\"\"\n",
        "    Full naive attention:\n",
        "      S = QK^T\n",
        "      P = softmax(S + mask)\n",
        "      O = P V\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # - Call qk_t_triton\n",
        "    # - Call softmax_triton\n",
        "    # - Call pv_triton\n",
        "    # raise NotImplementedError\n",
        "    _assert_cuda(Q, \"Q\")\n",
        "    _assert_cuda(K, \"K\")\n",
        "    _assert_cuda(V, \"V\")\n",
        "    assert Q.shape == K.shape, \"Q and K must have shape [N, D]\"\n",
        "    assert Q.shape == V.shape, \"For this toy naive version, assume V has shape [N, D]\"\n",
        "    N, D = Q.shape\n",
        "\n",
        "    if mask is not None:\n",
        "        _assert_cuda(mask, \"mask\")\n",
        "        assert mask.shape == (N, N), \"mask must be [N, N] additive mask (0 / -inf)\"\n",
        "\n",
        "    # 1) Scores: S = Q @ K^T   -> [N, N] (often fp32)\n",
        "    S = qk_t_triton(Q, K)\n",
        "\n",
        "    # 2) Probabilities: P = softmax(S + mask)  -> [N, N]\n",
        "    P = softmax_triton(S, mask)\n",
        "\n",
        "    # 3) Output: O = P @ V     -> [N, D]\n",
        "    O = pv_triton(P, V)\n",
        "\n",
        "    return O\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PyTorch reference & correctness checks (NO SOLUTION)\n",
        "# ============================================================\n",
        "def naive_attention_torch(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask: torch.Tensor | None = None):\n",
        "    \"\"\"\n",
        "    Reference implementation in PyTorch:\n",
        "      attn = softmax(Q @ K.T + mask) @ V\n",
        "    \"\"\"\n",
        "    # TODO: implement torch reference (use float32 accumulation if needed)\n",
        "    # raise NotImplementedError\n",
        "    assert Q.shape == K.shape == V.shape, \"This toy reference assumes Q,K,V are all [N, D]\"\n",
        "    N, D = Q.shape\n",
        "\n",
        "    # Use fp32 for scores/softmax stability, regardless of input dtype\n",
        "    Qf = Q.to(torch.float32)\n",
        "    Kf = K.to(torch.float32)\n",
        "    Vf = V.to(torch.float32)\n",
        "\n",
        "    scores = Qf @ Kf.transpose(0, 1)  # [N, N]\n",
        "\n",
        "    if mask is not None:\n",
        "        assert mask.shape == (N, N), f\"mask must be [N, N], got {mask.shape}\"\n",
        "        scores = scores + mask.to(torch.float32)\n",
        "\n",
        "    P = torch.softmax(scores, dim=-1)  # row-wise softmax\n",
        "    O = P @ Vf  # [N, D]\n",
        "\n",
        "    return O\n",
        "\n",
        "@torch.no_grad()\n",
        "def check_correctness(device=\"cuda\", dtype=torch.float16, N=256, D=64, use_mask=True):\n",
        "    torch.manual_seed(0)\n",
        "    Q = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    K = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    V = torch.randn((N, D), device=device, dtype=dtype)\n",
        "\n",
        "    mask = None\n",
        "    if use_mask:\n",
        "        # TODO: create a causal mask (or padding mask)\n",
        "        mask = _make_additive_causal_mask(N, device=device, dtype=torch.float32)\n",
        "\n",
        "    # TODO:\n",
        "    # - Run torch reference\n",
        "    # - Run triton naive attention\n",
        "    # - Compare max/mean error\n",
        "    # raise NotImplementedError\n",
        "    # Reference (PyTorch)\n",
        "    out_ref = naive_attention_torch(Q, K, V, mask=mask)\n",
        "\n",
        "    # Triton naive\n",
        "    out_tri = naive_attention_triton(Q, K, V, mask=mask)\n",
        "\n",
        "    # Compare (cast both to fp32 for fair error)\n",
        "    diff = (out_tri.to(torch.float32) - out_ref.to(torch.float32)).abs()\n",
        "    max_err = diff.max().item()\n",
        "    mean_err = diff.mean().item()\n",
        "    rmse = torch.sqrt((diff * diff).mean()).item()\n",
        "\n",
        "    print(f\"[check_correctness] N={N}, D={D}, dtype={dtype}, use_mask={use_mask}\")\n",
        "    print(f\"  max_abs_err : {max_err:.6e}\")\n",
        "    print(f\"  mean_abs_err: {mean_err:.6e}\")\n",
        "    print(f\"  rmse        : {rmse:.6e}\")\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def quick_bench(device=\"cuda\", dtype=torch.float16, N=1024, D=64, iters=50, warmup=10, use_mask=False):\n",
        "    \"\"\"\n",
        "    Simple benchmark harness (intentionally minimal).\n",
        "    \"\"\"\n",
        "    Q = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    K = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    V = torch.randn((N, D), device=device, dtype=dtype)\n",
        "\n",
        "    mask = None\n",
        "    if use_mask:\n",
        "        # TODO: create mask\n",
        "        mask = _make_additive_causal_mask(N, device=device, dtype=torch.float32)\n",
        "\n",
        "    # TODO:\n",
        "    # - Warmup runs\n",
        "    # - Time with CUDA events\n",
        "    # - Print ms/iter for torch vs triton\n",
        "    # raise NotImplementedError\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    Q = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    K = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    V = torch.randn((N, D), device=device, dtype=dtype)\n",
        "\n",
        "    mask = None\n",
        "    if use_mask:\n",
        "        # Additive causal mask: 0 or -inf\n",
        "        mask = _make_additive_causal_mask(N, device=device, dtype=torch.float32)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Warmup\n",
        "    # ----------------------------\n",
        "    for _ in range(warmup):\n",
        "        naive_attention_triton(Q, K, V, mask=mask)\n",
        "        naive_attention_torch(Q, K, V, mask=mask)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # ----------------------------\n",
        "    # Benchmark Triton\n",
        "    # ----------------------------\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    start.record()\n",
        "    for _ in range(iters):\n",
        "        naive_attention_triton(Q, K, V, mask=mask)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    triton_ms = start.elapsed_time(end) / iters\n",
        "\n",
        "    # ----------------------------\n",
        "    # Benchmark Torch\n",
        "    # ----------------------------\n",
        "    start.record()\n",
        "    for _ in range(iters):\n",
        "        naive_attention_torch(Q, K, V, mask=mask)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    torch_ms = start.elapsed_time(end) / iters\n",
        "\n",
        "    speedup = torch_ms / triton_ms\n",
        "\n",
        "    print(f\"\\n[quick_bench]\")\n",
        "    print(f\"N={N}, D={D}, dtype={dtype}, mask={use_mask}\")\n",
        "    print(f\"Triton: {triton_ms:.3f} ms\")\n",
        "    print(f\"Torch : {torch_ms:.3f} ms\")\n",
        "    print(f\"Speedup (Torch/Triton): {speedup:.2f}x\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # TODO: run correctness + small bench\n",
        "    check_correctness(N=128, D=64, use_mask=True)\n",
        "    quick_bench(N=1024, D=64, use_mask=False)\n",
        "    # pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCE6hjmjwJaz",
        "outputId": "57c2984a-4256-4298-a8d6-ec650768a187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[check_correctness] N=128, D=64, dtype=torch.float16, use_mask=True\n",
            "  max_abs_err : 7.545948e-04\n",
            "  mean_abs_err: 1.005516e-04\n",
            "  rmse        : 1.451281e-04\n",
            "\n",
            "[quick_bench]\n",
            "N=1024, D=64, dtype=torch.float16, mask=False\n",
            "Triton: 4.250 ms\n",
            "Torch : 0.186 ms\n",
            "Speedup (Torch/Triton): 0.04x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCKUF3QM8pml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6932150-d48b-4145-d6b5-602487c4dbb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# Day 3 Results (Naive Triton vs Torch)\n",
            "\n",
            "| impl | N | D | cfg | ms | GFLOP/s | est_GB/s | speedup_vs_torch |\n",
            "|------|---|---|-----|----|---------|----------|------------------|\n",
            "| torch | 256 | 64 | - | 0.1146 | 146.35 | 5.72 | 1.00x |\n",
            "| naive_triton | 256 | 64 | BM=64,BN=64,BK=32,SB=256,w=4 | 1.4813 | 11.33 | 0.44 | 0.06x |\n",
            "| torch | 256 | 64 | - | 0.0969 | 173.08 | 6.76 | 1.00x |\n",
            "| naive_triton | 256 | 64 | BM=128,BN=64,BK=32,SB=256,w=4 | 1.3891 | 12.08 | 0.47 | 0.07x |\n",
            "| torch | 256 | 64 | - | 0.0909 | 184.47 | 7.21 | 1.00x |\n",
            "| naive_triton | 256 | 64 | BM=64,BN=128,BK=32,SB=512,w=8 | 1.3529 | 12.40 | 0.48 | 0.07x |\n",
            "| torch | 512 | 64 | - | 0.0928 | 722.82 | 25.41 | 1.00x |\n",
            "| naive_triton | 512 | 64 | BM=64,BN=64,BK=32,SB=256,w=4 | 2.2811 | 29.42 | 1.03 | 0.04x |\n",
            "| torch | 512 | 64 | - | 0.1242 | 540.42 | 19.00 | 1.00x |\n",
            "| naive_triton | 512 | 64 | BM=128,BN=64,BK=32,SB=256,w=4 | 1.3115 | 51.17 | 1.80 | 0.07x |\n",
            "| torch | 512 | 64 | - | 0.0913 | 734.81 | 25.83 | 1.00x |\n",
            "| naive_triton | 512 | 64 | BM=64,BN=128,BK=32,SB=512,w=8 | 1.3059 | 51.39 | 1.81 | 0.07x |\n",
            "| torch | 1024 | 64 | - | 0.1238 | 2168.87 | 72.01 | 1.00x |\n",
            "| naive_triton | 1024 | 64 | BM=64,BN=64,BK=32,SB=256,w=4 | 3.6040 | 74.48 | 2.47 | 0.03x |\n",
            "| torch | 1024 | 64 | - | 0.1254 | 2140.72 | 71.08 | 1.00x |\n",
            "| naive_triton | 1024 | 64 | BM=128,BN=64,BK=32,SB=256,w=4 | 3.6125 | 74.31 | 2.47 | 0.03x |\n",
            "| torch | 1024 | 64 | - | 0.1251 | 2145.24 | 71.23 | 1.00x |\n",
            "| naive_triton | 1024 | 64 | BM=64,BN=128,BK=32,SB=512,w=8 | 3.6227 | 74.10 | 2.46 | 0.03x |\n"
          ]
        }
      ],
      "source": [
        "# day3_profile_bottleneck_skeleton.py\n",
        "# ============================================================\n",
        "# Day 3 ‚Äî Profiling + Bottleneck Analysis (NO SOLUTION)\n",
        "#\n",
        "# Goal:\n",
        "#   Profile naive attention vs torch attention, identify bottlenecks.\n",
        "#\n",
        "# Tasks:\n",
        "#   - Nsight Compute metrics:\n",
        "#       * DRAM throughput\n",
        "#       * SM efficiency\n",
        "#       * Stall reasons\n",
        "#   - Decide bottleneck:\n",
        "#       * memory-bound?\n",
        "#       * reduction-bound?\n",
        "#   - Sweep:\n",
        "#       * different block sizes\n",
        "#       * different sequence lengths\n",
        "#\n",
        "# Outputs:\n",
        "#   - Markdown table comparing naive vs torch (printed)\n",
        "#   - Bottleneck analysis template (printed)\n",
        "#\n",
        "# Notes:\n",
        "#   - Plug in your Day2 implementations:\n",
        "#       naive_attention_triton(Q,K,V,mask,cfg)\n",
        "#       naive_attention_torch(Q,K,V,mask)\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import json\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, Dict, Any, List, Tuple\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TODO: import your Day2 implementations\n",
        "# ============================================================\n",
        "def naive_attention_triton(Q, K, V, mask=None, cfg=None):\n",
        "    # TODO: call your Triton naive attention implementation\n",
        "    # raise NotImplementedError\n",
        "    # return naive_attention_torch(Q, K, V, mask=mask)\n",
        "    _assert_cuda(Q, \"Q\")\n",
        "    _assert_cuda(K, \"K\")\n",
        "    _assert_cuda(V, \"V\")\n",
        "    assert Q.shape == K.shape, \"Q and K must have shape [N, D]\"\n",
        "    assert Q.shape == V.shape, \"For this toy naive version, assume V has shape [N, D]\"\n",
        "    N, D = Q.shape\n",
        "\n",
        "    if mask is not None:\n",
        "        _assert_cuda(mask, \"mask\")\n",
        "        assert mask.shape == (N, N), \"mask must be [N, N] additive mask (0 / -inf)\"\n",
        "\n",
        "    # 1) Scores: S = Q @ K^T   -> [N, N] (often fp32)\n",
        "    S = qk_t_triton(Q, K)\n",
        "\n",
        "    # 2) Probabilities: P = softmax(S + mask)  -> [N, N]\n",
        "    P = softmax_triton(S, mask)\n",
        "\n",
        "    # 3) Output: O = P @ V     -> [N, D]\n",
        "    O = pv_triton(P, V)\n",
        "\n",
        "    return O\n",
        "\n",
        "\n",
        "\n",
        "def naive_attention_torch(Q, K, V, mask=None):\n",
        "    # TODO: call your PyTorch reference implementation\n",
        "    # raise NotImplementedError\n",
        "    # return naive_attention_torch(Q, K, V, mask=mask)\n",
        "    assert Q.shape == K.shape == V.shape, \"This toy reference assumes Q,K,V are all [N, D]\"\n",
        "    N, D = Q.shape\n",
        "\n",
        "    # Use fp32 for scores/softmax stability, regardless of input dtype\n",
        "    Qf = Q.to(torch.float32)\n",
        "    Kf = K.to(torch.float32)\n",
        "    Vf = V.to(torch.float32)\n",
        "\n",
        "    scores = Qf @ Kf.transpose(0, 1)  # [N, N]\n",
        "\n",
        "    if mask is not None:\n",
        "        assert mask.shape == (N, N), f\"mask must be [N, N], got {mask.shape}\"\n",
        "        scores = scores + mask.to(torch.float32)\n",
        "\n",
        "    P = torch.softmax(scores, dim=-1)  # row-wise softmax\n",
        "    O = P @ Vf  # [N, D]\n",
        "\n",
        "    return O\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Config definition for block size sweep\n",
        "# ============================================================\n",
        "@dataclass(frozen=True)\n",
        "class TritonNaiveCfg:\n",
        "    BLOCK_M: int\n",
        "    BLOCK_N: int\n",
        "    BLOCK_K: int\n",
        "    SOFTMAX_BLOCK: int\n",
        "    num_warps: int = 4\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Benchmark utilities\n",
        "# ============================================================\n",
        "def cuda_time_ms(fn, iters=30, warmup=10) -> float:\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    start.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    return start.elapsed_time(end) / iters\n",
        "\n",
        "\n",
        "def gflops_qk_pv(N: int, D: int) -> float:\n",
        "    flops = 4.0 * N * N * D\n",
        "    return flops / 1e9\n",
        "\n",
        "\n",
        "def estimate_bytes(N: int, D: int, elem_bytes: int = 2) -> int:\n",
        "    # TODO: refine if using fp32 intermediate\n",
        "    qkv = 3 * N * D * elem_bytes\n",
        "    s_mat = 2 * N * N * elem_bytes\n",
        "    p_mat = 2 * N * N * elem_bytes\n",
        "    out = N * D * elem_bytes\n",
        "    return qkv + s_mat + p_mat + out\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Experiment execution\n",
        "# ============================================================\n",
        "@dataclass\n",
        "class ResultRow:\n",
        "    impl: str\n",
        "    N: int\n",
        "    D: int\n",
        "    cfg: Optional[Dict[str, Any]]\n",
        "    ms: float\n",
        "    gflops: float\n",
        "    est_gbs: float\n",
        "\n",
        "\n",
        "def run_case(N: int, D: int, cfg: TritonNaiveCfg):\n",
        "    device = \"cuda\"\n",
        "    dtype = torch.float16\n",
        "\n",
        "    Q = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    K = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    V = torch.randn((N, D), device=device, dtype=dtype)\n",
        "\n",
        "    # --- Torch baseline ---\n",
        "    def fn_torch():\n",
        "        return naive_attention_torch(Q, K, V, mask=None)\n",
        "\n",
        "    ms_torch = cuda_time_ms(fn_torch)\n",
        "    gflops = gflops_qk_pv(N, D)\n",
        "    bytes_est = estimate_bytes(N, D)\n",
        "\n",
        "    torch_row = ResultRow(\n",
        "        impl=\"torch\",\n",
        "        N=N,\n",
        "        D=D,\n",
        "        cfg=None,\n",
        "        ms=ms_torch,\n",
        "        gflops=gflops / (ms_torch / 1e3),\n",
        "        est_gbs=(bytes_est / (ms_torch / 1e3)) / 1e9,\n",
        "    )\n",
        "\n",
        "    # --- Triton naive ---\n",
        "    def fn_triton():\n",
        "        return naive_attention_triton(Q, K, V, mask=None, cfg=cfg)\n",
        "\n",
        "    ms_triton = cuda_time_ms(fn_triton)\n",
        "\n",
        "    triton_row = ResultRow(\n",
        "        impl=\"naive_triton\",\n",
        "        N=N,\n",
        "        D=D,\n",
        "        cfg=asdict(cfg),\n",
        "        ms=ms_triton,\n",
        "        gflops=gflops / (ms_triton / 1e3),\n",
        "        est_gbs=(bytes_est / (ms_triton / 1e3)) / 1e9,\n",
        "    )\n",
        "\n",
        "    return torch_row, triton_row\n",
        "\n",
        "\n",
        "def sweep(seq_lens: List[int], D: int, cfgs: List[TritonNaiveCfg]):\n",
        "    rows: List[ResultRow] = []\n",
        "    for N in seq_lens:\n",
        "        for cfg in cfgs:\n",
        "            torch_row, triton_row = run_case(N, D, cfg)\n",
        "            rows.append(torch_row)\n",
        "            rows.append(triton_row)\n",
        "    return rows\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Output formatting\n",
        "# ============================================================\n",
        "def print_markdown_table(rows: List[ResultRow]):\n",
        "    print(\"\\n# Day 3 Results (Naive Triton vs Torch)\\n\")\n",
        "    print(\"| impl | N | D | cfg | ms | GFLOP/s | est_GB/s | speedup_vs_torch |\")\n",
        "    print(\"|------|---|---|-----|----|---------|----------|------------------|\")\n",
        "\n",
        "    torch_map = {(r.N, r.D): r.ms for r in rows if r.impl == \"torch\"}\n",
        "\n",
        "    for r in rows:\n",
        "        base = torch_map.get((r.N, r.D), None)\n",
        "        speedup = base / r.ms if (base and r.impl != \"torch\") else 1.0\n",
        "\n",
        "        cfg_str = \"-\"\n",
        "        if r.cfg:\n",
        "            cfg_str = f\"BM={r.cfg['BLOCK_M']},BN={r.cfg['BLOCK_N']},BK={r.cfg['BLOCK_K']},SB={r.cfg['SOFTMAX_BLOCK']},w={r.cfg['num_warps']}\"\n",
        "\n",
        "        print(f\"| {r.impl} | {r.N} | {r.D} | {cfg_str} | \"\n",
        "              f\"{r.ms:.4f} | {r.gflops:.2f} | {r.est_gbs:.2f} | \"\n",
        "              f\"{speedup:.2f}x |\")\n",
        "\n",
        "\n",
        "def print_bottleneck():\n",
        "    print(\"\\n\\n# Bottleneck Analysis (Fill After Nsight Compute)\\n\")\n",
        "    print(\"## Nsight Compute Observations\")\n",
        "    print(\"- DRAM throughput (% peak): TODO\")\n",
        "    print(\"- SM throughput (% peak): TODO\")\n",
        "    print(\"- Dominant stall reasons:\")\n",
        "    print(\"  - long scoreboard: TODO\")\n",
        "    print(\"  - memory dependency: TODO\")\n",
        "    print(\"  - barrier: TODO\")\n",
        "    print(\"  - math pipe throttle: TODO\\n\")\n",
        "\n",
        "    print(\"## Bottleneck Classification\")\n",
        "    print(\"- [ ] Memory-bound\")\n",
        "    print(\"- [ ] Reduction-bound\")\n",
        "    print(\"- [ ] Compute-bound\\n\")\n",
        "\n",
        "    print(\"## Interpretation\")\n",
        "    print(\"- Naive attention materializes N√óN matrices.\")\n",
        "    print(\"- Softmax requires multiple passes (max/sum/normalize).\")\n",
        "    print(\"- Heavy DRAM traffic likely dominates performance.\\n\")\n",
        "\n",
        "    print(\"## Next Steps\")\n",
        "    print(\"- Tune block sizes.\")\n",
        "    print(\"- Increase arithmetic intensity.\")\n",
        "    print(\"- Consider fusion (FlashAttention).\\n\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Main\n",
        "# ============================================================\n",
        "def main():\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"CUDA required.\")\n",
        "\n",
        "    seq_lens = [256, 512, 1024]  # TODO: extend if desired\n",
        "    D = 64\n",
        "\n",
        "    cfgs = [\n",
        "        TritonNaiveCfg(64, 64, 32, 256, 4),\n",
        "        TritonNaiveCfg(128, 64, 32, 256, 4),\n",
        "        TritonNaiveCfg(64, 128, 32, 512, 8),\n",
        "    ]\n",
        "\n",
        "    rows = sweep(seq_lens, D, cfgs)\n",
        "\n",
        "    print_markdown_table(rows)\n",
        "    # print_bottleneck()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hmawQOy28pml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a4da1e-0bed-48ee-ce55-105813bc58ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Memory Stats ===\n",
            "Contiguous KV: allocated=0.262 MB, used=0.262 MB\n",
            "Paged KV     : allocated=0.524 MB, used=0.262 MB, frag=50.00%\n",
            "====================\n",
            "\n",
            "\n",
            "=== Correctness Check ===\n",
            "T=1024, D=64, block_T=16, PB=128, M=1, dtype=torch.float16, use_mask=False\n",
            "max_abs_err  = 0.000000e+00\n",
            "mean_abs_err = 0.000000e+00\n",
            "[OK] Within tolerance.\n"
          ]
        }
      ],
      "source": [
        "# day4_paged_attention_toy_skeleton.py\n",
        "# ============================================================\n",
        "# Day 4 ‚Äî PageAttention (Toy) (NO SOLUTION)\n",
        "#\n",
        "# Goal:\n",
        "#   Understand vLLM-style KV cache paging by building a toy PageAttention in Triton.\n",
        "#\n",
        "# What you'll implement (toy scope):\n",
        "#   - Two KV cache modes:\n",
        "#       (1) Contiguous KV: K,V stored as [T, D] for each sequence (single sequence toy)\n",
        "#       (2) Paged KV: K,V stored in fixed-size blocks; a block table maps logical blocks to physical blocks\n",
        "#   - A toy attention computation that reads K,V via the selected layout:\n",
        "#       out = softmax(Q @ K^T + mask) @ V\n",
        "#\n",
        "# Tasks:\n",
        "#   - Study: contiguous KV vs paged KV\n",
        "#   - Design: KV block layout + block table\n",
        "#   - Implement: Triton PageAttention (toy)\n",
        "#   - Validate correctness vs torch reference\n",
        "#   - Memory usage stats (allocated bytes, fragmentation estimate)\n",
        "#\n",
        "# Notes:\n",
        "#   - This is a skeleton with TODOs only.\n",
        "#   - Keep it SIMPLE: single-head, single sequence, fp16 inputs, fp32 accum.\n",
        "#   - You can extend later to multi-head/batch.\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import math\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, Dict, Any, Tuple\n",
        "\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Data structures\n",
        "# ============================================================\n",
        "@dataclass(frozen=True)\n",
        "class PageCfg:\n",
        "    # page/block size in tokens\n",
        "    BLOCK_T: int\n",
        "    # head dim\n",
        "    D: int\n",
        "    # number of physical blocks allocated in the KV pool\n",
        "    NUM_PHYS_BLOCKS: int\n",
        "\n",
        "    # toy kernel tiling knobs (optional)\n",
        "    BLOCK_M: int = 64      # query rows (here usually 1 query, but keep generic)\n",
        "    BLOCK_N: int = 128     # keys columns tile\n",
        "    num_warps: int = 4\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class MemStats:\n",
        "    mode: str\n",
        "    logical_T: int\n",
        "    D: int\n",
        "    block_T: int\n",
        "    num_logical_blocks: int\n",
        "    num_phys_blocks: int\n",
        "    kv_bytes_allocated: int\n",
        "    kv_bytes_used: int\n",
        "    fragmentation_bytes: int\n",
        "    fragmentation_ratio: float\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Helper: build additive causal mask (optional)\n",
        "# ============================================================\n",
        "def make_additive_causal_mask(T: int, device, dtype=torch.float32) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Returns [T, T] additive causal mask:\n",
        "      0 for j <= i\n",
        "      -inf for j > i\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    # raise NotImplementedError\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Contiguous KV layout (toy)\n",
        "# ============================================================\n",
        "def alloc_contiguous_kv(T: int, D: int, device=\"cuda\", dtype=torch.float16) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Allocate contiguous K,V as [T, D]\n",
        "    \"\"\"\n",
        "    K = torch.empty((T, D), device=device, dtype=dtype)\n",
        "    V = torch.empty((T, D), device=device, dtype=dtype)\n",
        "    return K, V\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Paged KV layout (toy)\n",
        "# ============================================================\n",
        "def alloc_paged_kv_pool(num_phys_blocks: int, block_T: int, D: int, device=\"cuda\", dtype=torch.float16):\n",
        "    \"\"\"\n",
        "    Allocate a KV pool with fixed blocks:\n",
        "      K_pool: [num_phys_blocks, block_T, D]\n",
        "      V_pool: [num_phys_blocks, block_T, D]\n",
        "    \"\"\"\n",
        "    K_pool = torch.empty((num_phys_blocks, block_T, D), device=device, dtype=dtype)\n",
        "    V_pool = torch.empty((num_phys_blocks, block_T, D), device=device, dtype=dtype)\n",
        "    return K_pool, V_pool\n",
        "\n",
        "\n",
        "def build_block_table(num_logical_blocks: int, num_phys_blocks: int, device=\"cuda\") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Block table maps logical block idx -> physical block idx:\n",
        "      block_table[lb] = pb\n",
        "\n",
        "    For toy:\n",
        "      - you can map 0..L-1 to some subset of physical blocks\n",
        "      - support non-contiguous placement to mimic fragmentation avoidance\n",
        "\n",
        "    Returns:\n",
        "      block_table: [num_logical_blocks] int32\n",
        "    \"\"\"\n",
        "    # TODO: implement mapping strategy (e.g., random perm, or identity)\n",
        "    # raise NotImplementedError\n",
        "\n",
        "    assert num_logical_blocks <= num_phys_blocks, \"num_logical_blocks must be <= num_phys_blocks\"\n",
        "    assert num_logical_blocks >= 0, \"num_logical_blocks must be >= 0\"\n",
        "    assert num_phys_blocks > 0, \"num_phys_blocks must be > 0\"\n",
        "\n",
        "    if num_logical_blocks == 0:\n",
        "        return torch.empty((0,), device=device, dtype=torch.int32)\n",
        "\n",
        "    # random non-contiguous mapping (deterministic if you set torch.manual_seed outside)\n",
        "    perm = torch.randperm(num_phys_blocks, device=device, dtype=torch.int64)\n",
        "    block_table = perm[:num_logical_blocks].to(torch.int32)\n",
        "\n",
        "    return block_table\n",
        "\n",
        "\n",
        "def write_tokens_to_paged_kv(\n",
        "    K_tokens: torch.Tensor, V_tokens: torch.Tensor,\n",
        "    K_pool: torch.Tensor, V_pool: torch.Tensor,\n",
        "    block_table: torch.Tensor, block_T: int\n",
        "):\n",
        "    \"\"\"\n",
        "    Scatter logical tokens [T, D] into paged pools using block_table.\n",
        "    This simulates how vLLM stores KV into pages.\n",
        "\n",
        "    Inputs:\n",
        "      K_tokens, V_tokens: [T, D]\n",
        "      K_pool, V_pool: [PB, block_T, D]\n",
        "      block_table: [LB] (logical blocks)\n",
        "      block_T: tokens per block\n",
        "\n",
        "    TODO:\n",
        "      - For each logical token index t:\n",
        "          lb = t // block_T\n",
        "          off = t % block_T\n",
        "          pb = block_table[lb]\n",
        "          write into K_pool[pb, off, :]\n",
        "    \"\"\"\n",
        "    # TODO: implement scatter\n",
        "\n",
        "    # -----------------------------\n",
        "    # Step 1: Validate inputs (shapes / dtypes / devices)\n",
        "    # -----------------------------\n",
        "\n",
        "    assert isinstance(K_tokens, torch.Tensor) and isinstance(V_tokens, torch.Tensor)\n",
        "    assert isinstance(K_pool, torch.Tensor) and isinstance(V_pool, torch.Tensor)\n",
        "    assert isinstance(block_table, torch.Tensor)\n",
        "\n",
        "    # token tensors: [T, D]\n",
        "    assert K_tokens.ndim == 2, f\"K_tokens must be [T,D], got {K_tokens.shape}\"\n",
        "    assert V_tokens.ndim == 2, f\"V_tokens must be [T,D], got {V_tokens.shape}\"\n",
        "    assert K_tokens.shape == V_tokens.shape, f\"K_tokens and V_tokens must match, got {K_tokens.shape} vs {V_tokens.shape}\"\n",
        "    T, D = K_tokens.shape\n",
        "\n",
        "    # pool tensors: [PB, block_T, D]\n",
        "    assert K_pool.ndim == 3, f\"K_pool must be [PB,block_T,D], got {K_pool.shape}\"\n",
        "    assert V_pool.ndim == 3, f\"V_pool must be [PB,block_T,D], got {V_pool.shape}\"\n",
        "    assert K_pool.shape == V_pool.shape, f\"K_pool and V_pool must match, got {K_pool.shape} vs {V_pool.shape}\"\n",
        "    PB, BT, Dp = K_pool.shape\n",
        "\n",
        "    assert BT == block_T, f\"pool block_T={BT} must equal block_T arg={block_T}\"\n",
        "    assert Dp == D, f\"pool D={Dp} must equal tokens D={D}\"\n",
        "\n",
        "    # device consistency\n",
        "    dev = K_pool.device\n",
        "    assert K_tokens.device == dev, f\"K_tokens device {K_tokens.device} must match K_pool device {dev}\"\n",
        "    assert V_tokens.device == dev, f\"V_tokens device {V_tokens.device} must match K_pool device {dev}\"\n",
        "    assert V_pool.device == dev, f\"V_pool device {V_pool.device} must match K_pool device {dev}\"\n",
        "    assert block_table.device == dev, f\"block_table device {block_table.device} must match K_pool device {dev}\"\n",
        "\n",
        "    # dtype sanity (toy: usually fp16/bf16 for K/V pools)\n",
        "    assert K_tokens.dtype == K_pool.dtype, f\"K_tokens dtype {K_tokens.dtype} must match K_pool dtype {K_pool.dtype}\"\n",
        "    assert V_tokens.dtype == V_pool.dtype, f\"V_tokens dtype {V_tokens.dtype} must match V_pool dtype {V_pool.dtype}\"\n",
        "\n",
        "    # block_table: [LB] integer\n",
        "    assert block_table.ndim == 1, f\"block_table must be 1D [LB], got {block_table.shape}\"\n",
        "    assert block_table.dtype in (torch.int32, torch.int64), f\"block_table must be int32/int64, got {block_table.dtype}\"\n",
        "\n",
        "    # block_table must cover all logical blocks for T tokens\n",
        "    LB = (T + block_T - 1) // block_T\n",
        "    assert block_table.numel() >= LB, f\"block_table too short: need LB={LB}, got {block_table.numel()}\"\n",
        "\n",
        "    # pb range check (optional but strongly recommended)\n",
        "    # Only check the portion we will actually use (first LB entries)\n",
        "    bt_used = block_table[:LB].to(torch.int64)\n",
        "    assert int(bt_used.min().item()) >= 0, \"block_table contains negative physical block id\"\n",
        "    assert int(bt_used.max().item()) < PB, f\"block_table contains pb >= PB (PB={PB})\"\n",
        "\n",
        "    # Check uniqueness\n",
        "    unique_pb = torch.unique(bt_used)\n",
        "\n",
        "    assert unique_pb.numel() == bt_used.numel(), (\n",
        "        \"block_table contains duplicate physical block ids \"\n",
        "        \"(would cause KV overwrite)\"\n",
        "    )\n",
        "\n",
        "    # -----------------------------\n",
        "    # Step 2: Scatter write (pb scalar + write V_pool too)\n",
        "    # -----------------------------\n",
        "\n",
        "    for t in range(T):\n",
        "        lb = t // block_T\n",
        "        off = t % block_T\n",
        "\n",
        "        pb = int(block_table[lb].item())\n",
        "\n",
        "        K_pool[pb, off, :] = K_tokens[t, :]\n",
        "        V_pool[pb, off, :] = V_tokens[t, :]\n",
        "\n",
        "\n",
        "    # -----------------------------\n",
        "    # Step3: zero out unused slots in the last block (debug-friendly)\n",
        "    # -----------------------------\n",
        "    valid = T % block_T\n",
        "    if valid > 0:\n",
        "        lb_last = LB - 1  # last logical block\n",
        "        pb_last = int(block_table[lb_last].item())\n",
        "\n",
        "        pb_last = int(block_table[lb_last].item())\n",
        "        K_pool[pb_last, valid:, :].zero_()\n",
        "        V_pool[pb_last, valid:, :].zero_()\n",
        "\n",
        "    # raise NotImplementedError\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Memory stats\n",
        "# ============================================================\n",
        "def mem_stats_contiguous(T: int, D: int, dtype=torch.float16) -> MemStats:\n",
        "    elem = torch.tensor([], dtype=dtype).element_size()\n",
        "    used = 2 * T * D * elem  # K and V\n",
        "    return MemStats(\n",
        "        mode=\"contiguous\",\n",
        "        logical_T=T,\n",
        "        D=D,\n",
        "        block_T=0,\n",
        "        num_logical_blocks=0,\n",
        "        num_phys_blocks=0,\n",
        "        kv_bytes_allocated=used,\n",
        "        kv_bytes_used=used,\n",
        "        fragmentation_bytes=0,\n",
        "        fragmentation_ratio=0.0,\n",
        "    )\n",
        "\n",
        "\n",
        "def mem_stats_paged(T: int, cfg: PageCfg, dtype=torch.float16) -> MemStats:\n",
        "    elem = torch.tensor([], dtype=dtype).element_size()\n",
        "    block_T = cfg.BLOCK_T\n",
        "    LB = (T + block_T - 1) // block_T\n",
        "    allocated = 2 * cfg.NUM_PHYS_BLOCKS * block_T * cfg.D * elem\n",
        "    used = 2 * T * cfg.D * elem\n",
        "    frag = allocated - used\n",
        "    return MemStats(\n",
        "        mode=\"paged\",\n",
        "        logical_T=T,\n",
        "        D=cfg.D,\n",
        "        block_T=block_T,\n",
        "        num_logical_blocks=LB,\n",
        "        num_phys_blocks=cfg.NUM_PHYS_BLOCKS,\n",
        "        kv_bytes_allocated=allocated,\n",
        "        kv_bytes_used=used,\n",
        "        fragmentation_bytes=max(0, frag),\n",
        "        fragmentation_ratio=max(0.0, frag / max(1, allocated)),\n",
        "    )\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Triton: toy \"paged gather\" helper (kernel-side addressing)\n",
        "# ============================================================\n",
        "@triton.jit\n",
        "def paged_kv_gather_kernel(\n",
        "    # pointers\n",
        "    k_pool_ptr, v_pool_ptr,\n",
        "    block_table_ptr,\n",
        "    # output contiguous buffers for debugging (optional)\n",
        "    k_out_ptr, v_out_ptr,\n",
        "    # sizes\n",
        "    T: tl.constexpr, D: tl.constexpr,\n",
        "    BLOCK_T: tl.constexpr,\n",
        "    # strides (pool is [PB, BLOCK_T, D])\n",
        "    stride_kpb: tl.constexpr, stride_kpt: tl.constexpr, stride_kd: tl.constexpr,\n",
        "    stride_vpb: tl.constexpr, stride_vpt: tl.constexpr, stride_vd: tl.constexpr,\n",
        "    stride_out_t: tl.constexpr, stride_out_d: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    OPTIONAL helper kernel:\n",
        "      Gather paged K/V into contiguous [T, D] buffers.\n",
        "    This is NOT how vLLM does it (they avoid materializing), but useful for debugging.\n",
        "\n",
        "    TODO:\n",
        "      - Map program id to a token block\n",
        "      - For each token t in the block:\n",
        "          lb = t // BLOCK_T\n",
        "          off = t % BLOCK_T\n",
        "          pb = block_table[lb]\n",
        "          load K_pool[pb, off, :]\n",
        "          store into k_out[t, :]\n",
        "      - Similarly for V\n",
        "    \"\"\"\n",
        "    # TODO: implement (optional)\n",
        "    # raise NotImplementedError\n",
        "\n",
        "    # ---- program id -> logical block id ----\n",
        "    pid = tl.program_id(axis=0)\n",
        "    t_idx = pid * BLOCK_T + tl.arange(0, BLOCK_T)\n",
        "    mask_t = t_idx < T\n",
        "    mask_td = mask_t[:, None]\n",
        "\n",
        "    # ---- logical block -> physical block ----\n",
        "    pb = tl.load(block_table_ptr + pid).to(tl.int64)\n",
        "\n",
        "    # ---- offsets within a block (tokens) and within a vector (D) ----\n",
        "    offs_t_in_block = tl.arange(0, BLOCK_T)\n",
        "    offs_d = tl.arange(0, D)\n",
        "\n",
        "    # ---- build pool pointers [BLOCK_T, D] ----\n",
        "    k_ptrs = k_pool_ptr + pb * stride_kpb + offs_t_in_block[:, None] * stride_kpt + offs_d[None, :] * stride_kd\n",
        "    v_ptrs = v_pool_ptr + pb * stride_vpb + offs_t_in_block[:, None] * stride_vpt + offs_d[None, :] * stride_vd\n",
        "\n",
        "    # ---- build output pointers [BLOCK_T, D] ----\n",
        "    k_out_ptrs = k_out_ptr + t_idx[:, None] * stride_out_t + offs_d[None, :] * stride_out_d\n",
        "    v_out_ptrs = v_out_ptr + t_idx[:, None] * stride_out_t + offs_d[None, :] * stride_out_d\n",
        "\n",
        "    # ---- load from pool and store to contiguous outputs ----\n",
        "    k = tl.load(k_ptrs, mask=mask_td, other=0.0)\n",
        "    v = tl.load(v_ptrs, mask=mask_td, other=0.0)\n",
        "\n",
        "    tl.store(k_out_ptrs, k, mask=mask_td)\n",
        "    tl.store(v_out_ptrs, v, mask=mask_td)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Triton PageAttention (toy)\n",
        "# ============================================================\n",
        "@triton.jit\n",
        "def page_attention_kernel(\n",
        "    q_ptr,                    # [M, D]\n",
        "    k_pool_ptr, v_pool_ptr,   # [PB, BLOCK_T, D]\n",
        "    block_table_ptr,          # [LB]\n",
        "    mask_ptr,                 # [M, T] additive mask (optional, can be null/dummy if HAS_MASK=False)\n",
        "    out_ptr,                  # [M, D]\n",
        "    # sizes\n",
        "    M: tl.constexpr,\n",
        "    T: tl.constexpr,\n",
        "    D: tl.constexpr,\n",
        "    BLOCK_T: tl.constexpr,\n",
        "    # strides for Q [M, D]\n",
        "    stride_qm: tl.constexpr, stride_qd: tl.constexpr,\n",
        "    # strides for pool [PB, BLOCK_T, D]\n",
        "    stride_kpb: tl.constexpr, stride_kpt: tl.constexpr, stride_kd: tl.constexpr,\n",
        "    stride_vpb: tl.constexpr, stride_vpt: tl.constexpr, stride_vd: tl.constexpr,\n",
        "    # strides for mask [M, T]\n",
        "    stride_mm: tl.constexpr, stride_mt: tl.constexpr,\n",
        "    # strides for Out [M, D]\n",
        "    stride_om: tl.constexpr, stride_od: tl.constexpr,\n",
        "    # tiling\n",
        "    BLOCK_N: tl.constexpr,\n",
        "    HAS_MASK: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Toy paged attention:\n",
        "      out[m, :] = softmax( q[m,:] @ K[:T,:]^T + mask ) @ V[:T,:]\n",
        "\n",
        "    Constraints / simplifying assumptions:\n",
        "      - Single head\n",
        "      - Uses block_table to locate K/V blocks\n",
        "      - Does NOT attempt FlashAttention fusion tricks (this is day4, not day6)\n",
        "      - You may implement:\n",
        "          (A) full materialization of scores for toy correctness\n",
        "          or\n",
        "          (B) streaming softmax (more advanced, optional)\n",
        "    Skeleton expects TODOs only.\n",
        "\n",
        "    TODO:\n",
        "      1) Load q vector for row m\n",
        "      2) Iterate over key tiles t0:t0+BLOCK_N\n",
        "          - For each token t in tile:\n",
        "              lb = t // BLOCK_T\n",
        "              off = t % BLOCK_T\n",
        "              pb = block_table[lb]\n",
        "              load k = K_pool[pb, off, :]\n",
        "              compute score = dot(q, k)\n",
        "              apply mask if HAS_MASK\n",
        "          - softmax over T tokens (requires reduction across tiles)\n",
        "      3) Weighted sum over V similarly:\n",
        "          out = sum_j p_j * v_j\n",
        "\n",
        "    Because softmax needs a global normalization across all T,\n",
        "    you will likely need:\n",
        "      - a two-pass approach (scores -> softmax -> PV), OR\n",
        "      - an online softmax approach.\n",
        "\n",
        "    For this Day4 toy, pick the simplest correct approach.\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    # raise NotImplementedError\n",
        "    m = tl.program_id(0)\n",
        "\n",
        "\n",
        "    # ---- load q[m, :] ----\n",
        "    d = tl.arange(0, D)\n",
        "    mask_d = d < D\n",
        "    q = tl.load(q_ptr + m * stride_qm + d * stride_qd, mask = mask_d, other = 0.0).to(tl.float32)\n",
        "    inv_sqrt_d = 1.0 / tl.sqrt(tl.full([], D, tl.float32))\n",
        "\n",
        "    # Pass 1: compute global max score for numerical stability\n",
        "    max_s = tl.full([], -float(\"inf\"), tl.float32)\n",
        "\n",
        "    for t0 in range(0, T, BLOCK_N):\n",
        "        t = t0 + tl.arange(0, BLOCK_N)               # [BN]\n",
        "        mask_t = t < T\n",
        "\n",
        "        #page address translation\n",
        "        lb = t // BLOCK_T                             # [BN]\n",
        "        off = t % BLOCK_T                             # [BN]\n",
        "\n",
        "        pb = tl.load(block_table_ptr + lb, mask=mask_t, other=0).to(tl.int64)  # [BN]\n",
        "\n",
        "\n",
        "        # load K tile: [BN, D]\n",
        "        k_ptrs = (k_pool_ptr + pb[:, None] * stride_kpb + off[:, None] * stride_kpt + d[None, :] * stride_kd)\n",
        "        k = tl.load(k_ptrs, mask=mask_t[:, None], other=0.0).to(tl.float32)\n",
        "\n",
        "        #score\n",
        "        s = tl.sum(k * q[None, :], axis=1) * inv_sqrt_d   # [BN]\n",
        "\n",
        "        if HAS_MASK:\n",
        "          mvals = tl.load(mask_ptr + m * stride_mm + t * stride_mt, mask=mask_t, other=-float(\"inf\")).to(tl.float32)\n",
        "          s = s + mvals\n",
        "\n",
        "        # invalid tokens -> -inf so they don't affect max\n",
        "        s = tl.where(mask_t, s, -float(\"inf\"))\n",
        "        max_s = tl.maximum(max_s, tl.max(s, axis=0))\n",
        "\n",
        "    # Pass 2: compute sumexp and accumulate PV\n",
        "    denom = tl.full([], 0.0, tl.float32)\n",
        "    out = tl.zeros([D], dtype=tl.float32)\n",
        "\n",
        "    for t0 in range(0, T, BLOCK_N):\n",
        "        t = t0 + tl.arange(0, BLOCK_N)\n",
        "        mask_t = t < T\n",
        "\n",
        "        lb = t // BLOCK_T\n",
        "        off = t % BLOCK_T\n",
        "\n",
        "        pb = tl.load(block_table_ptr + lb, mask=mask_t, other=0).to(tl.int64)\n",
        "\n",
        "        # load K tile: [BN, D]\n",
        "        k_ptrs = (k_pool_ptr + pb[:, None] * stride_kpb + off[:, None] * stride_kpt + d[None, :] * stride_kd)\n",
        "        k = tl.load(k_ptrs, mask=mask_t[:, None], other=0.0).to(tl.float32)\n",
        "\n",
        "        s = tl.sum(k * q[None, :], axis=1) * inv_sqrt_d  # [BN]\n",
        "\n",
        "        if HAS_MASK:\n",
        "            mvals = tl.load(mask_ptr + m * stride_mm + t * stride_mt, mask=mask_t, other=-float(\"inf\")).to(tl.float32)\n",
        "            s = s + mvals\n",
        "\n",
        "        s = tl.where(mask_t, s, -float(\"inf\"))\n",
        "\n",
        "        # exp(score - max)\n",
        "        w = tl.exp(s - max_s)                           # [BN]\n",
        "        w = tl.where(mask_t, w, 0.0)\n",
        "\n",
        "        denom += tl.sum(w, axis=0)\n",
        "\n",
        "        # load V: [BN, D]\n",
        "        v_ptrs = (v_pool_ptr + pb[:, None] * stride_vpb + off[:, None] * stride_vpt + d[None, :] * stride_vd)\n",
        "        v = tl.load(v_ptrs, mask=mask_t[:, None], other=0.0).to(tl.float32)\n",
        "\n",
        "        # out += sum_j w_j * v_j\n",
        "        out += tl.sum(v * w[:, None], axis=0)\n",
        "\n",
        "    # normalize\n",
        "    denom = tl.maximum(denom, 1e-9)\n",
        "    out = out / denom\n",
        "\n",
        "    # store\n",
        "    tl.store(out_ptr + m * stride_om + d * stride_od, out.to(tl.float16))\n",
        "\n",
        "# ============================================================\n",
        "# Torch references\n",
        "# ============================================================\n",
        "def attention_torch_contiguous(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
        "    \"\"\"\n",
        "    Reference attention for contiguous KV:\n",
        "      out = softmax(Q @ K.T + mask) @ V\n",
        "    \"\"\"\n",
        "    # TODO: implement (use fp32 scores for stability)\n",
        "    # raise NotImplementedError\n",
        "    d = Q.shape[-1]\n",
        "\n",
        "\n",
        "    # fp32 compute for stability\n",
        "    scores = torch.matmul(Q.float(), K.float().transpose(-1, -2))\n",
        "    scores /= math.sqrt(d)\n",
        "\n",
        "    if mask is not None:\n",
        "        scores += mask.to(scores.dtype)\n",
        "\n",
        "    scores = scores.softmax(dim=-1)\n",
        "    out = torch.matmul(scores, V.float())\n",
        "\n",
        "    return out.to(Q.dtype)\n",
        "\n",
        "\n",
        "\n",
        "def attention_torch_from_paged(\n",
        "    Q: torch.Tensor,\n",
        "    K_pool: torch.Tensor, V_pool: torch.Tensor,\n",
        "    block_table: torch.Tensor, T: int, cfg: PageCfg,\n",
        "    mask: Optional[torch.Tensor] = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Reference attention by first gathering paged KV into contiguous K,V (for correctness only).\n",
        "    Then run standard torch attention.\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # - gather K,V into [T,D] using block_table\n",
        "    # - call attention_torch_contiguous\n",
        "    # raise NotImplementedError\n",
        "\n",
        "\n",
        "    # Shapes (toy):\n",
        "    # Q:        [M, D]\n",
        "    # K_pool:   [PB, BLOCK_T, D]\n",
        "    # V_pool:   [PB, BLOCK_T, D]\n",
        "    # block_table: [LB] where LB = ceil(T / BLOCK_T)\n",
        "    # mask:     None or [M, T] additive mask\n",
        "\n",
        "    assert Q.ndim == 2, f\"Q must be [M,D], got {Q.shape}\"\n",
        "    assert K_pool.ndim == 3 and V_pool.ndim == 3, f\"K_pool/V_pool must be [PB,BLOCK_T,D]\"\n",
        "    assert K_pool.shape == V_pool.shape, \"K_pool and V_pool must have same shape\"\n",
        "    PB, BLOCK_T, D = K_pool.shape\n",
        "\n",
        "    assert D == cfg.D, f\"Pool D={D} must match cfg.D={cfg.D}\"\n",
        "    assert BLOCK_T == cfg.BLOCK_T, f\"Pool BLOCK_T={BLOCK_T} must match cfg.BLOCK_T={cfg.BLOCK_T}\"\n",
        "    assert Q.shape[1] == D, f\"Q D={Q.shape[1]} must match pool D={D}\"\n",
        "    assert 0 < T <= cfg.BLOCK_T * K_pool.shape[0] * 10_000, \"T looks unreasonable for given pool (sanity check)\"\n",
        "\n",
        "\n",
        "    # block_table length must cover all logical blocks needed for T tokens\n",
        "    LB = (T + BLOCK_T - 1) // BLOCK_T\n",
        "    assert block_table.ndim == 1, f\"block_table must be 1D, got {block_table.shape}\"\n",
        "    assert block_table.numel() >= LB, f\"block_table too short: need {LB}, got {block_table.numel()}\"\n",
        "    assert block_table.dtype in (torch.int32, torch.int64)\n",
        "\n",
        "    # Build logical token indices [0..T-1]\n",
        "    device = Q.device\n",
        "    t_idx = torch.arange(T, device=device, dtype=torch.int64)          # [T]\n",
        "    lb = t_idx // BLOCK_T                                              # [T]\n",
        "    off = t_idx % BLOCK_T                                              # [T]\n",
        "    pb = block_table[lb].to(torch.int64)                               # [T]\n",
        "\n",
        "\n",
        "    # Gather K,V into contiguous [T,D]\n",
        "    # Advanced indexing: K_pool[pb, off] -> [T,D]\n",
        "    K_contig = K_pool[pb, off, :]                                      # [T,D]\n",
        "    V_contig = V_pool[pb, off, :]                                      # [T,D]\n",
        "\n",
        "\n",
        "    return attention_torch_contiguous(Q, K_contig, V_contig, mask=mask)\n",
        "\n",
        "# ============================================================\n",
        "# Driver: build toy data, run correctness checks\n",
        "# ============================================================\n",
        "@torch.no_grad()\n",
        "def check_correctness(\n",
        "    T: int = 1024,\n",
        "    D: int = 64,\n",
        "    block_T: int = 16,\n",
        "    num_phys_blocks: int = 128,\n",
        "    M: int = 1,\n",
        "    dtype=torch.float16,\n",
        "    use_mask: bool = False,\n",
        "):\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    cfg = PageCfg(BLOCK_T=block_T, D=D, NUM_PHYS_BLOCKS=num_phys_blocks)\n",
        "\n",
        "    # Create a toy query (M queries)\n",
        "    Q = torch.randn((M, D), device=device, dtype=dtype)\n",
        "\n",
        "    # Create logical tokens for KV (as if appended over time)\n",
        "    K_tokens = torch.randn((T, D), device=device, dtype=dtype)\n",
        "    V_tokens = torch.randn((T, D), device=device, dtype=dtype)\n",
        "\n",
        "    # --- Contiguous baseline ---\n",
        "    K_contig, V_contig = alloc_contiguous_kv(T, D, device=device, dtype=dtype)\n",
        "    K_contig.copy_(K_tokens)\n",
        "    V_contig.copy_(V_tokens)\n",
        "\n",
        "    mask = None\n",
        "    if use_mask:\n",
        "        # TODO: define mask shape; for toy use [M, T] or [M, T] additive\n",
        "        # or full [M, T] if you compute scores row-wise.\n",
        "        # mask = ...\n",
        "        # raise NotImplementedError(\"TODO: mask construction\")\n",
        "        keep = T // 2\n",
        "        mask = torch.zeros((M, T), device=device, dtype=torch.float32)\n",
        "        mask[:, keep:] = float(\"-inf\")\n",
        "\n",
        "    # TODO: torch contiguous reference\n",
        "    # out_ref = attention_torch_contiguous(Q, K_contig, V_contig, mask=mask)\n",
        "    out_ref = attention_torch_contiguous(Q, K_contig, V_contig, mask=mask)\n",
        "\n",
        "    # --- Paged layout ---\n",
        "    K_pool, V_pool = alloc_paged_kv_pool(num_phys_blocks, block_T, D, device=device, dtype=dtype)\n",
        "    LB = (T + block_T - 1) // block_T\n",
        "    block_table = build_block_table(LB, num_phys_blocks, device=device)\n",
        "\n",
        "    write_tokens_to_paged_kv(K_tokens, V_tokens, K_pool, V_pool, block_table, block_T)\n",
        "\n",
        "    # TODO: Triton paged attention\n",
        "    # out_paged = page_attention_triton(Q, K_pool, V_pool, block_table, T, cfg, mask=mask)\n",
        "    out_paged = page_attention_triton(Q, K_pool, V_pool, block_table, T, cfg, mask=mask)\n",
        "\n",
        "    # TODO: compare out_paged with out_ref\n",
        "    # max_err = (out_paged - out_ref).abs().max().item()\n",
        "    # mean_err = (out_paged - out_ref).abs().mean().item()\n",
        "    # print(...)\n",
        "    # raise NotImplementedError\n",
        "    diff = (out_paged - out_ref).float()\n",
        "    max_err = diff.abs().max().item()\n",
        "    mean_err = diff.abs().mean().item()\n",
        "\n",
        "    print(\"\\n=== Correctness Check ===\")\n",
        "    print(f\"T={T}, D={D}, block_T={block_T}, PB={num_phys_blocks}, M={M}, dtype={dtype}, use_mask={use_mask}\")\n",
        "    print(f\"max_abs_err  = {max_err:.6e}\")\n",
        "    print(f\"mean_abs_err = {mean_err:.6e}\")\n",
        "\n",
        "    # Simple tolerance guidance (toy fp16): adjust if needed\n",
        "    tol = 5e-2 if dtype in (torch.float16, torch.bfloat16) else 1e-4\n",
        "    if max_err > tol:\n",
        "        print(f\"[WARN] max_err {max_err:.3e} > tol {tol:.3e} (check paging addr / mask / numerics)\")\n",
        "    else:\n",
        "        print(\"[OK] Within tolerance.\")\n",
        "\n",
        "    return {\n",
        "        \"max_abs_err\": max_err,\n",
        "        \"mean_abs_err\": mean_err,\n",
        "        \"out_ref\": out_ref,\n",
        "        \"out_paged\": out_paged,\n",
        "        \"mask\": mask,\n",
        "        \"block_table\": block_table,\n",
        "    }\n",
        "\n",
        "\n",
        "def page_attention_triton(\n",
        "    Q: torch.Tensor,\n",
        "    K_pool: torch.Tensor,\n",
        "    V_pool: torch.Tensor,\n",
        "    block_table: torch.Tensor,\n",
        "    T: int,\n",
        "    cfg: PageCfg,\n",
        "    mask: Optional[torch.Tensor] = None,\n",
        "    BLOCK_N: int = 128,\n",
        "    num_warps: int = 4,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Launcher for page_attention_kernel.\n",
        "    Q: [M, D]\n",
        "    K_pool/V_pool: [PB, BLOCK_T, D]\n",
        "    block_table: [LB]\n",
        "    Returns:\n",
        "      out: [M, D]\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # - Validate shapes/dtypes\n",
        "    # - Allocate out\n",
        "    # - Define grid (e.g., one program per query row m)\n",
        "    # - Pass strides and constexpr args\n",
        "    # - HAS_MASK toggle\n",
        "    # raise NotImplementedError\n",
        "    assert Q.ndim == 2\n",
        "    M, D = Q.shape\n",
        "    assert K_pool.ndim == 3 and V_pool.ndim == 3\n",
        "    PB, BT, Dp = K_pool.shape\n",
        "    assert (PB, BT, Dp) == V_pool.shape\n",
        "    assert BT == cfg.BLOCK_T and Dp == D\n",
        "    LB = (T + cfg.BLOCK_T - 1) // cfg.BLOCK_T\n",
        "    assert block_table.ndim == 1 and block_table.numel() >= LB\n",
        "    assert block_table.dtype in (torch.int32, torch.int64)\n",
        "\n",
        "    out = torch.empty((M, D), device=Q.device, dtype=Q.dtype)\n",
        "\n",
        "    HAS_MASK = mask is not None\n",
        "    if HAS_MASK:\n",
        "        assert mask.shape == (M, T), f\"mask must be [M,T], got {mask.shape}\"\n",
        "        # mask can be fp16/fp32; kernel reads as fp32\n",
        "        mask_ptr = mask\n",
        "        stride_mm, stride_mt = mask.stride()\n",
        "    else:\n",
        "        # dummy tensor (won't be read when HAS_MASK=False)\n",
        "        mask_ptr = out  # any valid pointer on device\n",
        "        stride_mm, stride_mt = 0, 0\n",
        "\n",
        "    grid = (M,)\n",
        "    page_attention_kernel[grid](\n",
        "        Q, K_pool, V_pool, block_table, mask_ptr, out,\n",
        "        M=M, T=T, D=D, BLOCK_T=cfg.BLOCK_T,\n",
        "        stride_qm=Q.stride(0), stride_qd=Q.stride(1),\n",
        "        stride_kpb=K_pool.stride(0), stride_kpt=K_pool.stride(1), stride_kd=K_pool.stride(2),\n",
        "        stride_vpb=V_pool.stride(0), stride_vpt=V_pool.stride(1), stride_vd=V_pool.stride(2),\n",
        "        stride_mm=stride_mm, stride_mt=stride_mt,\n",
        "        stride_om=out.stride(0), stride_od=out.stride(1),\n",
        "        BLOCK_N=BLOCK_N,\n",
        "        HAS_MASK=HAS_MASK,\n",
        "        num_warps=num_warps,\n",
        "    )\n",
        "\n",
        "    return out\n",
        "\n",
        "# ============================================================\n",
        "# Memory statistics printing\n",
        "# ============================================================\n",
        "def print_mem_stats(T: int, D: int, cfg: PageCfg, dtype=torch.float16):\n",
        "    c = mem_stats_contiguous(T, D, dtype=dtype)\n",
        "    p = mem_stats_paged(T, cfg, dtype=dtype)\n",
        "\n",
        "    print(\"\\n=== Memory Stats ===\")\n",
        "    print(f\"Contiguous KV: allocated={c.kv_bytes_allocated/1e6:.3f} MB, used={c.kv_bytes_used/1e6:.3f} MB\")\n",
        "    print(f\"Paged KV     : allocated={p.kv_bytes_allocated/1e6:.3f} MB, used={p.kv_bytes_used/1e6:.3f} MB, \"\n",
        "          f\"frag={p.fragmentation_ratio*100:.2f}%\")\n",
        "    print(\"====================\\n\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Main\n",
        "# ============================================================\n",
        "def main():\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"CUDA required.\")\n",
        "\n",
        "    # TODO: adjust toy parameters\n",
        "    T = 1024\n",
        "    D = 64\n",
        "    block_T = 16\n",
        "    num_phys_blocks = 128\n",
        "    M = 1\n",
        "\n",
        "    cfg = PageCfg(BLOCK_T=block_T, D=D, NUM_PHYS_BLOCKS=num_phys_blocks)\n",
        "    print_mem_stats(T, D, cfg, dtype=torch.float16)\n",
        "\n",
        "    # TODO: run correctness\n",
        "    check_correctness(T=T, D=D, block_T=block_T, num_phys_blocks=num_phys_blocks, M=M, use_mask=False)\n",
        "    # raise NotImplementedError(\"TODO: wire up correctness once kernels are implemented\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN2WzkMz8pmm"
      },
      "outputs": [],
      "source": [
        "# day6_flashattention_mini_skeleton.py\n",
        "# ============================================================\n",
        "# Day 6 ‚Äî Triton FlashAttention (Mini) (NO SOLUTION)\n",
        "#\n",
        "# Goal:\n",
        "#   Implement a mini FlashAttention-style kernel in Triton:\n",
        "#     - tiled QK^T\n",
        "#     - online softmax (streaming max/sum)\n",
        "#     - fuse V multiplication\n",
        "#     - single kernel end-to-end\n",
        "#   Then:\n",
        "#     - validate correctness vs PyTorch\n",
        "#     - compare performance vs naive attention (Day2)\n",
        "#\n",
        "# Scope (toy but realistic):\n",
        "#   - Single head (extend later)\n",
        "#   - One batch (extend later)\n",
        "#   - Q,K,V: [N, D] (N = seq length, D = head dim)\n",
        "#   - Output O: [N, D]\n",
        "#   - Optional causal mask (recommended)\n",
        "#   - Inputs fp16/bf16, accumulate fp32\n",
        "#\n",
        "# Notes:\n",
        "#   - This is a skeleton with TODOs only (no solution).\n",
        "#   - You will need to choose tiling sizes that fit SRAM (shared memory/registers).\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "import math\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, Dict, Any, Tuple, List\n",
        "\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TODO: import your Day2 naive attention for comparison\n",
        "# ============================================================\n",
        "def naive_attention_triton(Q, K, V, mask=None, cfg=None):\n",
        "    # TODO: import and call your Day2 implementation\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Config\n",
        "# ============================================================\n",
        "@dataclass(frozen=True)\n",
        "class FlashCfg:\n",
        "    BLOCK_M: int     # rows of Q processed per program\n",
        "    BLOCK_N: int     # cols of K/V per step (streaming over N)\n",
        "    BLOCK_D: int     # head dim tile (usually == D, but keep generic)\n",
        "    num_warps: int = 4\n",
        "    num_stages: int = 2  # optional pipelining\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Mask helper (optional)\n",
        "# ============================================================\n",
        "def make_additive_causal_mask(N: int, device=\"cuda\", dtype=torch.float32) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Returns additive causal mask [N, N]:\n",
        "      0 for j <= i, -inf for j > i\n",
        "    Used as: scores = scores + mask\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FlashAttention mini kernel (single kernel)\n",
        "# ============================================================\n",
        "@triton.jit\n",
        "def flashattn_mini_kernel(\n",
        "    q_ptr, k_ptr, v_ptr, o_ptr,\n",
        "    # optional mask pointer (additive), can be None by HAS_MASK flag\n",
        "    mask_ptr,\n",
        "    N: tl.constexpr, D: tl.constexpr,\n",
        "    stride_qn: tl.constexpr, stride_qd: tl.constexpr,\n",
        "    stride_kn: tl.constexpr, stride_kd: tl.constexpr,\n",
        "    stride_vn: tl.constexpr, stride_vd: tl.constexpr,\n",
        "    stride_on: tl.constexpr, stride_od: tl.constexpr,\n",
        "    # mask strides (if used): mask is [N, N] additive\n",
        "    stride_mn: tl.constexpr, stride_mm: tl.constexpr,\n",
        "    # tiling\n",
        "    BLOCK_M: tl.constexpr,\n",
        "    BLOCK_N: tl.constexpr,\n",
        "    BLOCK_D: tl.constexpr,\n",
        "    HAS_MASK: tl.constexpr,\n",
        "    IS_CAUSAL: tl.constexpr,\n",
        "    # scale (typically 1/sqrt(D))\n",
        "    SCALE: tl.constexpr,\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute O = softmax(QK^T + mask) V using tiling + online softmax, fused with V.\n",
        "\n",
        "    Structure (conceptual):\n",
        "      For each block of queries (m tile):\n",
        "        - initialize:\n",
        "            m_i = -inf         # running max per query row\n",
        "            l_i = 0            # running sum(exp(scores - m_i))\n",
        "            acc = 0            # running output accumulator (fp32)\n",
        "        - for n_tile over keys/values:\n",
        "            scores = q_tile @ k_tile^T * SCALE + mask_tile\n",
        "            # online softmax update:\n",
        "            m_new = max(m_i, rowmax(scores))\n",
        "            alpha = exp(m_i - m_new)\n",
        "            p = exp(scores - m_new)\n",
        "            l_new = l_i * alpha + rowsum(p)\n",
        "            acc = acc * alpha[:,None] + p @ v_tile\n",
        "            m_i = m_new\n",
        "            l_i = l_new\n",
        "        - normalize:\n",
        "            out = acc / l_i[:,None]\n",
        "        - store out\n",
        "\n",
        "    TODOs:\n",
        "      - Map program_id to query block start\n",
        "      - Load Q tile [BLOCK_M, D] (or [BLOCK_M, BLOCK_D] with loop if needed)\n",
        "      - Loop over K/V tiles along N:\n",
        "          * Load K tile [BLOCK_N, D]\n",
        "          * Compute score tile [BLOCK_M, BLOCK_N] in fp32\n",
        "          * Apply causal masking if IS_CAUSAL (score for j>i = -inf)\n",
        "          * Apply additive mask if HAS_MASK (mask_ptr)\n",
        "          * Update online softmax stats (m_i, l_i)\n",
        "          * Fuse V multiplication: acc += p @ V_tile\n",
        "      - Final normalize acc by l_i\n",
        "      - Store O tile\n",
        "    \"\"\"\n",
        "    # TODO: implement\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Launcher\n",
        "# ============================================================\n",
        "def flashattn_mini_triton(\n",
        "    Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,\n",
        "    mask: Optional[torch.Tensor] = None,\n",
        "    causal: bool = False,\n",
        "    cfg: FlashCfg = FlashCfg(BLOCK_M=64, BLOCK_N=128, BLOCK_D=64, num_warps=4),\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Q,K,V: [N, D]\n",
        "    mask: additive [N, N] or None\n",
        "    causal: if True, apply causal mask internally (avoid materializing full mask if you implement it that way)\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # - Validate shapes, dtypes, contiguity\n",
        "    # - Allocate output O [N, D]\n",
        "    # - Define grid (e.g., grid = (ceil_div(N, BLOCK_M),))\n",
        "    # - Set HAS_MASK / IS_CAUSAL flags\n",
        "    # - SCALE = 1/sqrt(D)\n",
        "    # - Call flashattn_mini_kernel[grid](...)\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PyTorch reference + correctness\n",
        "# ============================================================\n",
        "def flashattn_ref_torch(\n",
        "    Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,\n",
        "    mask: Optional[torch.Tensor] = None,\n",
        "    causal: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Reference attention in torch:\n",
        "      scores = Q @ K.T / sqrt(D)\n",
        "      if causal: apply causal mask\n",
        "      if mask: scores += mask\n",
        "      P = softmax(scores)\n",
        "      O = P @ V\n",
        "    \"\"\"\n",
        "    # TODO: implement reference (use fp32 for scores/softmax for stability)\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def check_correctness(\n",
        "    N=1024, D=64, dtype=torch.float16,\n",
        "    use_mask=False, causal=True,\n",
        "    cfg: FlashCfg = FlashCfg(BLOCK_M=64, BLOCK_N=128, BLOCK_D=64, num_warps=4),\n",
        "):\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    Q = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    K = torch.randn((N, D), device=device, dtype=dtype)\n",
        "    V = torch.randn((N, D), device=device, dtype=dtype)\n",
        "\n",
        "    mask = None\n",
        "    if use_mask:\n",
        "        # TODO: create additive mask (e.g., padding or random -inf positions)\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # TODO:\n",
        "    # - out_ref = flashattn_ref_torch(...)\n",
        "    # - out_tri = flashattn_mini_triton(...)\n",
        "    # - print max/mean abs error\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Benchmark: compare naive vs flash\n",
        "# ============================================================\n",
        "def cuda_time_ms(fn, iters=30, warmup=10) -> float:\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    s = torch.cuda.Event(enable_timing=True)\n",
        "    e = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    s.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    e.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return s.elapsed_time(e) / iters\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def compare_perf(\n",
        "    N_list: List[int] = [256, 512, 1024, 2048],\n",
        "    D: int = 64,\n",
        "    dtype=torch.float16,\n",
        "    causal: bool = True,\n",
        "    cfg_flash: FlashCfg = FlashCfg(BLOCK_M=64, BLOCK_N=128, BLOCK_D=64, num_warps=4),\n",
        "    cfg_naive: Optional[Dict[str, Any]] = None,\n",
        "):\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    print(\"| N | Impl | ms | speedup_vs_naive |\")\n",
        "    print(\"|---|------|----|------------------|\")\n",
        "\n",
        "    for N in N_list:\n",
        "        Q = torch.randn((N, D), device=device, dtype=dtype)\n",
        "        K = torch.randn((N, D), device=device, dtype=dtype)\n",
        "        V = torch.randn((N, D), device=device, dtype=dtype)\n",
        "\n",
        "        mask = None\n",
        "        if causal:\n",
        "            # For naive attention you may need a materialized mask; for flash you might do internal causal.\n",
        "            # TODO: create mask for naive if required by your implementation.\n",
        "            pass\n",
        "\n",
        "        # --- naive ---\n",
        "        def fn_naive():\n",
        "            return naive_attention_triton(Q, K, V, mask=mask, cfg=cfg_naive)\n",
        "\n",
        "        # --- flash ---\n",
        "        def fn_flash():\n",
        "            return flashattn_mini_triton(Q, K, V, mask=None, causal=causal, cfg=cfg_flash)\n",
        "\n",
        "        # TODO: optionally benchmark torch reference too\n",
        "        ms_naive = cuda_time_ms(fn_naive)\n",
        "        ms_flash = cuda_time_ms(fn_flash)\n",
        "\n",
        "        speedup = ms_naive / ms_flash if ms_flash > 0 else float(\"inf\")\n",
        "\n",
        "        print(f\"| {N} | naive | {ms_naive:.4f} | 1.00x |\")\n",
        "        print(f\"| {N} | flash | {ms_flash:.4f} | {speedup:.2f}x |\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Main\n",
        "# ============================================================\n",
        "def main():\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"CUDA required.\")\n",
        "\n",
        "    # TODO: run correctness first on small N\n",
        "    # check_correctness(N=256, D=64, causal=True, use_mask=False)\n",
        "\n",
        "    # TODO: then benchmark scaling\n",
        "    # compare_perf(N_list=[256, 512, 1024, 2048], D=64)\n",
        "\n",
        "    raise NotImplementedError(\"TODO: wire up your kernels and run correctness/bench\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}