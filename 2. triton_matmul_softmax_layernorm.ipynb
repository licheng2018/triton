{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs_bXqDJWwhs"
      },
      "source": [
        "## Triton Core Kernels & Benchmarking\n",
        "\n",
        "üéØ **Weekly Goal**  \n",
        "Implement and tune three core ML kernels (MatMul, Softmax, LayerNorm) using Triton,  \n",
        "develop intuition for **block sizes, `num_warps`, numerical stability, and kernel fusion**,  \n",
        "and perform systematic benchmarking against PyTorch to understand real performance trade-offs.\n",
        "\n",
        "---\n",
        "\n",
        "## Tuned MatMul (Block Size / `num_warps`)\n",
        "\n",
        "### Objective\n",
        "Implement a tile-based Triton GEMM kernel and tune **`BLOCK_M / BLOCK_N / BLOCK_K`**\n",
        "and **`num_warps`** to study their impact on throughput, occupancy, and register pressure.\n",
        "\n",
        "### Tasks\n",
        "- [ ] Implement a basic Triton matmul kernel (tile-based)\n",
        "- [ ] Define `BLOCK_M / BLOCK_N / BLOCK_K` as `tl.constexpr`\n",
        "- [ ] Evaluate multiple tile configurations (e.g., 64√ó64√ó32, 128√ó128√ó32)\n",
        "- [ ] Sweep `num_warps ‚àà {4, 8}` and compare performance\n",
        "- [ ] Compare against `torch.matmul` as a baseline\n",
        "\n",
        "### Key Concepts\n",
        "- **Tiling**: each program computes one output tile\n",
        "- **Block size** controls arithmetic intensity (FLOPs / byte)\n",
        "- **`num_warps`** trades off parallelism vs. register usage\n",
        "- GEMM is typically **compute-bound**, unlike vector add\n",
        "\n",
        "### Deliverables\n",
        "- Runnable `triton_matmul.py`\n",
        "- Performance table (ms / TFLOPS) for different configurations\n",
        "- Short analysis identifying the best configuration and why\n",
        "\n",
        "---\n",
        "\n",
        "## Triton Softmax (Numerically Stable)\n",
        "\n",
        "### Objective\n",
        "Implement a numerically stable row-wise softmax kernel in Triton,\n",
        "focusing on **max-subtraction**, reduction patterns, and block/warp mapping.\n",
        "\n",
        "### Tasks\n",
        "- [ ] Implement row-wise softmax in Triton\n",
        "- [ ] Apply `x - max(x)` for numerical stability\n",
        "- [ ] Decompose into two stages:\n",
        "  - max reduction\n",
        "  - exp + sum reduction\n",
        "- [ ] Correctly handle arbitrary feature dimensions (non-power-of-two)\n",
        "- [ ] Compare against `torch.softmax`\n",
        "\n",
        "### Key Concepts\n",
        "- **Numerical stability** for exponential operations\n",
        "- **Reduction patterns** within a program\n",
        "- Softmax is often **memory-bound with reductions**\n",
        "- Triton enables explicit control over reduction structure\n",
        "\n",
        "### Deliverables\n",
        "- `triton_softmax.py`\n",
        "- Correctness check vs. PyTorch (max / mean error)\n",
        "- Performance comparison table (ms / GB/s)\n",
        "\n",
        "---\n",
        "\n",
        "## Triton LayerNorm\n",
        "\n",
        "### Objective\n",
        "Implement Triton LayerNorm (forward pass) and understand\n",
        "**mean/variance reductions**, `eps` stabilization, and the performance benefits of kernel fusion.\n",
        "\n",
        "### Tasks\n",
        "- [ ] Implement LayerNorm forward in Triton\n",
        "- [ ] Compute per-row mean and variance\n",
        "- [ ] Apply `rsqrt(var + eps)`\n",
        "- [ ] Support affine parameters (`gamma`, `beta`)\n",
        "- [ ] Compare against `torch.nn.functional.layer_norm`\n",
        "\n",
        "### Key Concepts\n",
        "- Two reductions: mean ‚Üí variance\n",
        "- **Kernel fusion**: normalization + affine in a single kernel\n",
        "- LayerNorm is typically **memory-bound with reductions**\n",
        "- Triton avoids intermediate tensor materialization\n",
        "\n",
        "### Deliverables\n",
        "- `triton_layernorm.py`\n",
        "- Correctness validation (max / mean error)\n",
        "- Triton vs. PyTorch performance comparison\n",
        "\n",
        "---\n",
        "\n",
        "## Benchmark: Triton vs PyTorch\n",
        "\n",
        "### Objective\n",
        "Systematically benchmark Triton kernels against PyTorch eager kernels\n",
        "to identify **when Triton wins, when it does not, and why**.\n",
        "\n",
        "### Tasks\n",
        "- [ ] Build a unified benchmark framework (CUDA events)\n",
        "- [ ] Compare the following operators:\n",
        "  - vector add\n",
        "  - fused add + ReLU\n",
        "  - softmax\n",
        "  - layernorm\n",
        "- [ ] Record:\n",
        "  - latency (ms)\n",
        "  - effective bandwidth / FLOPs\n",
        "  - speedup\n",
        "- [ ] Repeat experiments across different tensor sizes\n",
        "\n",
        "### Key Concepts\n",
        "- **Bandwidth-bound vs. compute-bound** kernels\n",
        "- Kernel launch overhead\n",
        "- Real benefits of operator fusion\n",
        "- Why Triton excels at fused kernels rather than single primitive ops\n",
        "\n",
        "### Deliverables\n",
        "- `benchmark_triton_vs_torch.py`\n",
        "- Unified comparison table (Markdown / CSV)\n",
        "- Summary covering:\n",
        "  - which kernels benefit most from Triton\n",
        "  - which PyTorch kernels are already near-optimal\n",
        "  - implications for ML systems optimization\n",
        "\n",
        "---\n",
        "\n",
        "## End-of-Week Takeaways\n",
        "\n",
        "- Triton is **not** ‚Äúa faster PyTorch‚Äù\n",
        "- Triton enables **CUDA-level kernel design in Python**\n",
        "- The real performance gains come from:\n",
        "  - kernel fusion\n",
        "  - explicit reduction control\n",
        "  - tile-aware kernel design\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#matmul_skeleton\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "# ============================================================\n",
        "# Day 3: Tuned MatMul Skeleton (NO SOLUTION)\n",
        "# Goal:\n",
        "#   - Implement a tile-based matmul kernel in Triton\n",
        "#   - Tune BLOCK_M / BLOCK_N / BLOCK_K and num_warps\n",
        "#   - Validate vs torch.matmul\n",
        "#\n",
        "# Notes:\n",
        "#   - Assume A: [M, K], B: [K, N], C: [M, N], fp16 inputs, fp16 output (acc fp32).\n",
        "#   - You may start with fp16 and accumulate in fp32.\n",
        "# ============================================================\n",
        "\n",
        "@triton.jit\n",
        "def matmul_kernel(\n",
        "    a_ptr, b_ptr, c_ptr,\n",
        "    M, N, K,\n",
        "    stride_am, stride_ak,\n",
        "    stride_bk, stride_bn,\n",
        "    stride_cm, stride_cn,\n",
        "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
        "):\n",
        "    # TODO:\n",
        "    # 1) program ids for 2D tiling: pid_m, pid_n\n",
        "    # 2) compute offsets for A tile and B tile\n",
        "    # 3) loop over K tiles:\n",
        "    #    - tl.load A and B tiles with masks\n",
        "    #    - accumulate using tl.dot / manual FMA\n",
        "    # 4) tl.store to C with mask for M,N boundaries\n",
        "    #\n",
        "    # Hints:\n",
        "    #   pid_m = tl.program_id(0)\n",
        "    #   pid_n = tl.program_id(1)\n",
        "    #   offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    #   offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    #   offs_k = tl.arange(0, BLOCK_K)\n",
        "    #\n",
        "    #   use tl.multiple_of / tl.assume if needed (optional)\n",
        "    # raise NotImplementedError(\"TODO: implement matmul_kernel\")\n",
        "\n",
        "    # Compute row/col offsets for this program's C tile\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    # Compute row/col offsets for this program's C tile\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "\n",
        "\n",
        "    m_mask = offs_m < M          # [BM]\n",
        "    n_mask = offs_n < N          # [BN]\n",
        "\n",
        "    for k0 in range(0, K, BLOCK_K):\n",
        "        # current K indices for this chunk\n",
        "        k_offsets = k0 + offs_k          # [BK]\n",
        "        k_mask = k_offsets < K           # [BK]\n",
        "\n",
        "        # build pointer grids for this chunk\n",
        "        a_ptrs = a_ptr + offs_m[:, None] * stride_am + k_offsets[None, :] * stride_ak   # [BM, BK]\n",
        "        b_ptrs = b_ptr + k_offsets[:, None] * stride_bk + offs_n[None, :] * stride_bn   # [BK, BN]\n",
        "\n",
        "        # 2D masks for loads\n",
        "        a_load_mask = m_mask[:, None] & k_mask[None, :]    # [BM, BK]\n",
        "        b_load_mask = k_mask[:, None] & n_mask[None, :]    # [BK, BN]\n",
        "\n",
        "        # masked loads: out-of-bounds => 0\n",
        "        a_tile = tl.load(a_ptrs, mask=a_load_mask, other=0.0)  # [BM, BK], fp16/bf16\n",
        "        b_tile = tl.load(b_ptrs, mask=b_load_mask, other=0.0)  # [BK, BN], fp16/bf16\n",
        "\n",
        "        # accumulate (fp32)\n",
        "        # tl.dot will typically accumulate in fp32 when acc is fp32\n",
        "        acc += tl.dot(a_tile, b_tile)\n",
        "\n",
        "    c_tile = acc.to(tl.float16)\n",
        "    c_ptrs = c_ptr + offs_m[:,None]*stride_cm + offs_n[None,:]*stride_cn\n",
        "    tl.store(c_ptrs, c_tile, mask=m_mask[:,None] & n_mask[None,:])\n",
        "\n",
        "def triton_matmul(A: torch.Tensor, B: torch.Tensor,\n",
        "                  BLOCK_M=128, BLOCK_N=128, BLOCK_K=32,\n",
        "                  num_warps=8):\n",
        "    assert A.is_cuda and B.is_cuda\n",
        "    assert A.dtype in (torch.float16, torch.bfloat16)\n",
        "    assert B.dtype in (torch.float16, torch.bfloat16)\n",
        "    assert A.is_contiguous() and B.is_contiguous()\n",
        "    M, K = A.shape\n",
        "    K2, N = B.shape\n",
        "    assert K == K2\n",
        "\n",
        "    C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n",
        "\n",
        "    grid = (\n",
        "        triton.cdiv(M, BLOCK_M),\n",
        "        triton.cdiv(N, BLOCK_N),\n",
        "    )\n",
        "\n",
        "    matmul_kernel[grid](\n",
        "        A, B, C,\n",
        "        M, N, K,\n",
        "        A.stride(0), A.stride(1),\n",
        "        B.stride(0), B.stride(1),\n",
        "        C.stride(0), C.stride(1),\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n",
        "        num_warps=num_warps,   # Triton launch meta\n",
        "    )\n",
        "    return C\n",
        "\n",
        "@torch.no_grad()\n",
        "def check_matmul():\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA not available.\")\n",
        "        return\n",
        "\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    M, K, N = 512, 1024, 768\n",
        "    A = torch.randn((M, K), device=device, dtype=torch.float16)\n",
        "    B = torch.randn((K, N), device=device, dtype=torch.float16)\n",
        "\n",
        "    # PyTorch baseline\n",
        "    C_ref = A @ B\n",
        "\n",
        "\n",
        "    # Triton (will fail until you implement kernel)\n",
        "    try:\n",
        "        C_tri = triton_matmul(A, B, BLOCK_M=128, BLOCK_N=128, BLOCK_K=32, num_warps=8)\n",
        "    except NotImplementedError as e:\n",
        "        print(f\"[Day3] matmul kernel not implemented yet: {e}\")\n",
        "        return\n",
        "\n",
        "\n",
        "    C_ref_fp16 = A @ B                 # PyTorch fp16 Ë∑ØÂæÑ\n",
        "    C_ref_fp32 = A.float() @ B.float() # Êõ¥‰∏•Ê†º reference\n",
        "\n",
        "    # ‰Ω†ÁöÑËæìÂá∫ C_tri ÊòØ fp16\n",
        "    print(\"err vs torch fp16:\", (C_tri - C_ref_fp16).abs().max().item())\n",
        "    print(\"err vs fp32 ref  :\", (C_tri.float() - C_ref_fp32).abs().max().item())\n",
        "\n",
        "    # correctness\n",
        "    max_err = (C_tri - C_ref).abs().max().item()\n",
        "    print(f\"[Day3] max_abs_err = {max_err:.3e}\")\n",
        "    # You can tighten thresholds after tuning\n",
        "    assert max_err < 7e-2, \"Too large error (fp16). Improve implementation.\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_matmul()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvND3OxsZRtf",
        "outputId": "bdedb454-bcc6-4ee4-fcd3-f60e3df27fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "err vs torch fp16: 0.0625\n",
            "err vs fp32 ref  : 0.0601959228515625\n",
            "[Day3] max_abs_err = 6.250e-02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "D0leVK_gWwhu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcfef703-7b27-4c73-e853-6671203dcbaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "err vs torch fp16: 0.0625\n",
            "err vs fp32 ref  : 0.0601959228515625\n",
            "[Day3] max_abs_err = 6.250e-02\n"
          ]
        }
      ],
      "source": [
        "#matmul_skeleton soft_pipeline\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "# ============================================================\n",
        "# Day 3: Tuned MatMul Skeleton (NO SOLUTION)\n",
        "# Goal:\n",
        "#   - Implement a tile-based matmul kernel in Triton\n",
        "#   - Tune BLOCK_M / BLOCK_N / BLOCK_K and num_warps\n",
        "#   - Validate vs torch.matmul\n",
        "#\n",
        "# Notes:\n",
        "#   - Assume A: [M, K], B: [K, N], C: [M, N], fp16 inputs, fp16 output (acc fp32).\n",
        "#   - You may start with fp16 and accumulate in fp32.\n",
        "# ============================================================\n",
        "\n",
        "@triton.jit\n",
        "def matmul_kernel(\n",
        "    a_ptr, b_ptr, c_ptr,\n",
        "    M, N, K,\n",
        "    stride_am, stride_ak,\n",
        "    stride_bk, stride_bn,\n",
        "    stride_cm, stride_cn,\n",
        "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
        "    MAX_K_TILES: tl.constexpr,            # [NEW] meta: static_range upperboundÔºàavoid 1024 compileÔºâ\n",
        "    OUT_DTYPE: tl.constexpr,              # [NEW] meta: 0->fp16, 1->bf16\n",
        "):\n",
        "    # TODO:\n",
        "    # 1) program ids for 2D tiling: pid_m, pid_n\n",
        "    # 2) compute offsets for A tile and B tile\n",
        "    # 3) loop over K tiles:\n",
        "    #    - tl.load A and B tiles with masks\n",
        "    #    - accumulate using tl.dot / manual FMA\n",
        "    # 4) tl.store to C with mask for M,N boundaries\n",
        "    #\n",
        "    # Hints:\n",
        "    #   pid_m = tl.program_id(0)\n",
        "    #   pid_n = tl.program_id(1)\n",
        "    #   offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    #   offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    #   offs_k = tl.arange(0, BLOCK_K)\n",
        "    #\n",
        "    #   use tl.multiple_of / tl.assume if needed (optional)\n",
        "    # raise NotImplementedError(\"TODO: implement matmul_kernel\")\n",
        "\n",
        "    # Compute row/col offsets for this program's C tile\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    # Compute row/col offsets for this program's C tile\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "\n",
        "\n",
        "    m_mask = offs_m < M          # [BM]\n",
        "    n_mask = offs_n < N          # [BN]\n",
        "    #for k0 in range(0, K, BLOCK_K):\n",
        "        #k_offsets = k0 + offs_k          # [BK]\n",
        "    # Software Pipelining\n",
        "    #time ‚Üí\n",
        "    #    iter 0:  load\n",
        "    #    iter 1:       load     compute\n",
        "    #    iter 2:              load     compute\n",
        "    #    iter 3:                     load     compute\n",
        "    k_tiles = tl.cdiv(K, BLOCK_K)\n",
        "    for k_it in tl.static_range(0, MAX_K_TILES):\n",
        "        # if k_it >= k_tiles:\n",
        "        #    break\n",
        "        valid_k_iter = k_it < k_tiles\n",
        "\n",
        "        # k0 = k_it * BLOCK_K\n",
        "        # k_offsets = k0 + offs_k\n",
        "        k_offsets = k_it * BLOCK_K + offs_k\n",
        "\n",
        "        k_mask = k_offsets < K           # [BK]\n",
        "\n",
        "        final_k_mask = valid_k_iter & k_mask   # [BK]\n",
        "\n",
        "        # build pointer grids for this chunk\n",
        "        a_ptrs = a_ptr + offs_m[:, None] * stride_am + k_offsets[None, :] * stride_ak   # [BM, BK]\n",
        "        b_ptrs = b_ptr + k_offsets[:, None] * stride_bk + offs_n[None, :] * stride_bn   # [BK, BN]\n",
        "\n",
        "        # 2D masks for loads\n",
        "        a_load_mask = m_mask[:, None] & final_k_mask[None, :]    # [BM, BK]\n",
        "        b_load_mask = final_k_mask[:, None] & n_mask[None, :]    # [BK, BN]\n",
        "\n",
        "        # masked loads: out-of-bounds => 0\n",
        "        a_tile = tl.load(a_ptrs, mask=a_load_mask, other=0.0)  # [BM, BK], fp16/bf16\n",
        "        b_tile = tl.load(b_ptrs, mask=b_load_mask, other=0.0)  # [BK, BN], fp16/bf16\n",
        "\n",
        "        # accumulate (fp32)\n",
        "        # tl.dot will typically accumulate in fp32 when acc is fp32\n",
        "        acc += tl.dot(a_tile, b_tile)\n",
        "\n",
        "    c_tile = tl.where(OUT_DTYPE == 1, acc.to(tl.bfloat16), acc.to(tl.float16))\n",
        "    c_ptrs = c_ptr + offs_m[:,None]*stride_cm + offs_n[None,:]*stride_cn\n",
        "    tl.store(c_ptrs, c_tile, mask=m_mask[:,None] & n_mask[None,:])\n",
        "\n",
        "def triton_matmul(A: torch.Tensor, B: torch.Tensor,\n",
        "                  BLOCK_M=128, BLOCK_N=128, BLOCK_K=32,\n",
        "                  num_warps=8, num_stages=4):\n",
        "    assert A.is_cuda and B.is_cuda\n",
        "    assert A.dtype in (torch.float16, torch.bfloat16)\n",
        "    assert B.dtype in (torch.float16, torch.bfloat16)\n",
        "    assert A.is_contiguous() and B.is_contiguous()\n",
        "    M, K = A.shape\n",
        "    K2, N = B.shape\n",
        "    assert K == K2\n",
        "    assert A.dtype == B.dtype, \"For now, require A and B have same dtype (fp16 or bf16).\"\n",
        "\n",
        "    C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n",
        "\n",
        "    grid = (\n",
        "        triton.cdiv(M, BLOCK_M),\n",
        "        triton.cdiv(N, BLOCK_N),\n",
        "    )\n",
        "\n",
        "    MAX_K_TILES = triton.cdiv(K, BLOCK_K)\n",
        "\n",
        "    OUT_DTYPE = 1 if A.dtype == torch.bfloat16 else 0\n",
        "\n",
        "    matmul_kernel[grid](\n",
        "        A, B, C,\n",
        "        M, N, K,\n",
        "        A.stride(0), A.stride(1),\n",
        "        B.stride(0), B.stride(1),\n",
        "        C.stride(0), C.stride(1),\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n",
        "        MAX_K_TILES=MAX_K_TILES,\n",
        "        OUT_DTYPE=OUT_DTYPE,\n",
        "        num_warps=num_warps,   # Triton launch meta\n",
        "        num_stages=num_stages,\n",
        "\n",
        "    )\n",
        "    return C\n",
        "\n",
        "@torch.no_grad()\n",
        "def check_matmul():\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA not available.\")\n",
        "        return\n",
        "\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    M, K, N = 512, 1024, 768\n",
        "    A = torch.randn((M, K), device=device, dtype=torch.float16)\n",
        "    B = torch.randn((K, N), device=device, dtype=torch.float16)\n",
        "\n",
        "    # PyTorch baseline\n",
        "    C_ref = A @ B\n",
        "\n",
        "    # Triton (will fail until you implement kernel)\n",
        "    try:\n",
        "        C_tri = triton_matmul(A, B, BLOCK_M=128, BLOCK_N=128, BLOCK_K=32, num_warps=8, num_stages=4)\n",
        "    except NotImplementedError as e:\n",
        "        print(f\"[Day3] matmul kernel not implemented yet: {e}\")\n",
        "        return\n",
        "\n",
        "\n",
        "    C_ref_fp16 = A @ B                 # PyTorch fp16 Ë∑ØÂæÑ\n",
        "    C_ref_fp32 = A.float() @ B.float() # Êõ¥‰∏•Ê†º reference\n",
        "\n",
        "    # ‰Ω†ÁöÑËæìÂá∫ C_tri ÊòØ fp16\n",
        "    print(\"err vs torch fp16:\", (C_tri - C_ref_fp16).abs().max().item())\n",
        "    print(\"err vs fp32 ref  :\", (C_tri.float() - C_ref_fp32).abs().max().item())\n",
        "    # correctness\n",
        "    max_err = (C_tri - C_ref).abs().max().item()\n",
        "    print(f\"[Day3] max_abs_err = {max_err:.3e}\")\n",
        "    # You can tighten thresholds after tuning\n",
        "    assert max_err < 7e-2, \"Too large error (fp16). Improve implementation.\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_matmul()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "zHP1_JcUWwhv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342c0510-2db0-4f76-ee37-c144379feec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPU: Tesla T4\n",
            "Shape: B=4096, D=2048, dtype=torch.float32\n",
            "====================================================================================================\n",
            "Kernel     | warps | stages |       ms |       GB/s |    max_err\n",
            "----------------------------------------------------------------------------------------------------\n",
            "A          |     2 |      1 |   0.2892 |     232.03 |  5.588e-09\n",
            "A          |     2 |      2 |   0.2894 |     231.86 |  5.588e-09\n",
            "A          |     2 |      3 |   0.2894 |     231.91 |  5.588e-09\n",
            "A          |     4 |      1 |   0.2834 |     236.84 |  3.725e-09\n",
            "A          |     4 |      2 |   0.2833 |     236.91 |  3.725e-09\n",
            "A          |     4 |      3 |   0.2832 |     236.94 |  3.725e-09\n",
            "A          |     8 |      1 |   0.2825 |     237.58 |  5.588e-09\n",
            "A          |     8 |      2 |   0.2835 |     236.69 |  5.588e-09\n",
            "A          |     8 |      3 |   0.2835 |     236.71 |  5.588e-09\n",
            "B          |     2 |      1 |   0.2958 |     453.82 |  4.602e-02\n",
            "B          |     2 |      2 |   0.2955 |     454.22 |  4.602e-02\n",
            "B          |     2 |      3 |   0.2957 |     453.85 |  4.602e-02\n",
            "B          |     4 |      1 |   0.2922 |     459.34 |  4.602e-02\n",
            "B          |     4 |      2 |   0.2919 |     459.84 |  4.602e-02\n",
            "B          |     4 |      3 |   0.2924 |     459.06 |  4.602e-02\n",
            "B          |     8 |      1 |   0.2862 |     468.92 |  4.602e-02\n",
            "B          |     8 |      2 |   0.2867 |     468.11 |  4.602e-02\n",
            "B          |     8 |      3 |   0.2861 |     469.17 |  4.602e-02\n",
            "====================================================================================================\n",
            "Notes:\n",
            " - Kernel A performs 1 read + 1 write per element.\n",
            " - Kernel B performs 3 reads + 1 write per element.\n",
            " - num_warps affects parallelism and register pressure.\n",
            " - num_stages affects pipelining of memory loads.\n",
            " - For large D, kernel B scales better.\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "# ============================================================\n",
        "# Day 4: Numerically Stable Softmax Skeleton (NO SOLUTION)\n",
        "# Goal:\n",
        "#   - Implement row-wise stable softmax: y = exp(x - max) / sum(exp(x - max))\n",
        "#   - Handle any D (not necessarily power of two)\n",
        "#   - Validate vs torch.softmax\n",
        "# ============================================================\n",
        "    # TODO:\n",
        "    # 1) pid = tl.program_id(0) for row index\n",
        "    # 2) offsets = tl.arange(0, BLOCK_D)\n",
        "    # 3) load x row block(s) with mask\n",
        "    # 4) compute max over D (may need multiple loads if D > BLOCK_D)\n",
        "    # 5) compute exp(x - max), sum, and normalize\n",
        "    #\n",
        "    # Minimal baseline is \"one program handles one row\", and choose BLOCK_D >= D for first version.\n",
        "    # Then extend to D > BLOCK_D using multiple chunks.\n",
        "# ============================================================\n",
        "# Kernel A: One program handles one row\n",
        "# Requirement: BLOCK_D >= D\n",
        "# Single-pass: read once, write once\n",
        "# ============================================================\n",
        "\n",
        "@triton.jit\n",
        "def softmax_kernel_A(\n",
        "    x_ptr, y_ptr,\n",
        "    B, D,\n",
        "    stride_xb, stride_xd,\n",
        "    stride_yb, stride_yd,\n",
        "    BLOCK_D: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(0)  # row index\n",
        "    offs = tl.arange(0, BLOCK_D)\n",
        "\n",
        "    x_row_ptr = x_ptr + pid * stride_xb + offs * stride_xd\n",
        "    y_row_ptr = y_ptr + pid * stride_yb + offs * stride_yd\n",
        "\n",
        "    mask = offs < D\n",
        "\n",
        "    x = tl.load(x_row_ptr, mask=mask, other=-float(\"inf\"))\n",
        "    x_max = tl.max(x, axis=0)\n",
        "    ex = tl.exp(x - x_max)\n",
        "    denom = tl.sum(ex, axis=0)\n",
        "    y = ex / denom\n",
        "\n",
        "    tl.store(y_row_ptr, y, mask=mask)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Kernel B: Chunked version (supports D > BLOCK_D)\n",
        "# 3 passes over the row\n",
        "# D must be constexpr\n",
        "# ============================================================\n",
        "\n",
        "@triton.jit\n",
        "def softmax_kernel_B(\n",
        "    x_ptr, y_ptr,\n",
        "    B,\n",
        "    stride_xb, stride_xd,\n",
        "    stride_yb, stride_yd,\n",
        "    D: tl.constexpr,\n",
        "    BLOCK_D: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(0)\n",
        "    offs = tl.arange(0, BLOCK_D)\n",
        "\n",
        "    # Pass 1: compute row max\n",
        "    row_max = -float(\"inf\")\n",
        "    for start in tl.static_range(0, D, BLOCK_D):\n",
        "        cols = start + offs\n",
        "        mask = cols < D\n",
        "        x = tl.load(x_ptr + pid * stride_xb + cols * stride_xd,\n",
        "                    mask=mask, other=-float(\"inf\"))\n",
        "        row_max = tl.maximum(row_max, tl.max(x, axis=0))\n",
        "\n",
        "    # Pass 2: compute denominator\n",
        "    denom = 0.0\n",
        "    for start in tl.static_range(0, D, BLOCK_D):\n",
        "        cols = start + offs\n",
        "        mask = cols < D\n",
        "        x = tl.load(x_ptr + pid * stride_xb + cols * stride_xd,\n",
        "                    mask=mask, other=-float(\"inf\"))\n",
        "        denom += tl.sum(tl.exp(x - row_max), axis=0)\n",
        "\n",
        "    # Pass 3: write output\n",
        "    for start in tl.static_range(0, D, BLOCK_D):\n",
        "        cols = start + offs\n",
        "        mask = cols < D\n",
        "        x = tl.load(x_ptr + pid * stride_xb + cols * stride_xd,\n",
        "                    mask=mask, other=-float(\"inf\"))\n",
        "        y = tl.exp(x - row_max) / denom\n",
        "        tl.store(y_ptr + pid * stride_yb + cols * stride_yd,\n",
        "                 y, mask=mask)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Benchmark helpers\n",
        "# ============================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def bench(fn, iters=200, warmup=50):\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    start.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    return start.elapsed_time(end) / iters  # ms/iter\n",
        "\n",
        "\n",
        "def max_abs_err(a, b):\n",
        "    return float((a - b).abs().max().item())\n",
        "\n",
        "\n",
        "def compute_gbps(bytes_processed, ms):\n",
        "    seconds = ms / 1000.0\n",
        "    return bytes_processed / seconds / 1e9\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Main experiment\n",
        "# ============================================================\n",
        "\n",
        "def main():\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    device = \"cuda\"\n",
        "    dtype = torch.float32\n",
        "    elem_size = torch.tensor([], dtype=dtype).element_size()\n",
        "\n",
        "    B, D = 4096, 2048\n",
        "    x = torch.randn((B, D), device=device, dtype=dtype)\n",
        "    y_ref = torch.softmax(x, dim=1)\n",
        "\n",
        "    print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Shape: B={B}, D={D}, dtype={dtype}\")\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "    warps_list = [2, 4, 8]\n",
        "    stages_list = [1, 2, 3]\n",
        "\n",
        "    print(f\"{'Kernel':<10} | {'warps':>5} | {'stages':>6} | {'ms':>8} | {'GB/s':>10} | {'max_err':>10}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    # ------------------------\n",
        "    # Kernel A\n",
        "    # ------------------------\n",
        "    BLOCK_D_A = triton.next_power_of_2(D)\n",
        "\n",
        "    for w in warps_list:\n",
        "        for s in stages_list:\n",
        "\n",
        "            def run_A():\n",
        "                y = torch.empty_like(x)\n",
        "                grid = (B,)\n",
        "                softmax_kernel_A[grid](\n",
        "                    x, y,\n",
        "                    B, D,\n",
        "                    x.stride(0), x.stride(1),\n",
        "                    y.stride(0), y.stride(1),\n",
        "                    BLOCK_D=BLOCK_D_A,\n",
        "                    num_warps=w,\n",
        "                    num_stages=s,\n",
        "                )\n",
        "                return y\n",
        "\n",
        "            yA = run_A()\n",
        "            err = max_abs_err(yA, y_ref)\n",
        "\n",
        "            ms = bench(run_A)\n",
        "\n",
        "            bytes_processed = B * D * elem_size * 2  # 1 read + 1 write\n",
        "            gbps = compute_gbps(bytes_processed, ms)\n",
        "\n",
        "            print(f\"{'A':<10} | {w:5d} | {s:6d} | {ms:8.4f} | {gbps:10.2f} | {err:10.3e}\")\n",
        "\n",
        "    # ------------------------\n",
        "    # Kernel B\n",
        "    # ------------------------\n",
        "    BLOCK_D_B = 1024\n",
        "\n",
        "    for w in warps_list:\n",
        "        for s in stages_list:\n",
        "\n",
        "            def run_B():\n",
        "                y = torch.empty_like(x)\n",
        "                grid = (B,)\n",
        "                softmax_kernel_B[grid](\n",
        "                    x, y,\n",
        "                    B,\n",
        "                    x.stride(0), x.stride(1),\n",
        "                    y.stride(0), y.stride(1),\n",
        "                    D=D,\n",
        "                    BLOCK_D=BLOCK_D_B,\n",
        "                    num_warps=w,\n",
        "                    num_stages=s,\n",
        "                )\n",
        "                return y\n",
        "\n",
        "            yB = run_B()\n",
        "            err = max_abs_err(yB, y_ref)\n",
        "\n",
        "            ms = bench(run_B)\n",
        "\n",
        "            bytes_processed = B * D * elem_size * 4  # 3 reads + 1 write\n",
        "            gbps = compute_gbps(bytes_processed, ms)\n",
        "\n",
        "            print(f\"{'B':<10} | {w:5d} | {s:6d} | {ms:8.4f} | {gbps:10.2f} | {err:10.3e}\")\n",
        "\n",
        "    print(\"=\" * 100)\n",
        "    print(\"Notes:\")\n",
        "    print(\" - Kernel A performs 1 read + 1 write per element.\")\n",
        "    print(\" - Kernel B performs 3 reads + 1 write per element.\")\n",
        "    print(\" - num_warps affects parallelism and register pressure.\")\n",
        "    print(\" - num_stages affects pipelining of memory loads.\")\n",
        "    print(\" - For large D, kernel B scales better.\")\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "bvgBmpFQWwhw"
      },
      "outputs": [],
      "source": [
        "#layernorm_skeleton\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ============================================================\n",
        "# Day 5: LayerNorm Forward Skeleton (NO SOLUTION)\n",
        "# Goal:\n",
        "#   - Implement LayerNorm forward:\n",
        "#       y = (x - mean) * rsqrt(var + eps) * gamma + beta\n",
        "#   - Validate vs torch.nn.functional.layer_norm\n",
        "# ============================================================\n",
        "\n",
        "@triton.jit\n",
        "def layernorm_fwd_kernel(\n",
        "    x_ptr, gamma_ptr, beta_ptr, y_ptr,\n",
        "    B, D,\n",
        "    stride_xb, stride_xd,\n",
        "    stride_yb, stride_yd,\n",
        "    eps,\n",
        "    BLOCK_D: tl.constexpr,\n",
        "):\n",
        "    # TODO:\n",
        "    # 1) pid = tl.program_id(0) => row\n",
        "    # 2) load x row (possibly in chunks if D > BLOCK_D)\n",
        "    # 3) compute mean and var\n",
        "    # 4) normalize + affine\n",
        "    # 5) store y\n",
        "    raise NotImplementedError(\"TODO: implement layernorm_fwd_kernel\")\n",
        "\n",
        "def triton_layernorm(x: torch.Tensor, gamma: torch.Tensor, beta: torch.Tensor,\n",
        "                     eps=1e-5, BLOCK_D=1024):\n",
        "    assert x.is_cuda and gamma.is_cuda and beta.is_cuda\n",
        "    assert x.dtype == torch.float32 and gamma.dtype == torch.float32 and beta.dtype == torch.float32\n",
        "    assert x.is_contiguous() and gamma.is_contiguous() and beta.is_contiguous()\n",
        "    B, D = x.shape\n",
        "    y = torch.empty_like(x)\n",
        "    grid = (B,)\n",
        "    layernorm_fwd_kernel[grid](\n",
        "        x, gamma, beta, y,\n",
        "        B, D,\n",
        "        x.stride(0), x.stride(1),\n",
        "        y.stride(0), y.stride(1),\n",
        "        eps,\n",
        "        BLOCK_D=BLOCK_D,\n",
        "        num_warps=4,\n",
        "    )\n",
        "    return y\n",
        "\n",
        "@torch.no_grad()\n",
        "def check_layernorm():\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA not available.\")\n",
        "        return\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    B, D = 4096, 1024\n",
        "    x = torch.randn((B, D), device=device, dtype=torch.float32)\n",
        "    gamma = torch.randn((D,), device=device, dtype=torch.float32)\n",
        "    beta = torch.randn((D,), device=device, dtype=torch.float32)\n",
        "    eps = 1e-5\n",
        "\n",
        "    y_ref = F.layer_norm(x, (D,), gamma, beta, eps=eps)\n",
        "\n",
        "    try:\n",
        "        y_tri = triton_layernorm(x, gamma, beta, eps=eps, BLOCK_D=1024)\n",
        "    except NotImplementedError as e:\n",
        "        print(f\"[Day5] layernorm kernel not implemented yet: {e}\")\n",
        "        return\n",
        "\n",
        "    max_err = (y_tri - y_ref).abs().max().item()\n",
        "    mean_err = (y_tri - y_ref).abs().mean().item()\n",
        "    print(f\"[Day5] max_abs_err = {max_err:.3e}, mean_abs_err = {mean_err:.3e}\")\n",
        "    assert max_err < 2e-4, \"Too large error for fp32 layernorm.\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_layernorm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "FLVSC7ZuWwhw"
      },
      "outputs": [],
      "source": [
        "#benchmark_skeleton\n",
        "import torch\n",
        "import time\n",
        "import importlib\n",
        "\n",
        "# ============================================================\n",
        "# Day 6: Benchmark Triton vs PyTorch for Day3/4/5\n",
        "# - If kernel not implemented, it will skip and print a message.\n",
        "# - Uses CUDA events for timing.\n",
        "# ============================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def bench_ms(fn, iters=200, warmup=50):\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "    s = torch.cuda.Event(enable_timing=True)\n",
        "    e = torch.cuda.Event(enable_timing=True)\n",
        "    s.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    e.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return s.elapsed_time(e) / iters\n",
        "\n",
        "def print_table(rows):\n",
        "    header = f\"{'Op':<10} | {'Impl':<8} | {'ms/iter':>10} | {'speedup':>8} | {'note':<20}\"\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "    for r in rows:\n",
        "        print(f\"{r['op']:<10} | {r['impl']:<8} | {r['ms']:>10.4f} | {r['speedup']:>8.2f} | {r['note']:<20}\")\n",
        "\n",
        "def main():\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA not available.\")\n",
        "        return\n",
        "\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    # -------------------------\n",
        "    # Day3 MatMul\n",
        "    # -------------------------\n",
        "    try:\n",
        "        day3 = importlib.import_module(\"day3_matmul_skeleton\")\n",
        "        M, K, N = 1024, 1024, 1024\n",
        "        A = torch.randn((M, K), device=device, dtype=torch.float16)\n",
        "        B = torch.randn((K, N), device=device, dtype=torch.float16)\n",
        "\n",
        "        torch_fn = lambda: A @ B\n",
        "        torch_ms = bench_ms(torch_fn, iters=100, warmup=20)\n",
        "\n",
        "        def triton_fn():\n",
        "            return day3.triton_matmul(A, B, BLOCK_M=128, BLOCK_N=128, BLOCK_K=32, num_warps=8)\n",
        "\n",
        "        try:\n",
        "            tri_ms = bench_ms(triton_fn, iters=100, warmup=20)\n",
        "            rows.append({\"op\":\"matmul\", \"impl\":\"torch\", \"ms\":torch_ms, \"speedup\":1.0, \"note\":\"torch.matmul\"})\n",
        "            rows.append({\"op\":\"matmul\", \"impl\":\"triton\", \"ms\":tri_ms, \"speedup\":torch_ms/tri_ms, \"note\":\"BM=128 BN=128\"})\n",
        "        except NotImplementedError as e:\n",
        "            rows.append({\"op\":\"matmul\", \"impl\":\"triton\", \"ms\":float(\"nan\"), \"speedup\":float(\"nan\"), \"note\":\"TODO kernel\"})\n",
        "    except Exception as e:\n",
        "        rows.append({\"op\":\"matmul\", \"impl\":\"triton\", \"ms\":float(\"nan\"), \"speedup\":float(\"nan\"), \"note\":f\"import fail: {e}\"})\n",
        "\n",
        "\n",
        "    # -------------------------\n",
        "    # Day4 Softmax\n",
        "    # -------------------------\n",
        "    try:\n",
        "        day4 = importlib.import_module(\"day4_softmax_skeleton\")\n",
        "        Bsz, D = 4096, 1024\n",
        "        x = torch.randn((Bsz, D), device=device, dtype=torch.float32)\n",
        "\n",
        "        torch_fn = lambda: torch.softmax(x, dim=1)\n",
        "        torch_ms = bench_ms(torch_fn)\n",
        "\n",
        "        def triton_fn():\n",
        "            return day4.triton_softmax(x, BLOCK_D=1024)\n",
        "\n",
        "        try:\n",
        "            tri_ms = bench_ms(triton_fn)\n",
        "            rows.append({\"op\":\"softmax\", \"impl\":\"torch\", \"ms\":torch_ms, \"speedup\":1.0, \"note\":\"torch.softmax\"})\n",
        "            rows.append({\"op\":\"softmax\", \"impl\":\"triton\", \"ms\":tri_ms, \"speedup\":torch_ms/tri_ms, \"note\":\"BLOCK_D=1024\"})\n",
        "        except NotImplementedError as e:\n",
        "            rows.append({\"op\":\"softmax\", \"impl\":\"triton\", \"ms\":float(\"nan\"), \"speedup\":float(\"nan\"), \"note\":\"TODO kernel\"})\n",
        "    except Exception as e:\n",
        "        rows.append({\"op\":\"softmax\", \"impl\":\"triton\", \"ms\":float(\"nan\"), \"speedup\":float(\"nan\"), \"note\":f\"import fail: {e}\"})\n",
        "\n",
        "\n",
        "    # -------------------------\n",
        "    # Day5 LayerNorm\n",
        "    # -------------------------\n",
        "    try:\n",
        "        day5 = importlib.import_module(\"day5_layernorm_skeleton\")\n",
        "        Bsz, D = 4096, 1024\n",
        "        x = torch.randn((Bsz, D), device=device, dtype=torch.float32)\n",
        "        gamma = torch.randn((D,), device=device, dtype=torch.float32)\n",
        "        beta = torch.randn((D,), device=device, dtype=torch.float32)\n",
        "        eps = 1e-5\n",
        "\n",
        "        torch_fn = lambda: torch.nn.functional.layer_norm(x, (D,), gamma, beta, eps=eps)\n",
        "        torch_ms = bench_ms(torch_fn)\n",
        "\n",
        "        def triton_fn():\n",
        "            return day5.triton_layernorm(x, gamma, beta, eps=eps, BLOCK_D=1024)\n",
        "\n",
        "        try:\n",
        "            tri_ms = bench_ms(triton_fn)\n",
        "            rows.append({\"op\":\"layernorm\", \"impl\":\"torch\", \"ms\":torch_ms, \"speedup\":1.0, \"note\":\"F.layer_norm\"})\n",
        "            rows.append({\"op\":\"layernorm\", \"impl\":\"triton\", \"ms\":tri_ms, \"speedup\":torch_ms/tri_ms, \"note\":\"BLOCK_D=1024\"})\n",
        "        except NotImplementedError as e:\n",
        "            rows.append({\"op\":\"layernorm\", \"impl\":\"triton\", \"ms\":float(\"nan\"), \"speedup\":float(\"nan\"), \"note\":\"TODO kernel\"})\n",
        "    except Exception as e:\n",
        "        rows.append({\"op\":\"layernorm\", \"impl\":\"triton\", \"ms\":float(\"nan\"), \"speedup\":float(\"nan\"), \"note\":f\"import fail: {e}\"})\n",
        "\n",
        "\n",
        "    print()\n",
        "    print_table(rows)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}