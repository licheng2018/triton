{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs_bXqDJWwhs"
      },
      "source": [
        "## Triton Core Kernels & Benchmarking\n",
        "\n",
        "üéØ **Weekly Goal**  \n",
        "Implement and tune three core ML kernels (MatMul, Softmax, LayerNorm) using Triton,  \n",
        "develop intuition for **block sizes, `num_warps`, numerical stability, and kernel fusion**,  \n",
        "and perform systematic benchmarking against PyTorch to understand real performance trade-offs.\n",
        "\n",
        "---\n",
        "\n",
        "## Tuned MatMul (Block Size / `num_warps`)\n",
        "\n",
        "### Objective\n",
        "Implement a tile-based Triton GEMM kernel and tune **`BLOCK_M / BLOCK_N / BLOCK_K`**\n",
        "and **`num_warps`** to study their impact on throughput, occupancy, and register pressure.\n",
        "\n",
        "### Tasks\n",
        "- [ ] Implement a basic Triton matmul kernel (tile-based)\n",
        "- [ ] Define `BLOCK_M / BLOCK_N / BLOCK_K` as `tl.constexpr`\n",
        "- [ ] Evaluate multiple tile configurations (e.g., 64√ó64√ó32, 128√ó128√ó32)\n",
        "- [ ] Sweep `num_warps ‚àà {4, 8}` and compare performance\n",
        "- [ ] Compare against `torch.matmul` as a baseline\n",
        "\n",
        "### Key Concepts\n",
        "- **Tiling**: each program computes one output tile\n",
        "- **Block size** controls arithmetic intensity (FLOPs / byte)\n",
        "- **`num_warps`** trades off parallelism vs. register usage\n",
        "- GEMM is typically **compute-bound**, unlike vector add\n",
        "\n",
        "### Deliverables\n",
        "- Runnable `triton_matmul.py`\n",
        "- Performance table (ms / TFLOPS) for different configurations\n",
        "- Short analysis identifying the best configuration and why\n",
        "\n",
        "---\n",
        "\n",
        "## Triton Softmax (Numerically Stable)\n",
        "\n",
        "### Objective\n",
        "Implement a numerically stable row-wise softmax kernel in Triton,\n",
        "focusing on **max-subtraction**, reduction patterns, and block/warp mapping.\n",
        "\n",
        "### Tasks\n",
        "- [ ] Implement row-wise softmax in Triton\n",
        "- [ ] Apply `x - max(x)` for numerical stability\n",
        "- [ ] Decompose into two stages:\n",
        "  - max reduction\n",
        "  - exp + sum reduction\n",
        "- [ ] Correctly handle arbitrary feature dimensions (non-power-of-two)\n",
        "- [ ] Compare against `torch.softmax`\n",
        "\n",
        "### Key Concepts\n",
        "- **Numerical stability** for exponential operations\n",
        "- **Reduction patterns** within a program\n",
        "- Softmax is often **memory-bound with reductions**\n",
        "- Triton enables explicit control over reduction structure\n",
        "\n",
        "### Deliverables\n",
        "- `triton_softmax.py`\n",
        "- Correctness check vs. PyTorch (max / mean error)\n",
        "- Performance comparison table (ms / GB/s)\n",
        "\n",
        "---\n",
        "\n",
        "## Triton LayerNorm\n",
        "\n",
        "### Objective\n",
        "Implement Triton LayerNorm (forward pass) and understand\n",
        "**mean/variance reductions**, `eps` stabilization, and the performance benefits of kernel fusion.\n",
        "\n",
        "### Tasks\n",
        "- [ ] Implement LayerNorm forward in Triton\n",
        "- [ ] Compute per-row mean and variance\n",
        "- [ ] Apply `rsqrt(var + eps)`\n",
        "- [ ] Support affine parameters (`gamma`, `beta`)\n",
        "- [ ] Compare against `torch.nn.functional.layer_norm`\n",
        "\n",
        "### Key Concepts\n",
        "- Two reductions: mean ‚Üí variance\n",
        "- **Kernel fusion**: normalization + affine in a single kernel\n",
        "- LayerNorm is typically **memory-bound with reductions**\n",
        "- Triton avoids intermediate tensor materialization\n",
        "\n",
        "### Deliverables\n",
        "- `triton_layernorm.py`\n",
        "- Correctness validation (max / mean error)\n",
        "- Triton vs. PyTorch performance comparison\n",
        "\n",
        "---\n",
        "\n",
        "## Benchmark: Triton vs PyTorch\n",
        "\n",
        "### Objective\n",
        "Systematically benchmark Triton kernels against PyTorch eager kernels\n",
        "to identify **when Triton wins, when it does not, and why**.\n",
        "\n",
        "### Tasks\n",
        "- [ ] Build a unified benchmark framework (CUDA events)\n",
        "- [ ] Compare the following operators:\n",
        "  - vector add\n",
        "  - fused add + ReLU\n",
        "  - softmax\n",
        "  - layernorm\n",
        "- [ ] Record:\n",
        "  - latency (ms)\n",
        "  - effective bandwidth / FLOPs\n",
        "  - speedup\n",
        "- [ ] Repeat experiments across different tensor sizes\n",
        "\n",
        "### Key Concepts\n",
        "- **Bandwidth-bound vs. compute-bound** kernels\n",
        "- Kernel launch overhead\n",
        "- Real benefits of operator fusion\n",
        "- Why Triton excels at fused kernels rather than single primitive ops\n",
        "\n",
        "### Deliverables\n",
        "- `benchmark_triton_vs_torch.py`\n",
        "- Unified comparison table (Markdown / CSV)\n",
        "- Summary covering:\n",
        "  - which kernels benefit most from Triton\n",
        "  - which PyTorch kernels are already near-optimal\n",
        "  - implications for ML systems optimization\n",
        "\n",
        "---\n",
        "\n",
        "## End-of-Week Takeaways\n",
        "\n",
        "- Triton is **not** ‚Äúa faster PyTorch‚Äù\n",
        "- Triton enables **CUDA-level kernel design in Python**\n",
        "- The real performance gains come from:\n",
        "  - kernel fusion\n",
        "  - explicit reduction control\n",
        "  - tile-aware kernel design\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#matmul_skeleton\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "# ============================================================\n",
        "# Day 3: Tuned MatMul Skeleton (NO SOLUTION)\n",
        "# Goal:\n",
        "#   - Implement a tile-based matmul kernel in Triton\n",
        "#   - Tune BLOCK_M / BLOCK_N / BLOCK_K and num_warps\n",
        "#   - Validate vs torch.matmul\n",
        "#\n",
        "# Notes:\n",
        "#   - Assume A: [M, K], B: [K, N], C: [M, N], fp16 inputs, fp16 output (acc fp32).\n",
        "#   - You may start with fp16 and accumulate in fp32.\n",
        "# ============================================================\n",
        "\n",
        "@triton.jit\n",
        "def matmul_kernel(\n",
        "    a_ptr, b_ptr, c_ptr,\n",
        "    M, N, K,\n",
        "    stride_am, stride_ak,\n",
        "    stride_bk, stride_bn,\n",
        "    stride_cm, stride_cn,\n",
        "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
        "):\n",
        "    # TODO:\n",
        "    # 1) program ids for 2D tiling: pid_m, pid_n\n",
        "    # 2) compute offsets for A tile and B tile\n",
        "    # 3) loop over K tiles:\n",
        "    #    - tl.load A and B tiles with masks\n",
        "    #    - accumulate using tl.dot / manual FMA\n",
        "    # 4) tl.store to C with mask for M,N boundaries\n",
        "    #\n",
        "    # Hints:\n",
        "    #   pid_m = tl.program_id(0)\n",
        "    #   pid_n = tl.program_id(1)\n",
        "    #   offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    #   offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    #   offs_k = tl.arange(0, BLOCK_K)\n",
        "    #\n",
        "    #   use tl.multiple_of / tl.assume if needed (optional)\n",
        "    # raise NotImplementedError(\"TODO: implement matmul_kernel\")\n",
        "\n",
        "    # Compute row/col offsets for this program's C tile\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    # Compute row/col offsets for this program's C tile\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "\n",
        "\n",
        "    m_mask = offs_m < M          # [BM]\n",
        "    n_mask = offs_n < N          # [BN]\n",
        "\n",
        "    for k0 in range(0, K, BLOCK_K):\n",
        "        # current K indices for this chunk\n",
        "        k_offsets = k0 + offs_k          # [BK]\n",
        "        k_mask = k_offsets < K           # [BK]\n",
        "\n",
        "        # build pointer grids for this chunk\n",
        "        a_ptrs = a_ptr + offs_m[:, None] * stride_am + k_offsets[None, :] * stride_ak   # [BM, BK]\n",
        "        b_ptrs = b_ptr + k_offsets[:, None] * stride_bk + offs_n[None, :] * stride_bn   # [BK, BN]\n",
        "\n",
        "        # 2D masks for loads\n",
        "        a_load_mask = m_mask[:, None] & k_mask[None, :]    # [BM, BK]\n",
        "        b_load_mask = k_mask[:, None] & n_mask[None, :]    # [BK, BN]\n",
        "\n",
        "        # masked loads: out-of-bounds => 0\n",
        "        a_tile = tl.load(a_ptrs, mask=a_load_mask, other=0.0)  # [BM, BK], fp16/bf16\n",
        "        b_tile = tl.load(b_ptrs, mask=b_load_mask, other=0.0)  # [BK, BN], fp16/bf16\n",
        "\n",
        "        # accumulate (fp32)\n",
        "        # tl.dot will typically accumulate in fp32 when acc is fp32\n",
        "        acc += tl.dot(a_tile, b_tile)\n",
        "\n",
        "    c_tile = acc.to(tl.float16)\n",
        "    c_ptrs = c_ptr + offs_m[:,None]*stride_cm + offs_n[None,:]*stride_cn\n",
        "    tl.store(c_ptrs, c_tile, mask=m_mask[:,None] & n_mask[None,:])\n",
        "\n",
        "def triton_matmul(A: torch.Tensor, B: torch.Tensor,\n",
        "                  BLOCK_M=128, BLOCK_N=128, BLOCK_K=32,\n",
        "                  num_warps=8):\n",
        "    assert A.is_cuda and B.is_cuda\n",
        "    assert A.dtype in (torch.float16, torch.bfloat16)\n",
        "    assert B.dtype in (torch.float16, torch.bfloat16)\n",
        "    assert A.is_contiguous() and B.is_contiguous()\n",
        "    M, K = A.shape\n",
        "    K2, N = B.shape\n",
        "    assert K == K2\n",
        "\n",
        "    C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n",
        "\n",
        "    grid = (\n",
        "        triton.cdiv(M, BLOCK_M),\n",
        "        triton.cdiv(N, BLOCK_N),\n",
        "    )\n",
        "\n",
        "    matmul_kernel[grid](\n",
        "        A, B, C,\n",
        "        M, N, K,\n",
        "        A.stride(0), A.stride(1),\n",
        "        B.stride(0), B.stride(1),\n",
        "        C.stride(0), C.stride(1),\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n",
        "        num_warps=num_warps,   # Triton launch meta\n",
        "    )\n",
        "    return C\n",
        "\n",
        "@torch.no_grad()\n",
        "def check_matmul():\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA not available.\")\n",
        "        return\n",
        "\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    M, K, N = 512, 1024, 768\n",
        "    A = torch.randn((M, K), device=device, dtype=torch.float16)\n",
        "    B = torch.randn((K, N), device=device, dtype=torch.float16)\n",
        "\n",
        "    # PyTorch baseline\n",
        "    C_ref = A @ B\n",
        "\n",
        "\n",
        "    # Triton (will fail until you implement kernel)\n",
        "    try:\n",
        "        C_tri = triton_matmul(A, B, BLOCK_M=128, BLOCK_N=128, BLOCK_K=32, num_warps=8)\n",
        "    except NotImplementedError as e:\n",
        "        print(f\"[Day3] matmul kernel not implemented yet: {e}\")\n",
        "        return\n",
        "\n",
        "\n",
        "    C_ref_fp16 = A @ B                 # PyTorch fp16 Ë∑ØÂæÑ\n",
        "    C_ref_fp32 = A.float() @ B.float() # Êõ¥‰∏•Ê†º reference\n",
        "\n",
        "    # ‰Ω†ÁöÑËæìÂá∫ C_tri ÊòØ fp16\n",
        "    print(\"err vs torch fp16:\", (C_tri - C_ref_fp16).abs().max().item())\n",
        "    print(\"err vs fp32 ref  :\", (C_tri.float() - C_ref_fp32).abs().max().item())\n",
        "\n",
        "    # correctness\n",
        "    max_err = (C_tri - C_ref).abs().max().item()\n",
        "    print(f\"[Day3] max_abs_err = {max_err:.3e}\")\n",
        "    # You can tighten thresholds after tuning\n",
        "    assert max_err < 7e-2, \"Too large error (fp16). Improve implementation.\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_matmul()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvND3OxsZRtf",
        "outputId": "bdedb454-bcc6-4ee4-fcd3-f60e3df27fc0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "err vs torch fp16: 0.0625\n",
            "err vs fp32 ref  : 0.0601959228515625\n",
            "[Day3] max_abs_err = 6.250e-02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "D0leVK_gWwhu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcfef703-7b27-4c73-e853-6671203dcbaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "err vs torch fp16: 0.0625\n",
            "err vs fp32 ref  : 0.0601959228515625\n",
            "[Day3] max_abs_err = 6.250e-02\n"
          ]
        }
      ],
      "source": [
        "#matmul_skeleton soft_pipeline\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "# ============================================================\n",
        "# Day 3: Tuned MatMul Skeleton (NO SOLUTION)\n",
        "# Goal:\n",
        "#   - Implement a tile-based matmul kernel in Triton\n",
        "#   - Tune BLOCK_M / BLOCK_N / BLOCK_K and num_warps\n",
        "#   - Validate vs torch.matmul\n",
        "#\n",
        "# Notes:\n",
        "#   - Assume A: [M, K], B: [K, N], C: [M, N], fp16 inputs, fp16 output (acc fp32).\n",
        "#   - You may start with fp16 and accumulate in fp32.\n",
        "# ============================================================\n",
        "\n",
        "@triton.jit\n",
        "def matmul_kernel(\n",
        "    a_ptr, b_ptr, c_ptr,\n",
        "    M, N, K,\n",
        "    stride_am, stride_ak,\n",
        "    stride_bk, stride_bn,\n",
        "    stride_cm, stride_cn,\n",
        "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
        "    MAX_K_TILES: tl.constexpr,            # [NEW] meta: static_range upperboundÔºàavoid 1024 compileÔºâ\n",
        "    OUT_DTYPE: tl.constexpr,              # [NEW] meta: 0->fp16, 1->bf16\n",
        "):\n",
        "    # TODO:\n",
        "    # 1) program ids for 2D tiling: pid_m, pid_n\n",
        "    # 2) compute offsets for A tile and B tile\n",
        "    # 3) loop over K tiles:\n",
        "    #    - tl.load A and B tiles with masks\n",
        "    #    - accumulate using tl.dot / manual FMA\n",
        "    # 4) tl.store to C with mask for M,N boundaries\n",
        "    #\n",
        "    # Hints:\n",
        "    #   pid_m = tl.program_id(0)\n",
        "    #   pid_n = tl.program_id(1)\n",
        "    #   offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    #   offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    #   offs_k = tl.arange(0, BLOCK_K)\n",
        "    #\n",
        "    #   use tl.multiple_of / tl.assume if needed (optional)\n",
        "    # raise NotImplementedError(\"TODO: implement matmul_kernel\")\n",
        "\n",
        "    # Compute row/col offsets for this program's C tile\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    # Compute row/col offsets for this program's C tile\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "\n",
        "\n",
        "    m_mask = offs_m < M          # [BM]\n",
        "    n_mask = offs_n < N          # [BN]\n",
        "    #for k0 in range(0, K, BLOCK_K):\n",
        "        #k_offsets = k0 + offs_k          # [BK]\n",
        "    # Software Pipelining\n",
        "    #time ‚Üí\n",
        "    #    iter 0:  load\n",
        "    #    iter 1:       load     compute\n",
        "    #    iter 2:              load     compute\n",
        "    #    iter 3:                     load     compute\n",
        "    k_tiles = tl.cdiv(K, BLOCK_K)\n",
        "    for k_it in tl.static_range(0, MAX_K_TILES):\n",
        "        # if k_it >= k_tiles:\n",
        "        #    break\n",
        "        valid_k_iter = k_it < k_tiles\n",
        "\n",
        "        # k0 = k_it * BLOCK_K\n",
        "        # k_offsets = k0 + offs_k\n",
        "        k_offsets = k_it * BLOCK_K + offs_k\n",
        "\n",
        "        k_mask = k_offsets < K           # [BK]\n",
        "\n",
        "        final_k_mask = valid_k_iter & k_mask   # [BK]\n",
        "\n",
        "        # build pointer grids for this chunk\n",
        "        a_ptrs = a_ptr + offs_m[:, None] * stride_am + k_offsets[None, :] * stride_ak   # [BM, BK]\n",
        "        b_ptrs = b_ptr + k_offsets[:, None] * stride_bk + offs_n[None, :] * stride_bn   # [BK, BN]\n",
        "\n",
        "        # 2D masks for loads\n",
        "        a_load_mask = m_mask[:, None] & final_k_mask[None, :]    # [BM, BK]\n",
        "        b_load_mask = final_k_mask[:, None] & n_mask[None, :]    # [BK, BN]\n",
        "\n",
        "        # masked loads: out-of-bounds => 0\n",
        "        a_tile = tl.load(a_ptrs, mask=a_load_mask, other=0.0)  # [BM, BK], fp16/bf16\n",
        "        b_tile = tl.load(b_ptrs, mask=b_load_mask, other=0.0)  # [BK, BN], fp16/bf16\n",
        "\n",
        "        # accumulate (fp32)\n",
        "        # tl.dot will typically accumulate in fp32 when acc is fp32\n",
        "        acc += tl.dot(a_tile, b_tile)\n",
        "\n",
        "    c_tile = tl.where(OUT_DTYPE == 1, acc.to(tl.bfloat16), acc.to(tl.float16))\n",
        "    c_ptrs = c_ptr + offs_m[:,None]*stride_cm + offs_n[None,:]*stride_cn\n",
        "    tl.store(c_ptrs, c_tile, mask=m_mask[:,None] & n_mask[None,:])\n",
        "\n",
        "def triton_matmul(A: torch.Tensor, B: torch.Tensor,\n",
        "                  BLOCK_M=128, BLOCK_N=128, BLOCK_K=32,\n",
        "                  num_warps=8, num_stages=4):\n",
        "    assert A.is_cuda and B.is_cuda\n",
        "    assert A.dtype in (torch.float16, torch.bfloat16)\n",
        "    assert B.dtype in (torch.float16, torch.bfloat16)\n",
        "    assert A.is_contiguous() and B.is_contiguous()\n",
        "    M, K = A.shape\n",
        "    K2, N = B.shape\n",
        "    assert K == K2\n",
        "    assert A.dtype == B.dtype, \"For now, require A and B have same dtype (fp16 or bf16).\"\n",
        "\n",
        "    C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n",
        "\n",
        "    grid = (\n",
        "        triton.cdiv(M, BLOCK_M),\n",
        "        triton.cdiv(N, BLOCK_N),\n",
        "    )\n",
        "\n",
        "    MAX_K_TILES = triton.cdiv(K, BLOCK_K)\n",
        "\n",
        "    OUT_DTYPE = 1 if A.dtype == torch.bfloat16 else 0\n",
        "\n",
        "    matmul_kernel[grid](\n",
        "        A, B, C,\n",
        "        M, N, K,\n",
        "        A.stride(0), A.stride(1),\n",
        "        B.stride(0), B.stride(1),\n",
        "        C.stride(0), C.stride(1),\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n",
        "        MAX_K_TILES=MAX_K_TILES,\n",
        "        OUT_DTYPE=OUT_DTYPE,\n",
        "        num_warps=num_warps,   # Triton launch meta\n",
        "        num_stages=num_stages,\n",
        "\n",
        "    )\n",
        "    return C\n",
        "\n",
        "@torch.no_grad()\n",
        "def check_matmul():\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA not available.\")\n",
        "        return\n",
        "\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    M, K, N = 512, 1024, 768\n",
        "    A = torch.randn((M, K), device=device, dtype=torch.float16)\n",
        "    B = torch.randn((K, N), device=device, dtype=torch.float16)\n",
        "\n",
        "    # PyTorch baseline\n",
        "    C_ref = A @ B\n",
        "\n",
        "    # Triton (will fail until you implement kernel)\n",
        "    try:\n",
        "        C_tri = triton_matmul(A, B, BLOCK_M=128, BLOCK_N=128, BLOCK_K=32, num_warps=8, num_stages=4)\n",
        "    except NotImplementedError as e:\n",
        "        print(f\"[Day3] matmul kernel not implemented yet: {e}\")\n",
        "        return\n",
        "\n",
        "\n",
        "    C_ref_fp16 = A @ B                 # PyTorch fp16 Ë∑ØÂæÑ\n",
        "    C_ref_fp32 = A.float() @ B.float() # Êõ¥‰∏•Ê†º reference\n",
        "\n",
        "    # ‰Ω†ÁöÑËæìÂá∫ C_tri ÊòØ fp16\n",
        "    print(\"err vs torch fp16:\", (C_tri - C_ref_fp16).abs().max().item())\n",
        "    print(\"err vs fp32 ref  :\", (C_tri.float() - C_ref_fp32).abs().max().item())\n",
        "    # correctness\n",
        "    max_err = (C_tri - C_ref).abs().max().item()\n",
        "    print(f\"[Day3] max_abs_err = {max_err:.3e}\")\n",
        "    # You can tighten thresholds after tuning\n",
        "    assert max_err < 7e-2, \"Too large error (fp16). Improve implementation.\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_matmul()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#matmul_skeleton soft_pipeline\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "# ============================================================\n",
        "# Day 3: Tuned MatMul Skeleton (NO SOLUTION)\n",
        "# Goal:\n",
        "#   - Implement a tile-based matmul kernel in Triton\n",
        "#   - Tune BLOCK_M / BLOCK_N / BLOCK_K and num_warps\n",
        "#   - Validate vs torch.matmul\n",
        "#\n",
        "# Notes:\n",
        "#   - Assume A: [M, K], B: [K, N], C: [M, N], fp16 inputs, fp16 output (acc fp32).\n",
        "#   - You may start with fp16 and accumulate in fp32.\n",
        "# ============================================================\n",
        "\n",
        "@triton.jit\n",
        "def matmul_kernel(\n",
        "    a_ptr, b_ptr, c_ptr,\n",
        "    M, N, K,\n",
        "    stride_am, stride_ak,\n",
        "    stride_bk, stride_bn,\n",
        "    stride_cm, stride_cn,\n",
        "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
        "    MAX_K_TILES: tl.constexpr,            # [NEW] meta: static_range upperboundÔºàavoid 1024 compileÔºâ\n",
        "    OUT_DTYPE: tl.constexpr,              # [NEW] meta: 0->fp16, 1->bf16\n",
        "):\n",
        "    # TODO:\n",
        "    # 1) program ids for 2D tiling: pid_m, pid_n\n",
        "    # 2) compute offsets for A tile and B tile\n",
        "    # 3) loop over K tiles:\n",
        "    #    - tl.load A and B tiles with masks\n",
        "    #    - accumulate using tl.dot / manual FMA\n",
        "    # 4) tl.store to C with mask for M,N boundaries\n",
        "    #\n",
        "    # Hints:\n",
        "    #   pid_m = tl.program_id(0)\n",
        "    #   pid_n = tl.program_id(1)\n",
        "    #   offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    #   offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    #   offs_k = tl.arange(0, BLOCK_K)\n",
        "    #\n",
        "    #   use tl.multiple_of / tl.assume if needed (optional)\n",
        "    # raise NotImplementedError(\"TODO: implement matmul_kernel\")\n",
        "\n",
        "    # Compute row/col offsets for this program's C tile\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    # Compute row/col offsets for this program's C tile\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "\n",
        "\n",
        "    m_mask = offs_m < M          # [BM]\n",
        "    n_mask = offs_n < N          # [BN]\n",
        "    #for k0 in range(0, K, BLOCK_K):\n",
        "        #k_offsets = k0 + offs_k          # [BK]\n",
        "    # Software Pipelining\n",
        "    #time ‚Üí\n",
        "    #    iter 0:  load\n",
        "    #    iter 1:       load     compute\n",
        "    #    iter 2:              load     compute\n",
        "    #    iter 3:                     load     compute\n",
        "    k_tiles = tl.cdiv(K, BLOCK_K)\n",
        "    for k_it in tl.static_range(0, MAX_K_TILES):\n",
        "        # if k_it >= k_tiles:\n",
        "        #    break\n",
        "        valid_k_iter = k_it < k_tiles\n",
        "\n",
        "        # k0 = k_it * BLOCK_K\n",
        "        # k_offsets = k0 + offs_k\n",
        "        k_offsets = k_it * BLOCK_K + offs_k\n",
        "\n",
        "        k_mask = k_offsets < K           # [BK]\n",
        "\n",
        "        final_k_mask = valid_k_iter & k_mask   # [BK]\n",
        "\n",
        "        # build pointer grids for this chunk\n",
        "        a_ptrs = a_ptr + offs_m[:, None] * stride_am + k_offsets[None, :] * stride_ak   # [BM, BK]\n",
        "        b_ptrs = b_ptr + k_offsets[:, None] * stride_bk + offs_n[None, :] * stride_bn   # [BK, BN]\n",
        "\n",
        "        # 2D masks for loads\n",
        "        a_load_mask = m_mask[:, None] & final_k_mask[None, :]    # [BM, BK]\n",
        "        b_load_mask = final_k_mask[:, None] & n_mask[None, :]    # [BK, BN]\n",
        "\n",
        "        # masked loads: out-of-bounds => 0\n",
        "        a_tile = tl.load(a_ptrs, mask=a_load_mask, other=0.0)  # [BM, BK], fp16/bf16\n",
        "        b_tile = tl.load(b_ptrs, mask=b_load_mask, other=0.0)  # [BK, BN], fp16/bf16\n",
        "\n",
        "        # accumulate (fp32)\n",
        "        # tl.dot will typically accumulate in fp32 when acc is fp32\n",
        "        acc += tl.dot(a_tile, b_tile)\n",
        "\n",
        "    # c_tile = tl.where(OUT_DTYPE == 1, acc.to(tl.bfloat16), acc.to(tl.float16))\n",
        "    if OUT_DTYPE == 1:\n",
        "        c_tile = acc.to(tl.bfloat16)\n",
        "    else:\n",
        "        c_tile = acc.to(tl.float16)\n",
        "    c_ptrs = c_ptr + offs_m[:,None]*stride_cm + offs_n[None,:]*stride_cn\n",
        "    tl.store(c_ptrs, c_tile, mask=m_mask[:,None] & n_mask[None,:])\n",
        "\n",
        "def triton_matmul(A: torch.Tensor, B: torch.Tensor,\n",
        "                  BLOCK_M=128, BLOCK_N=128, BLOCK_K=32,\n",
        "                  num_warps=8, num_stages=4):\n",
        "    assert A.is_cuda and B.is_cuda\n",
        "    assert A.dtype in (torch.float16, torch.bfloat16)\n",
        "    assert B.dtype in (torch.float16, torch.bfloat16)\n",
        "    assert A.is_contiguous() and B.is_contiguous()\n",
        "    M, K = A.shape\n",
        "    K2, N = B.shape\n",
        "    assert K == K2\n",
        "    assert A.dtype == B.dtype, \"For now, require A and B have same dtype (fp16 or bf16).\"\n",
        "\n",
        "    C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n",
        "\n",
        "    grid = (\n",
        "        triton.cdiv(M, BLOCK_M),\n",
        "        triton.cdiv(N, BLOCK_N),\n",
        "    )\n",
        "\n",
        "    MAX_K_TILES = triton.cdiv(K, BLOCK_K)\n",
        "\n",
        "    OUT_DTYPE = 1 if A.dtype == torch.bfloat16 else 0\n",
        "\n",
        "    matmul_kernel[grid](\n",
        "        A, B, C,\n",
        "        M, N, K,\n",
        "        A.stride(0), A.stride(1),\n",
        "        B.stride(0), B.stride(1),\n",
        "        C.stride(0), C.stride(1),\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n",
        "        MAX_K_TILES=MAX_K_TILES,\n",
        "        OUT_DTYPE=OUT_DTYPE,\n",
        "        num_warps=num_warps,   # Triton launch meta\n",
        "        num_stages=num_stages,\n",
        "\n",
        "    )\n",
        "    return C\n",
        "\n",
        "@torch.no_grad()\n",
        "def check_matmul():\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA not available.\")\n",
        "        return\n",
        "\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    M, K, N = 512, 1024, 768\n",
        "    A = torch.randn((M, K), device=device, dtype=torch.float16)\n",
        "    B = torch.randn((K, N), device=device, dtype=torch.float16)\n",
        "\n",
        "    # PyTorch baseline\n",
        "    C_ref = A @ B\n",
        "\n",
        "    # Triton (will fail until you implement kernel)\n",
        "    try:\n",
        "        C_tri = triton_matmul(A, B, BLOCK_M=128, BLOCK_N=128, BLOCK_K=32, num_warps=8, num_stages=4)\n",
        "    except NotImplementedError as e:\n",
        "        print(f\"[Day3] matmul kernel not implemented yet: {e}\")\n",
        "        return\n",
        "\n",
        "    # correctness\n",
        "    max_err = (C_tri - C_ref).abs().max().item()\n",
        "    print(f\"[Day3] max_abs_err = {max_err:.3e}\")\n",
        "    # You can tighten thresholds after tuning\n",
        "    assert max_err < 5e-2, \"Too large error (fp16). Improve implementation.\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_matmul()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "GHfJSICebzdD",
        "outputId": "f6ad5103-7978-4f27-9125-248ec53b77c6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Day3] max_abs_err = 6.250e-02\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Too large error (fp16). Improve implementation.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3842202535.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0mcheck_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3842202535.py\u001b[0m in \u001b[0;36mcheck_matmul\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Day3] max_abs_err = {max_err:.3e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# You can tighten thresholds after tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mmax_err\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Too large error (fp16). Improve implementation.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Too large error (fp16). Improve implementation."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#matmul_skeleton soft_pipeline\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "# ============================================================\n",
        "# Day 3: Tuned MatMul Skeleton (NO SOLUTION)\n",
        "# Goal:\n",
        "#   - Implement a tile-based matmul kernel in Triton\n",
        "#   - Tune BLOCK_M / BLOCK_N / BLOCK_K and num_warps\n",
        "#   - Validate vs torch.matmul\n",
        "#\n",
        "# Notes:\n",
        "#   - Assume A: [M, K], B: [K, N], C: [M, N], fp16 inputs, fp16 output (acc fp32).\n",
        "#   - You may start with fp16 and accumulate in fp32.\n",
        "# ============================================================\n",
        "\n",
        "@triton.jit\n",
        "def matmul_kernel(\n",
        "    a_ptr, b_ptr, c_ptr,\n",
        "    M, N, K,\n",
        "    stride_am, stride_ak,\n",
        "    stride_bk, stride_bn,\n",
        "    stride_cm, stride_cn,\n",
        "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
        "    MAX_K_TILES: tl.constexpr,               # ‚úÖ NEW: static_range upper bound\n",
        "    OUT_DTYPE: tl.constexpr,                 # ‚úÖ NEW: \"fp16\" or \"bf16\"\n",
        "):\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "    m_mask = offs_m < M\n",
        "    n_mask = offs_n < N\n",
        "\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "\n",
        "    k_tiles = tl.cdiv(K, BLOCK_K)\n",
        "\n",
        "    for k_it in tl.static_range(0, MAX_K_TILES):\n",
        "        valid_k_iter = k_it < k_tiles  # scalar\n",
        "\n",
        "        k_offsets = k_it * BLOCK_K + offs_k\n",
        "        k_mask = k_offsets < K         # [BK]\n",
        "\n",
        "        final_k_mask = k_mask & valid_k_iter  # ‚úÖ broadcast-safe\n",
        "\n",
        "        a_ptrs = a_ptr + offs_m[:, None] * stride_am + k_offsets[None, :] * stride_ak\n",
        "        b_ptrs = b_ptr + k_offsets[:, None] * stride_bk + offs_n[None, :] * stride_bn\n",
        "\n",
        "        a_load_mask = m_mask[:, None] & final_k_mask[None, :]\n",
        "        b_load_mask = final_k_mask[:, None] & n_mask[None, :]\n",
        "\n",
        "        a_tile = tl.load(a_ptrs, mask=a_load_mask, other=0.0)\n",
        "        b_tile = tl.load(b_ptrs, mask=b_load_mask, other=0.0)\n",
        "\n",
        "        acc += tl.dot(a_tile, b_tile)\n",
        "\n",
        "    # ‚úÖ output dtype matches host expectation\n",
        "    if OUT_DTYPE == \"bf16\":\n",
        "        c_tile = acc.to(tl.bfloat16)\n",
        "    else:\n",
        "        c_tile = acc.to(tl.float16)\n",
        "\n",
        "    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n",
        "    tl.store(c_ptrs, c_tile, mask=m_mask[:, None] & n_mask[None, :])\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def triton_matmul(A: torch.Tensor, B: torch.Tensor,\n",
        "                  BLOCK_M=128, BLOCK_N=128, BLOCK_K=32,\n",
        "                  num_warps=8, num_stages=4):\n",
        "    assert A.is_cuda and B.is_cuda\n",
        "    assert A.dtype in (torch.float16, torch.bfloat16)\n",
        "    assert B.dtype in (torch.float16, torch.bfloat16)\n",
        "    assert A.is_contiguous() and B.is_contiguous()\n",
        "    M, K = A.shape\n",
        "    K2, N = B.shape\n",
        "    assert K == K2\n",
        "\n",
        "    C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n",
        "\n",
        "    grid = (\n",
        "        triton.cdiv(M, BLOCK_M),\n",
        "        triton.cdiv(N, BLOCK_N),\n",
        "    )\n",
        "\n",
        "    MAX_K_TILES = triton.cdiv(K, BLOCK_K)\n",
        "    OUT_DTYPE = 1 if A.dtype == torch.bfloat16 else 0\n",
        "\n",
        "    matmul_kernel[grid](\n",
        "        A, B, C,\n",
        "        M, N, K,\n",
        "        A.stride(0), A.stride(1),\n",
        "        B.stride(0), B.stride(1),\n",
        "        C.stride(0), C.stride(1),\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n",
        "        MAX_K_TILES=MAX_K_TILES,                      # [NEW]\n",
        "        OUT_DTYPE=OUT_DTYPE,                          # [NEW]\n",
        "        num_warps=num_warps,\n",
        "        num_stages=num_stages,                        # [NEW]\n",
        "    )\n",
        "\n",
        "\n",
        "    return C\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_matmul()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "25VtdX_ibIfe",
        "outputId": "6d3d2c45-83f5-42a6-f562-d9c16bbef6ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Day3] max_abs_err = 6.250e-02\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Too large error (fp16). Improve implementation.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3041201477.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mcheck_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1171738455.py\u001b[0m in \u001b[0;36mcheck_matmul\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Day3] max_abs_err = {max_err:.3e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m# You can tighten thresholds after tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mmax_err\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Too large error (fp16). Improve implementation.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Too large error (fp16). Improve implementation."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "MlTa4tgFZ5AL",
        "outputId": "aadfa8a2-7a2e-407c-e70e-772109d749e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'A' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1104035120.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mC_ref_fp16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mB\u001b[0m                 \u001b[0;31m# PyTorch fp16 Ë∑ØÂæÑ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mC_ref_fp32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Êõ¥‰∏•Ê†º reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ‰Ω†ÁöÑËæìÂá∫ C_tri ÊòØ fp16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"err vs torch fp16:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mC_tri\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mC_ref_fp16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'A' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "zHP1_JcUWwhv"
      },
      "outputs": [],
      "source": [
        "#softmax_skeleton\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "# ============================================================\n",
        "# Day 4: Numerically Stable Softmax Skeleton (NO SOLUTION)\n",
        "# Goal:\n",
        "#   - Implement row-wise stable softmax: y = exp(x - max) / sum(exp(x - max))\n",
        "#   - Handle any D (not necessarily power of two)\n",
        "#   - Validate vs torch.softmax\n",
        "# ============================================================\n",
        "\n",
        "@triton.jit\n",
        "def softmax_kernel(\n",
        "    x_ptr, y_ptr,\n",
        "    B, D,\n",
        "    stride_xb, stride_xd,\n",
        "    stride_yb, stride_yd,\n",
        "    BLOCK_D: tl.constexpr,\n",
        "):\n",
        "    # TODO:\n",
        "    # 1) pid = tl.program_id(0) for row index\n",
        "    # 2) offsets = tl.arange(0, BLOCK_D)\n",
        "    # 3) load x row block(s) with mask\n",
        "    # 4) compute max over D (may need multiple loads if D > BLOCK_D)\n",
        "    # 5) compute exp(x - max), sum, and normalize\n",
        "    #\n",
        "    # Minimal baseline is \"one program handles one row\", and choose BLOCK_D >= D for first version.\n",
        "    # Then extend to D > BLOCK_D using multiple chunks.\n",
        "    raise NotImplementedError(\"TODO: implement softmax_kernel\")\n",
        "\n",
        "def triton_softmax(x: torch.Tensor, BLOCK_D=1024):\n",
        "    assert x.is_cuda\n",
        "    assert x.dtype == torch.float32  # start with fp32 for stability\n",
        "    assert x.is_contiguous()\n",
        "    B, D = x.shape\n",
        "    y = torch.empty_like(x)\n",
        "    grid = (B,)\n",
        "    softmax_kernel[grid](\n",
        "        x, y,\n",
        "        B, D,\n",
        "        x.stride(0), x.stride(1),\n",
        "        y.stride(0), y.stride(1),\n",
        "        BLOCK_D=BLOCK_D,\n",
        "        num_warps=4,\n",
        "    )\n",
        "    return y\n",
        "\n",
        "@torch.no_grad()\n",
        "def check_softmax():\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA not available.\")\n",
        "        return\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    B, D = 1024, 1000\n",
        "    x = torch.randn((B, D), device=device, dtype=torch.float32)\n",
        "\n",
        "    y_ref = torch.softmax(x, dim=1)\n",
        "\n",
        "    try:\n",
        "        y_tri = triton_softmax(x, BLOCK_D=1024)\n",
        "    except NotImplementedError as e:\n",
        "        print(f\"[Day4] softmax kernel not implemented yet: {e}\")\n",
        "        return\n",
        "\n",
        "    max_err = (y_tri - y_ref).abs().max().item()\n",
        "    mean_err = (y_tri - y_ref).abs().mean().item()\n",
        "    print(f\"[Day4] max_abs_err = {max_err:.3e}, mean_abs_err = {mean_err:.3e}\")\n",
        "    assert max_err < 1e-4, \"Too large error for fp32 softmax.\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_softmax()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "bvgBmpFQWwhw"
      },
      "outputs": [],
      "source": [
        "#layernorm_skeleton\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ============================================================\n",
        "# Day 5: LayerNorm Forward Skeleton (NO SOLUTION)\n",
        "# Goal:\n",
        "#   - Implement LayerNorm forward:\n",
        "#       y = (x - mean) * rsqrt(var + eps) * gamma + beta\n",
        "#   - Validate vs torch.nn.functional.layer_norm\n",
        "# ============================================================\n",
        "\n",
        "@triton.jit\n",
        "def layernorm_fwd_kernel(\n",
        "    x_ptr, gamma_ptr, beta_ptr, y_ptr,\n",
        "    B, D,\n",
        "    stride_xb, stride_xd,\n",
        "    stride_yb, stride_yd,\n",
        "    eps,\n",
        "    BLOCK_D: tl.constexpr,\n",
        "):\n",
        "    # TODO:\n",
        "    # 1) pid = tl.program_id(0) => row\n",
        "    # 2) load x row (possibly in chunks if D > BLOCK_D)\n",
        "    # 3) compute mean and var\n",
        "    # 4) normalize + affine\n",
        "    # 5) store y\n",
        "    raise NotImplementedError(\"TODO: implement layernorm_fwd_kernel\")\n",
        "\n",
        "def triton_layernorm(x: torch.Tensor, gamma: torch.Tensor, beta: torch.Tensor,\n",
        "                     eps=1e-5, BLOCK_D=1024):\n",
        "    assert x.is_cuda and gamma.is_cuda and beta.is_cuda\n",
        "    assert x.dtype == torch.float32 and gamma.dtype == torch.float32 and beta.dtype == torch.float32\n",
        "    assert x.is_contiguous() and gamma.is_contiguous() and beta.is_contiguous()\n",
        "    B, D = x.shape\n",
        "    y = torch.empty_like(x)\n",
        "    grid = (B,)\n",
        "    layernorm_fwd_kernel[grid](\n",
        "        x, gamma, beta, y,\n",
        "        B, D,\n",
        "        x.stride(0), x.stride(1),\n",
        "        y.stride(0), y.stride(1),\n",
        "        eps,\n",
        "        BLOCK_D=BLOCK_D,\n",
        "        num_warps=4,\n",
        "    )\n",
        "    return y\n",
        "\n",
        "@torch.no_grad()\n",
        "def check_layernorm():\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA not available.\")\n",
        "        return\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    B, D = 4096, 1024\n",
        "    x = torch.randn((B, D), device=device, dtype=torch.float32)\n",
        "    gamma = torch.randn((D,), device=device, dtype=torch.float32)\n",
        "    beta = torch.randn((D,), device=device, dtype=torch.float32)\n",
        "    eps = 1e-5\n",
        "\n",
        "    y_ref = F.layer_norm(x, (D,), gamma, beta, eps=eps)\n",
        "\n",
        "    try:\n",
        "        y_tri = triton_layernorm(x, gamma, beta, eps=eps, BLOCK_D=1024)\n",
        "    except NotImplementedError as e:\n",
        "        print(f\"[Day5] layernorm kernel not implemented yet: {e}\")\n",
        "        return\n",
        "\n",
        "    max_err = (y_tri - y_ref).abs().max().item()\n",
        "    mean_err = (y_tri - y_ref).abs().mean().item()\n",
        "    print(f\"[Day5] max_abs_err = {max_err:.3e}, mean_abs_err = {mean_err:.3e}\")\n",
        "    assert max_err < 2e-4, \"Too large error for fp32 layernorm.\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_layernorm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "FLVSC7ZuWwhw"
      },
      "outputs": [],
      "source": [
        "#benchmark_skeleton\n",
        "import torch\n",
        "import time\n",
        "import importlib\n",
        "\n",
        "# ============================================================\n",
        "# Day 6: Benchmark Triton vs PyTorch for Day3/4/5\n",
        "# - If kernel not implemented, it will skip and print a message.\n",
        "# - Uses CUDA events for timing.\n",
        "# ============================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def bench_ms(fn, iters=200, warmup=50):\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "    s = torch.cuda.Event(enable_timing=True)\n",
        "    e = torch.cuda.Event(enable_timing=True)\n",
        "    s.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    e.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return s.elapsed_time(e) / iters\n",
        "\n",
        "def print_table(rows):\n",
        "    header = f\"{'Op':<10} | {'Impl':<8} | {'ms/iter':>10} | {'speedup':>8} | {'note':<20}\"\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "    for r in rows:\n",
        "        print(f\"{r['op']:<10} | {r['impl']:<8} | {r['ms']:>10.4f} | {r['speedup']:>8.2f} | {r['note']:<20}\")\n",
        "\n",
        "def main():\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA not available.\")\n",
        "        return\n",
        "\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    # -------------------------\n",
        "    # Day3 MatMul\n",
        "    # -------------------------\n",
        "    try:\n",
        "        day3 = importlib.import_module(\"day3_matmul_skeleton\")\n",
        "        M, K, N = 1024, 1024, 1024\n",
        "        A = torch.randn((M, K), device=device, dtype=torch.float16)\n",
        "        B = torch.randn((K, N), device=device, dtype=torch.float16)\n",
        "\n",
        "        torch_fn = lambda: A @ B\n",
        "        torch_ms = bench_ms(torch_fn, iters=100, warmup=20)\n",
        "\n",
        "        def triton_fn():\n",
        "            return day3.triton_matmul(A, B, BLOCK_M=128, BLOCK_N=128, BLOCK_K=32, num_warps=8)\n",
        "\n",
        "        try:\n",
        "            tri_ms = bench_ms(triton_fn, iters=100, warmup=20)\n",
        "            rows.append({\"op\":\"matmul\", \"impl\":\"torch\", \"ms\":torch_ms, \"speedup\":1.0, \"note\":\"torch.matmul\"})\n",
        "            rows.append({\"op\":\"matmul\", \"impl\":\"triton\", \"ms\":tri_ms, \"speedup\":torch_ms/tri_ms, \"note\":\"BM=128 BN=128\"})\n",
        "        except NotImplementedError as e:\n",
        "            rows.append({\"op\":\"matmul\", \"impl\":\"triton\", \"ms\":float(\"nan\"), \"speedup\":float(\"nan\"), \"note\":\"TODO kernel\"})\n",
        "    except Exception as e:\n",
        "        rows.append({\"op\":\"matmul\", \"impl\":\"triton\", \"ms\":float(\"nan\"), \"speedup\":float(\"nan\"), \"note\":f\"import fail: {e}\"})\n",
        "\n",
        "\n",
        "    # -------------------------\n",
        "    # Day4 Softmax\n",
        "    # -------------------------\n",
        "    try:\n",
        "        day4 = importlib.import_module(\"day4_softmax_skeleton\")\n",
        "        Bsz, D = 4096, 1024\n",
        "        x = torch.randn((Bsz, D), device=device, dtype=torch.float32)\n",
        "\n",
        "        torch_fn = lambda: torch.softmax(x, dim=1)\n",
        "        torch_ms = bench_ms(torch_fn)\n",
        "\n",
        "        def triton_fn():\n",
        "            return day4.triton_softmax(x, BLOCK_D=1024)\n",
        "\n",
        "        try:\n",
        "            tri_ms = bench_ms(triton_fn)\n",
        "            rows.append({\"op\":\"softmax\", \"impl\":\"torch\", \"ms\":torch_ms, \"speedup\":1.0, \"note\":\"torch.softmax\"})\n",
        "            rows.append({\"op\":\"softmax\", \"impl\":\"triton\", \"ms\":tri_ms, \"speedup\":torch_ms/tri_ms, \"note\":\"BLOCK_D=1024\"})\n",
        "        except NotImplementedError as e:\n",
        "            rows.append({\"op\":\"softmax\", \"impl\":\"triton\", \"ms\":float(\"nan\"), \"speedup\":float(\"nan\"), \"note\":\"TODO kernel\"})\n",
        "    except Exception as e:\n",
        "        rows.append({\"op\":\"softmax\", \"impl\":\"triton\", \"ms\":float(\"nan\"), \"speedup\":float(\"nan\"), \"note\":f\"import fail: {e}\"})\n",
        "\n",
        "\n",
        "    # -------------------------\n",
        "    # Day5 LayerNorm\n",
        "    # -------------------------\n",
        "    try:\n",
        "        day5 = importlib.import_module(\"day5_layernorm_skeleton\")\n",
        "        Bsz, D = 4096, 1024\n",
        "        x = torch.randn((Bsz, D), device=device, dtype=torch.float32)\n",
        "        gamma = torch.randn((D,), device=device, dtype=torch.float32)\n",
        "        beta = torch.randn((D,), device=device, dtype=torch.float32)\n",
        "        eps = 1e-5\n",
        "\n",
        "        torch_fn = lambda: torch.nn.functional.layer_norm(x, (D,), gamma, beta, eps=eps)\n",
        "        torch_ms = bench_ms(torch_fn)\n",
        "\n",
        "        def triton_fn():\n",
        "            return day5.triton_layernorm(x, gamma, beta, eps=eps, BLOCK_D=1024)\n",
        "\n",
        "        try:\n",
        "            tri_ms = bench_ms(triton_fn)\n",
        "            rows.append({\"op\":\"layernorm\", \"impl\":\"torch\", \"ms\":torch_ms, \"speedup\":1.0, \"note\":\"F.layer_norm\"})\n",
        "            rows.append({\"op\":\"layernorm\", \"impl\":\"triton\", \"ms\":tri_ms, \"speedup\":torch_ms/tri_ms, \"note\":\"BLOCK_D=1024\"})\n",
        "        except NotImplementedError as e:\n",
        "            rows.append({\"op\":\"layernorm\", \"impl\":\"triton\", \"ms\":float(\"nan\"), \"speedup\":float(\"nan\"), \"note\":\"TODO kernel\"})\n",
        "    except Exception as e:\n",
        "        rows.append({\"op\":\"layernorm\", \"impl\":\"triton\", \"ms\":float(\"nan\"), \"speedup\":float(\"nan\"), \"note\":f\"import fail: {e}\"})\n",
        "\n",
        "\n",
        "    print()\n",
        "    print_table(rows)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}