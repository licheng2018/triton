{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs_bXqDJWwhs"
      },
      "source": [
        "## Triton Core Kernels & Benchmarking\n",
        "\n",
        "üéØ **Weekly Goal**  \n",
        "Implement and tune three core ML kernels (MatMul, Softmax, LayerNorm) using Triton,  \n",
        "develop intuition for **block sizes, `num_warps`, numerical stability, and kernel fusion**,  \n",
        "and perform systematic benchmarking against PyTorch to understand real performance trade-offs.\n",
        "\n",
        "---\n",
        "\n",
        "## Tuned MatMul (Block Size / `num_warps`)\n",
        "\n",
        "### Objective\n",
        "Implement a tile-based Triton GEMM kernel and tune **`BLOCK_M / BLOCK_N / BLOCK_K`**\n",
        "and **`num_warps`** to study their impact on throughput, occupancy, and register pressure.\n",
        "\n",
        "### Tasks\n",
        "- [ ] Implement a basic Triton matmul kernel (tile-based)\n",
        "- [ ] Define `BLOCK_M / BLOCK_N / BLOCK_K` as `tl.constexpr`\n",
        "- [ ] Evaluate multiple tile configurations (e.g., 64√ó64√ó32, 128√ó128√ó32)\n",
        "- [ ] Sweep `num_warps ‚àà {4, 8}` and compare performance\n",
        "- [ ] Compare against `torch.matmul` as a baseline\n",
        "\n",
        "### Key Concepts\n",
        "- **Tiling**: each program computes one output tile\n",
        "- **Block size** controls arithmetic intensity (FLOPs / byte)\n",
        "- **`num_warps`** trades off parallelism vs. register usage\n",
        "- GEMM is typically **compute-bound**, unlike vector add\n",
        "\n",
        "### Deliverables\n",
        "- Runnable `triton_matmul.py`\n",
        "- Performance table (ms / TFLOPS) for different configurations\n",
        "- Short analysis identifying the best configuration and why\n",
        "\n",
        "---\n",
        "\n",
        "## Triton Softmax (Numerically Stable)\n",
        "\n",
        "### Objective\n",
        "Implement a numerically stable row-wise softmax kernel in Triton,\n",
        "focusing on **max-subtraction**, reduction patterns, and block/warp mapping.\n",
        "\n",
        "### Tasks\n",
        "- [ ] Implement row-wise softmax in Triton\n",
        "- [ ] Apply `x - max(x)` for numerical stability\n",
        "- [ ] Decompose into two stages:\n",
        "  - max reduction\n",
        "  - exp + sum reduction\n",
        "- [ ] Correctly handle arbitrary feature dimensions (non-power-of-two)\n",
        "- [ ] Compare against `torch.softmax`\n",
        "\n",
        "### Key Concepts\n",
        "- **Numerical stability** for exponential operations\n",
        "- **Reduction patterns** within a program\n",
        "- Softmax is often **memory-bound with reductions**\n",
        "- Triton enables explicit control over reduction structure\n",
        "\n",
        "### Deliverables\n",
        "- `triton_softmax.py`\n",
        "- Correctness check vs. PyTorch (max / mean error)\n",
        "- Performance comparison table (ms / GB/s)\n",
        "\n",
        "---\n",
        "\n",
        "## Triton LayerNorm\n",
        "\n",
        "### Objective\n",
        "Implement Triton LayerNorm (forward pass) and understand\n",
        "**mean/variance reductions**, `eps` stabilization, and the performance benefits of kernel fusion.\n",
        "\n",
        "### Tasks\n",
        "- [ ] Implement LayerNorm forward in Triton\n",
        "- [ ] Compute per-row mean and variance\n",
        "- [ ] Apply `rsqrt(var + eps)`\n",
        "- [ ] Support affine parameters (`gamma`, `beta`)\n",
        "- [ ] Compare against `torch.nn.functional.layer_norm`\n",
        "\n",
        "### Key Concepts\n",
        "- Two reductions: mean ‚Üí variance\n",
        "- **Kernel fusion**: normalization + affine in a single kernel\n",
        "- LayerNorm is typically **memory-bound with reductions**\n",
        "- Triton avoids intermediate tensor materialization\n",
        "\n",
        "### Deliverables\n",
        "- `triton_layernorm.py`\n",
        "- Correctness validation (max / mean error)\n",
        "- Triton vs. PyTorch performance comparison\n",
        "\n",
        "---\n",
        "\n",
        "## Benchmark: Triton vs PyTorch\n",
        "\n",
        "### Objective\n",
        "Systematically benchmark Triton kernels against PyTorch eager kernels\n",
        "to identify **when Triton wins, when it does not, and why**.\n",
        "\n",
        "### Tasks\n",
        "- [ ] Build a unified benchmark framework (CUDA events)\n",
        "- [ ] Compare the following operators:\n",
        "  - vector add\n",
        "  - fused add + ReLU\n",
        "  - softmax\n",
        "  - layernorm\n",
        "- [ ] Record:\n",
        "  - latency (ms)\n",
        "  - effective bandwidth / FLOPs\n",
        "  - speedup\n",
        "- [ ] Repeat experiments across different tensor sizes\n",
        "\n",
        "### Key Concepts\n",
        "- **Bandwidth-bound vs. compute-bound** kernels\n",
        "- Kernel launch overhead\n",
        "- Real benefits of operator fusion\n",
        "- Why Triton excels at fused kernels rather than single primitive ops\n",
        "\n",
        "### Deliverables\n",
        "- `benchmark_triton_vs_torch.py`\n",
        "- Unified comparison table (Markdown / CSV)\n",
        "- Summary covering:\n",
        "  - which kernels benefit most from Triton\n",
        "  - which PyTorch kernels are already near-optimal\n",
        "  - implications for ML systems optimization\n",
        "\n",
        "---\n",
        "\n",
        "## End-of-Week Takeaways\n",
        "\n",
        "- Triton is **not** ‚Äúa faster PyTorch‚Äù\n",
        "- Triton enables **CUDA-level kernel design in Python**\n",
        "- The real performance gains come from:\n",
        "  - kernel fusion\n",
        "  - explicit reduction control\n",
        "  - tile-aware kernel design\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#matmul_skeleton\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "# ============================================================\n",
        "# Day 3: Tuned MatMul Skeleton (NO SOLUTION)\n",
        "# Goal:\n",
        "#   - Implement a tile-based matmul kernel in Triton\n",
        "#   - Tune BLOCK_M / BLOCK_N / BLOCK_K and num_warps\n",
        "#   - Validate vs torch.matmul\n",
        "#\n",
        "# Notes:\n",
        "#   - Assume A: [M, K], B: [K, N], C: [M, N], fp16 inputs, fp16 output (acc fp32).\n",
        "#   - You may start with fp16 and accumulate in fp32.\n",
        "# ============================================================\n",
        "\n",
        "@triton.jit\n",
        "def matmul_kernel(\n",
        "    a_ptr, b_ptr, c_ptr,\n",
        "    M, N, K,\n",
        "    stride_am, stride_ak,\n",
        "    stride_bk, stride_bn,\n",
        "    stride_cm, stride_cn,\n",
        "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
        "):\n",
        "    # TODO:\n",
        "    # 1) program ids for 2D tiling: pid_m, pid_n\n",
        "    # 2) compute offsets for A tile and B tile\n",
        "    # 3) loop over K tiles:\n",
        "    #    - tl.load A and B tiles with masks\n",
        "    #    - accumulate using tl.dot / manual FMA\n",
        "    # 4) tl.store to C with mask for M,N boundaries\n",
        "    #\n",
        "    # Hints:\n",
        "    #   pid_m = tl.program_id(0)\n",
        "    #   pid_n = tl.program_id(1)\n",
        "    #   offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    #   offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    #   offs_k = tl.arange(0, BLOCK_K)\n",
        "    #\n",
        "    #   use tl.multiple_of / tl.assume if needed (optional)\n",
        "    # raise NotImplementedError(\"TODO: implement matmul_kernel\")\n",
        "\n",
        "    # Compute row/col offsets for this program's C tile\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    # Compute row/col offsets for this program's C tile\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "\n",
        "\n",
        "    m_mask = offs_m < M          # [BM]\n",
        "    n_mask = offs_n < N          # [BN]\n",
        "\n",
        "    for k0 in range(0, K, BLOCK_K):\n",
        "        # current K indices for this chunk\n",
        "        k_offsets = k0 + offs_k          # [BK]\n",
        "        k_mask = k_offsets < K           # [BK]\n",
        "\n",
        "        # build pointer grids for this chunk\n",
        "        a_ptrs = a_ptr + offs_m[:, None] * stride_am + k_offsets[None, :] * stride_ak   # [BM, BK]\n",
        "        b_ptrs = b_ptr + k_offsets[:, None] * stride_bk + offs_n[None, :] * stride_bn   # [BK, BN]\n",
        "\n",
        "        # 2D masks for loads\n",
        "        a_load_mask = m_mask[:, None] & k_mask[None, :]    # [BM, BK]\n",
        "        b_load_mask = k_mask[:, None] & n_mask[None, :]    # [BK, BN]\n",
        "\n",
        "        # masked loads: out-of-bounds => 0\n",
        "        a_tile = tl.load(a_ptrs, mask=a_load_mask, other=0.0)  # [BM, BK], fp16/bf16\n",
        "        b_tile = tl.load(b_ptrs, mask=b_load_mask, other=0.0)  # [BK, BN], fp16/bf16\n",
        "\n",
        "        # accumulate (fp32)\n",
        "        # tl.dot will typically accumulate in fp32 when acc is fp32\n",
        "        acc += tl.dot(a_tile, b_tile)\n",
        "\n",
        "    c_tile = acc.to(tl.float16)\n",
        "    c_ptrs = c_ptr + offs_m[:,None]*stride_cm + offs_n[None,:]*stride_cn\n",
        "    tl.store(c_ptrs, c_tile, mask=m_mask[:,None] & n_mask[None,:])\n",
        "\n",
        "def triton_matmul(A: torch.Tensor, B: torch.Tensor,\n",
        "                  BLOCK_M=128, BLOCK_N=128, BLOCK_K=32,\n",
        "                  num_warps=8):\n",
        "    assert A.is_cuda and B.is_cuda\n",
        "    assert A.dtype in (torch.float16, torch.bfloat16)\n",
        "    assert B.dtype in (torch.float16, torch.bfloat16)\n",
        "    assert A.is_contiguous() and B.is_contiguous()\n",
        "    M, K = A.shape\n",
        "    K2, N = B.shape\n",
        "    assert K == K2\n",
        "\n",
        "    C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n",
        "\n",
        "    grid = (\n",
        "        triton.cdiv(M, BLOCK_M),\n",
        "        triton.cdiv(N, BLOCK_N),\n",
        "    )\n",
        "\n",
        "    matmul_kernel[grid](\n",
        "        A, B, C,\n",
        "        M, N, K,\n",
        "        A.stride(0), A.stride(1),\n",
        "        B.stride(0), B.stride(1),\n",
        "        C.stride(0), C.stride(1),\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n",
        "        num_warps=num_warps,   # Triton launch meta\n",
        "    )\n",
        "    return C\n",
        "\n",
        "@torch.no_grad()\n",
        "def check_matmul():\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA not available.\")\n",
        "        return\n",
        "\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    M, K, N = 512, 1024, 768\n",
        "    A = torch.randn((M, K), device=device, dtype=torch.float16)\n",
        "    B = torch.randn((K, N), device=device, dtype=torch.float16)\n",
        "\n",
        "    # PyTorch baseline\n",
        "    C_ref = A @ B\n",
        "\n",
        "\n",
        "    # Triton (will fail until you implement kernel)\n",
        "    try:\n",
        "        C_tri = triton_matmul(A, B, BLOCK_M=128, BLOCK_N=128, BLOCK_K=32, num_warps=8)\n",
        "    except NotImplementedError as e:\n",
        "        print(f\"[Day3] matmul kernel not implemented yet: {e}\")\n",
        "        return\n",
        "\n",
        "\n",
        "    C_ref_fp16 = A @ B                 # PyTorch fp16 Ë∑ØÂæÑ\n",
        "    C_ref_fp32 = A.float() @ B.float() # Êõ¥‰∏•Ê†º reference\n",
        "\n",
        "    # ‰Ω†ÁöÑËæìÂá∫ C_tri ÊòØ fp16\n",
        "    print(\"err vs torch fp16:\", (C_tri - C_ref_fp16).abs().max().item())\n",
        "    print(\"err vs fp32 ref  :\", (C_tri.float() - C_ref_fp32).abs().max().item())\n",
        "\n",
        "    # correctness\n",
        "    max_err = (C_tri - C_ref).abs().max().item()\n",
        "    print(f\"[Day3] max_abs_err = {max_err:.3e}\")\n",
        "    # You can tighten thresholds after tuning\n",
        "    assert max_err < 7e-2, \"Too large error (fp16). Improve implementation.\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_matmul()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvND3OxsZRtf",
        "outputId": "bdedb454-bcc6-4ee4-fcd3-f60e3df27fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "err vs torch fp16: 0.0625\n",
            "err vs fp32 ref  : 0.0601959228515625\n",
            "[Day3] max_abs_err = 6.250e-02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "D0leVK_gWwhu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcfef703-7b27-4c73-e853-6671203dcbaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "err vs torch fp16: 0.0625\n",
            "err vs fp32 ref  : 0.0601959228515625\n",
            "[Day3] max_abs_err = 6.250e-02\n"
          ]
        }
      ],
      "source": [
        "#matmul_skeleton soft_pipeline\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "# ============================================================\n",
        "# Day 3: Tuned MatMul Skeleton (NO SOLUTION)\n",
        "# Goal:\n",
        "#   - Implement a tile-based matmul kernel in Triton\n",
        "#   - Tune BLOCK_M / BLOCK_N / BLOCK_K and num_warps\n",
        "#   - Validate vs torch.matmul\n",
        "#\n",
        "# Notes:\n",
        "#   - Assume A: [M, K], B: [K, N], C: [M, N], fp16 inputs, fp16 output (acc fp32).\n",
        "#   - You may start with fp16 and accumulate in fp32.\n",
        "# ============================================================\n",
        "\n",
        "@triton.jit\n",
        "def matmul_kernel(\n",
        "    a_ptr, b_ptr, c_ptr,\n",
        "    M, N, K,\n",
        "    stride_am, stride_ak,\n",
        "    stride_bk, stride_bn,\n",
        "    stride_cm, stride_cn,\n",
        "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
        "    MAX_K_TILES: tl.constexpr,            # [NEW] meta: static_range upperboundÔºàavoid 1024 compileÔºâ\n",
        "    OUT_DTYPE: tl.constexpr,              # [NEW] meta: 0->fp16, 1->bf16\n",
        "):\n",
        "    # TODO:\n",
        "    # 1) program ids for 2D tiling: pid_m, pid_n\n",
        "    # 2) compute offsets for A tile and B tile\n",
        "    # 3) loop over K tiles:\n",
        "    #    - tl.load A and B tiles with masks\n",
        "    #    - accumulate using tl.dot / manual FMA\n",
        "    # 4) tl.store to C with mask for M,N boundaries\n",
        "    #\n",
        "    # Hints:\n",
        "    #   pid_m = tl.program_id(0)\n",
        "    #   pid_n = tl.program_id(1)\n",
        "    #   offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    #   offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    #   offs_k = tl.arange(0, BLOCK_K)\n",
        "    #\n",
        "    #   use tl.multiple_of / tl.assume if needed (optional)\n",
        "    # raise NotImplementedError(\"TODO: implement matmul_kernel\")\n",
        "\n",
        "    # Compute row/col offsets for this program's C tile\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    # Compute row/col offsets for this program's C tile\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
        "\n",
        "\n",
        "    m_mask = offs_m < M          # [BM]\n",
        "    n_mask = offs_n < N          # [BN]\n",
        "    #for k0 in range(0, K, BLOCK_K):\n",
        "        #k_offsets = k0 + offs_k          # [BK]\n",
        "    # Software Pipelining\n",
        "    #time ‚Üí\n",
        "    #    iter 0:  load\n",
        "    #    iter 1:       load     compute\n",
        "    #    iter 2:              load     compute\n",
        "    #    iter 3:                     load     compute\n",
        "    k_tiles = tl.cdiv(K, BLOCK_K)\n",
        "    for k_it in tl.static_range(0, MAX_K_TILES):\n",
        "        # if k_it >= k_tiles:\n",
        "        #    break\n",
        "        valid_k_iter = k_it < k_tiles\n",
        "\n",
        "        # k0 = k_it * BLOCK_K\n",
        "        # k_offsets = k0 + offs_k\n",
        "        k_offsets = k_it * BLOCK_K + offs_k\n",
        "\n",
        "        k_mask = k_offsets < K           # [BK]\n",
        "\n",
        "        final_k_mask = valid_k_iter & k_mask   # [BK]\n",
        "\n",
        "        # build pointer grids for this chunk\n",
        "        a_ptrs = a_ptr + offs_m[:, None] * stride_am + k_offsets[None, :] * stride_ak   # [BM, BK]\n",
        "        b_ptrs = b_ptr + k_offsets[:, None] * stride_bk + offs_n[None, :] * stride_bn   # [BK, BN]\n",
        "\n",
        "        # 2D masks for loads\n",
        "        a_load_mask = m_mask[:, None] & final_k_mask[None, :]    # [BM, BK]\n",
        "        b_load_mask = final_k_mask[:, None] & n_mask[None, :]    # [BK, BN]\n",
        "\n",
        "        # masked loads: out-of-bounds => 0\n",
        "        a_tile = tl.load(a_ptrs, mask=a_load_mask, other=0.0)  # [BM, BK], fp16/bf16\n",
        "        b_tile = tl.load(b_ptrs, mask=b_load_mask, other=0.0)  # [BK, BN], fp16/bf16\n",
        "\n",
        "        # accumulate (fp32)\n",
        "        # tl.dot will typically accumulate in fp32 when acc is fp32\n",
        "        acc += tl.dot(a_tile, b_tile)\n",
        "\n",
        "    c_tile = tl.where(OUT_DTYPE == 1, acc.to(tl.bfloat16), acc.to(tl.float16))\n",
        "    c_ptrs = c_ptr + offs_m[:,None]*stride_cm + offs_n[None,:]*stride_cn\n",
        "    tl.store(c_ptrs, c_tile, mask=m_mask[:,None] & n_mask[None,:])\n",
        "\n",
        "def triton_matmul(A: torch.Tensor, B: torch.Tensor,\n",
        "                  BLOCK_M=128, BLOCK_N=128, BLOCK_K=32,\n",
        "                  num_warps=8, num_stages=4):\n",
        "    assert A.is_cuda and B.is_cuda\n",
        "    assert A.dtype in (torch.float16, torch.bfloat16)\n",
        "    assert B.dtype in (torch.float16, torch.bfloat16)\n",
        "    assert A.is_contiguous() and B.is_contiguous()\n",
        "    M, K = A.shape\n",
        "    K2, N = B.shape\n",
        "    assert K == K2\n",
        "    assert A.dtype == B.dtype, \"For now, require A and B have same dtype (fp16 or bf16).\"\n",
        "\n",
        "    C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n",
        "\n",
        "    grid = (\n",
        "        triton.cdiv(M, BLOCK_M),\n",
        "        triton.cdiv(N, BLOCK_N),\n",
        "    )\n",
        "\n",
        "    MAX_K_TILES = triton.cdiv(K, BLOCK_K)\n",
        "\n",
        "    OUT_DTYPE = 1 if A.dtype == torch.bfloat16 else 0\n",
        "\n",
        "    matmul_kernel[grid](\n",
        "        A, B, C,\n",
        "        M, N, K,\n",
        "        A.stride(0), A.stride(1),\n",
        "        B.stride(0), B.stride(1),\n",
        "        C.stride(0), C.stride(1),\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n",
        "        MAX_K_TILES=MAX_K_TILES,\n",
        "        OUT_DTYPE=OUT_DTYPE,\n",
        "        num_warps=num_warps,   # Triton launch meta\n",
        "        num_stages=num_stages,\n",
        "\n",
        "    )\n",
        "    return C\n",
        "\n",
        "@torch.no_grad()\n",
        "def check_matmul():\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA not available.\")\n",
        "        return\n",
        "\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    M, K, N = 512, 1024, 768\n",
        "    A = torch.randn((M, K), device=device, dtype=torch.float16)\n",
        "    B = torch.randn((K, N), device=device, dtype=torch.float16)\n",
        "\n",
        "    # PyTorch baseline\n",
        "    C_ref = A @ B\n",
        "\n",
        "    # Triton (will fail until you implement kernel)\n",
        "    try:\n",
        "        C_tri = triton_matmul(A, B, BLOCK_M=128, BLOCK_N=128, BLOCK_K=32, num_warps=8, num_stages=4)\n",
        "    except NotImplementedError as e:\n",
        "        print(f\"[Day3] matmul kernel not implemented yet: {e}\")\n",
        "        return\n",
        "\n",
        "\n",
        "    C_ref_fp16 = A @ B                 # PyTorch fp16 Ë∑ØÂæÑ\n",
        "    C_ref_fp32 = A.float() @ B.float() # Êõ¥‰∏•Ê†º reference\n",
        "\n",
        "    # ‰Ω†ÁöÑËæìÂá∫ C_tri ÊòØ fp16\n",
        "    print(\"err vs torch fp16:\", (C_tri - C_ref_fp16).abs().max().item())\n",
        "    print(\"err vs fp32 ref  :\", (C_tri.float() - C_ref_fp32).abs().max().item())\n",
        "    # correctness\n",
        "    max_err = (C_tri - C_ref).abs().max().item()\n",
        "    print(f\"[Day3] max_abs_err = {max_err:.3e}\")\n",
        "    # You can tighten thresholds after tuning\n",
        "    assert max_err < 7e-2, \"Too large error (fp16). Improve implementation.\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_matmul()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "zHP1_JcUWwhv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342c0510-2db0-4f76-ee37-c144379feec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPU: Tesla T4\n",
            "Shape: B=4096, D=2048, dtype=torch.float32\n",
            "====================================================================================================\n",
            "Kernel     | warps | stages |       ms |       GB/s |    max_err\n",
            "----------------------------------------------------------------------------------------------------\n",
            "A          |     2 |      1 |   0.2892 |     232.03 |  5.588e-09\n",
            "A          |     2 |      2 |   0.2894 |     231.86 |  5.588e-09\n",
            "A          |     2 |      3 |   0.2894 |     231.91 |  5.588e-09\n",
            "A          |     4 |      1 |   0.2834 |     236.84 |  3.725e-09\n",
            "A          |     4 |      2 |   0.2833 |     236.91 |  3.725e-09\n",
            "A          |     4 |      3 |   0.2832 |     236.94 |  3.725e-09\n",
            "A          |     8 |      1 |   0.2825 |     237.58 |  5.588e-09\n",
            "A          |     8 |      2 |   0.2835 |     236.69 |  5.588e-09\n",
            "A          |     8 |      3 |   0.2835 |     236.71 |  5.588e-09\n",
            "B          |     2 |      1 |   0.2958 |     453.82 |  4.602e-02\n",
            "B          |     2 |      2 |   0.2955 |     454.22 |  4.602e-02\n",
            "B          |     2 |      3 |   0.2957 |     453.85 |  4.602e-02\n",
            "B          |     4 |      1 |   0.2922 |     459.34 |  4.602e-02\n",
            "B          |     4 |      2 |   0.2919 |     459.84 |  4.602e-02\n",
            "B          |     4 |      3 |   0.2924 |     459.06 |  4.602e-02\n",
            "B          |     8 |      1 |   0.2862 |     468.92 |  4.602e-02\n",
            "B          |     8 |      2 |   0.2867 |     468.11 |  4.602e-02\n",
            "B          |     8 |      3 |   0.2861 |     469.17 |  4.602e-02\n",
            "====================================================================================================\n",
            "Notes:\n",
            " - Kernel A performs 1 read + 1 write per element.\n",
            " - Kernel B performs 3 reads + 1 write per element.\n",
            " - num_warps affects parallelism and register pressure.\n",
            " - num_stages affects pipelining of memory loads.\n",
            " - For large D, kernel B scales better.\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "# ============================================================\n",
        "# Day 4: Numerically Stable Softmax Skeleton (NO SOLUTION)\n",
        "# Goal:\n",
        "#   - Implement row-wise stable softmax: y = exp(x - max) / sum(exp(x - max))\n",
        "#   - Handle any D (not necessarily power of two)\n",
        "#   - Validate vs torch.softmax\n",
        "# ============================================================\n",
        "    # TODO:\n",
        "    # 1) pid = tl.program_id(0) for row index\n",
        "    # 2) offsets = tl.arange(0, BLOCK_D)\n",
        "    # 3) load x row block(s) with mask\n",
        "    # 4) compute max over D (may need multiple loads if D > BLOCK_D)\n",
        "    # 5) compute exp(x - max), sum, and normalize\n",
        "    #\n",
        "    # Minimal baseline is \"one program handles one row\", and choose BLOCK_D >= D for first version.\n",
        "    # Then extend to D > BLOCK_D using multiple chunks.\n",
        "# ============================================================\n",
        "# Kernel A: One program handles one row\n",
        "# Requirement: BLOCK_D >= D\n",
        "# Single-pass: read once, write once\n",
        "# ============================================================\n",
        "\n",
        "@triton.jit\n",
        "def softmax_kernel_A(\n",
        "    x_ptr, y_ptr,\n",
        "    B, D,\n",
        "    stride_xb, stride_xd,\n",
        "    stride_yb, stride_yd,\n",
        "    BLOCK_D: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(0)  # row index\n",
        "    offs = tl.arange(0, BLOCK_D)\n",
        "\n",
        "    x_row_ptr = x_ptr + pid * stride_xb + offs * stride_xd\n",
        "    y_row_ptr = y_ptr + pid * stride_yb + offs * stride_yd\n",
        "\n",
        "    mask = offs < D\n",
        "\n",
        "    x = tl.load(x_row_ptr, mask=mask, other=-float(\"inf\"))\n",
        "    x_max = tl.max(x, axis=0)\n",
        "    ex = tl.exp(x - x_max)\n",
        "    denom = tl.sum(ex, axis=0)\n",
        "    y = ex / denom\n",
        "\n",
        "    tl.store(y_row_ptr, y, mask=mask)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Kernel B: Chunked version (supports D > BLOCK_D)\n",
        "# 3 passes over the row\n",
        "# D must be constexpr\n",
        "# ============================================================\n",
        "\n",
        "@triton.jit\n",
        "def softmax_kernel_B(\n",
        "    x_ptr, y_ptr,\n",
        "    B,\n",
        "    stride_xb, stride_xd,\n",
        "    stride_yb, stride_yd,\n",
        "    D: tl.constexpr,\n",
        "    BLOCK_D: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(0)\n",
        "    offs = tl.arange(0, BLOCK_D)\n",
        "\n",
        "    # Pass 1: compute row max\n",
        "    row_max = -float(\"inf\")\n",
        "    for start in tl.static_range(0, D, BLOCK_D):\n",
        "        cols = start + offs\n",
        "        mask = cols < D\n",
        "        x = tl.load(x_ptr + pid * stride_xb + cols * stride_xd,\n",
        "                    mask=mask, other=-float(\"inf\"))\n",
        "        row_max = tl.maximum(row_max, tl.max(x, axis=0))\n",
        "\n",
        "    # Pass 2: compute denominator\n",
        "    denom = 0.0\n",
        "    for start in tl.static_range(0, D, BLOCK_D):\n",
        "        cols = start + offs\n",
        "        mask = cols < D\n",
        "        x = tl.load(x_ptr + pid * stride_xb + cols * stride_xd,\n",
        "                    mask=mask, other=-float(\"inf\"))\n",
        "        denom += tl.sum(tl.exp(x - row_max), axis=0)\n",
        "\n",
        "    # Pass 3: write output\n",
        "    for start in tl.static_range(0, D, BLOCK_D):\n",
        "        cols = start + offs\n",
        "        mask = cols < D\n",
        "        x = tl.load(x_ptr + pid * stride_xb + cols * stride_xd,\n",
        "                    mask=mask, other=-float(\"inf\"))\n",
        "        y = tl.exp(x - row_max) / denom\n",
        "        tl.store(y_ptr + pid * stride_yb + cols * stride_yd,\n",
        "                 y, mask=mask)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Benchmark helpers\n",
        "# ============================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def bench(fn, iters=200, warmup=50):\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    start.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    return start.elapsed_time(end) / iters  # ms/iter\n",
        "\n",
        "\n",
        "def max_abs_err(a, b):\n",
        "    return float((a - b).abs().max().item())\n",
        "\n",
        "\n",
        "def compute_gbps(bytes_processed, ms):\n",
        "    seconds = ms / 1000.0\n",
        "    return bytes_processed / seconds / 1e9\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Main experiment\n",
        "# ============================================================\n",
        "\n",
        "def main():\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    device = \"cuda\"\n",
        "    dtype = torch.float32\n",
        "    elem_size = torch.tensor([], dtype=dtype).element_size()\n",
        "\n",
        "    B, D = 4096, 2048\n",
        "    x = torch.randn((B, D), device=device, dtype=dtype)\n",
        "    y_ref = torch.softmax(x, dim=1)\n",
        "\n",
        "    print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Shape: B={B}, D={D}, dtype={dtype}\")\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "    warps_list = [2, 4, 8]\n",
        "    stages_list = [1, 2, 3]\n",
        "\n",
        "    print(f\"{'Kernel':<10} | {'warps':>5} | {'stages':>6} | {'ms':>8} | {'GB/s':>10} | {'max_err':>10}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    # ------------------------\n",
        "    # Kernel A\n",
        "    # ------------------------\n",
        "    BLOCK_D_A = triton.next_power_of_2(D)\n",
        "\n",
        "    for w in warps_list:\n",
        "        for s in stages_list:\n",
        "\n",
        "            def run_A():\n",
        "                y = torch.empty_like(x)\n",
        "                grid = (B,)\n",
        "                softmax_kernel_A[grid](\n",
        "                    x, y,\n",
        "                    B, D,\n",
        "                    x.stride(0), x.stride(1),\n",
        "                    y.stride(0), y.stride(1),\n",
        "                    BLOCK_D=BLOCK_D_A,\n",
        "                    num_warps=w,\n",
        "                    num_stages=s,\n",
        "                )\n",
        "                return y\n",
        "\n",
        "            yA = run_A()\n",
        "            err = max_abs_err(yA, y_ref)\n",
        "\n",
        "            ms = bench(run_A)\n",
        "\n",
        "            bytes_processed = B * D * elem_size * 2  # 1 read + 1 write\n",
        "            gbps = compute_gbps(bytes_processed, ms)\n",
        "\n",
        "            print(f\"{'A':<10} | {w:5d} | {s:6d} | {ms:8.4f} | {gbps:10.2f} | {err:10.3e}\")\n",
        "\n",
        "    # ------------------------\n",
        "    # Kernel B\n",
        "    # ------------------------\n",
        "    BLOCK_D_B = 1024\n",
        "\n",
        "    for w in warps_list:\n",
        "        for s in stages_list:\n",
        "\n",
        "            def run_B():\n",
        "                y = torch.empty_like(x)\n",
        "                grid = (B,)\n",
        "                softmax_kernel_B[grid](\n",
        "                    x, y,\n",
        "                    B,\n",
        "                    x.stride(0), x.stride(1),\n",
        "                    y.stride(0), y.stride(1),\n",
        "                    D=D,\n",
        "                    BLOCK_D=BLOCK_D_B,\n",
        "                    num_warps=w,\n",
        "                    num_stages=s,\n",
        "                )\n",
        "                return y\n",
        "\n",
        "            yB = run_B()\n",
        "            err = max_abs_err(yB, y_ref)\n",
        "\n",
        "            ms = bench(run_B)\n",
        "\n",
        "            bytes_processed = B * D * elem_size * 4  # 3 reads + 1 write\n",
        "            gbps = compute_gbps(bytes_processed, ms)\n",
        "\n",
        "            print(f\"{'B':<10} | {w:5d} | {s:6d} | {ms:8.4f} | {gbps:10.2f} | {err:10.3e}\")\n",
        "\n",
        "    print(\"=\" * 100)\n",
        "    print(\"Notes:\")\n",
        "    print(\" - Kernel A performs 1 read + 1 write per element.\")\n",
        "    print(\" - Kernel B performs 3 reads + 1 write per element.\")\n",
        "    print(\" - num_warps affects parallelism and register pressure.\")\n",
        "    print(\" - num_stages affects pipelining of memory loads.\")\n",
        "    print(\" - For large D, kernel B scales better.\")\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "bvgBmpFQWwhw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dc7a1ff-968f-45af-c2b8-7337dcf2543e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Day5] max_abs_err = 1.907e-06, mean_abs_err = 1.651e-08\n"
          ]
        }
      ],
      "source": [
        "#layernorm_skeleton\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ============================================================\n",
        "# Day 5: LayerNorm Forward Skeleton (NO SOLUTION)\n",
        "# Goal:\n",
        "#   - Implement LayerNorm forward:\n",
        "#       y = (x - mean) * rsqrt(var + eps) * gamma + beta\n",
        "#   - Validate vs torch.nn.functional.layer_norm\n",
        "# ============================================================\n",
        "\n",
        "@triton.jit\n",
        "def layernorm_fwd_kernel(\n",
        "    x_ptr, gamma_ptr, beta_ptr, y_ptr,\n",
        "    B, D,\n",
        "    stride_xb, stride_xd,\n",
        "    stride_yb, stride_yd,\n",
        "    eps,\n",
        "    BLOCK_D: tl.constexpr,\n",
        "):\n",
        "    # TODO:\n",
        "    # 1) pid = tl.program_id(0) => row\n",
        "    # 2) load x row (possibly in chunks if D > BLOCK_D)\n",
        "    # 3) compute mean and var\n",
        "    # 4) normalize + affine\n",
        "    # 5) store y\n",
        "    # raise NotImplementedError(\"TODO: implement layernorm_fwd_kernel\")\n",
        "    pid = tl.program_id(0)\n",
        "    offs = tl.arange(0, BLOCK_D)\n",
        "    mask = offs < D\n",
        "\n",
        "    x_row_ptr = x_ptr + pid * stride_xb + offs * stride_xd\n",
        "    x = tl.load(x_row_ptr, mask = mask, other = 0.0)\n",
        "\n",
        "    mu = tl.sum(x, axis=0) / D\n",
        "    diff = x - mu\n",
        "    var = tl.sum(diff * diff, axis = 0) / D\n",
        "    inv_std = tl.rsqrt(var + eps)\n",
        "\n",
        "    gamma = tl.load(gamma_ptr + offs, mask=mask, other=1.0)\n",
        "    beta  = tl.load(beta_ptr  + offs, mask=mask, other=0.0)\n",
        "\n",
        "    y = diff * inv_std * gamma + beta\n",
        "\n",
        "    y_row_ptr = y_ptr + pid * stride_yb + offs * stride_yd\n",
        "\n",
        "    tl.store(y_row_ptr, y, mask=mask)\n",
        "\n",
        "\n",
        "def triton_layernorm(x: torch.Tensor, gamma: torch.Tensor, beta: torch.Tensor,\n",
        "                     eps=1e-5, BLOCK_D=1024):\n",
        "    assert x.is_cuda and gamma.is_cuda and beta.is_cuda\n",
        "    assert x.dtype == torch.float32 and gamma.dtype == torch.float32 and beta.dtype == torch.float32\n",
        "    assert x.is_contiguous() and gamma.is_contiguous() and beta.is_contiguous()\n",
        "    B, D = x.shape\n",
        "    y = torch.empty_like(x)\n",
        "    grid = (B,)\n",
        "    layernorm_fwd_kernel[grid](\n",
        "        x, gamma, beta, y,\n",
        "        B, D,\n",
        "        x.stride(0), x.stride(1),\n",
        "        y.stride(0), y.stride(1),\n",
        "        eps,\n",
        "        BLOCK_D=BLOCK_D,\n",
        "        num_warps=4,\n",
        "    )\n",
        "    return y\n",
        "\n",
        "@torch.no_grad()\n",
        "def check_layernorm():\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA not available.\")\n",
        "        return\n",
        "    device = \"cuda\"\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    B, D = 4096, 1024\n",
        "    x = torch.randn((B, D), device=device, dtype=torch.float32)\n",
        "    gamma = torch.randn((D,), device=device, dtype=torch.float32)\n",
        "    beta = torch.randn((D,), device=device, dtype=torch.float32)\n",
        "    eps = 1e-5\n",
        "\n",
        "    y_ref = F.layer_norm(x, (D,), gamma, beta, eps=eps)\n",
        "\n",
        "    try:\n",
        "        y_tri = triton_layernorm(x, gamma, beta, eps=eps, BLOCK_D=1024)\n",
        "    except NotImplementedError as e:\n",
        "        print(f\"[Day5] layernorm kernel not implemented yet: {e}\")\n",
        "        return\n",
        "\n",
        "    max_err = (y_tri - y_ref).abs().max().item()\n",
        "    mean_err = (y_tri - y_ref).abs().mean().item()\n",
        "    print(f\"[Day5] max_abs_err = {max_err:.3e}, mean_abs_err = {mean_err:.3e}\")\n",
        "    assert max_err < 2e-4, \"Too large error for fp32 layernorm.\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_layernorm()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "# ============================================================\n",
        "# Day 5: LayerNorm Forward Skeleton (NO SOLUTION)\n",
        "# Goal:\n",
        "#   - Implement LayerNorm forward:\n",
        "#       y = (x - mean) * rsqrt(var + eps) * gamma + beta\n",
        "#   - Validate vs torch.nn.functional.layer_norm\n",
        "# ============================================================\n",
        "\n",
        "# ============================================================\n",
        "# Config\n",
        "# ============================================================\n",
        "\n",
        "EPS = 1e-5\n",
        "ITERS = 200\n",
        "WARMUP = 50\n",
        "\n",
        "Ds = [1024, 2048, 4096]\n",
        "BLOCK_D_LIST = [256, 512, 1024]\n",
        "WARPS_LIST = [2, 4, 8]\n",
        "STAGES_LIST = [1, 2, 3, 4]\n",
        "\n",
        "# Toggle this after you implement the chunked kernel\n",
        "CHUNK_IMPLEMENTED = False\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Baseline LayerNorm (single-chunk): requires BLOCK_D >= D\n",
        "# - One program handles one row.\n",
        "# - Loads exactly BLOCK_D elements and reduces over them.\n",
        "# - Correct ONLY if D <= BLOCK_D (and mask used).\n",
        "# ============================================================\n",
        "\n",
        "@triton.jit\n",
        "def layernorm_fwd_baseline(\n",
        "    x_ptr, gamma_ptr, beta_ptr, y_ptr,\n",
        "    B, D,\n",
        "    stride_xb, stride_xd,\n",
        "    stride_yb, stride_yd,\n",
        "    eps,\n",
        "    BLOCK_D: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(0)  # row index\n",
        "    offs = tl.arange(0, BLOCK_D)\n",
        "\n",
        "    mask = offs < D\n",
        "\n",
        "    x = tl.load(x_ptr + pid * stride_xb + offs * stride_xd, mask=mask, other=0.0)\n",
        "    # NOTE: If x is fp16/bf16, consider casting to fp32 for better numerics:\n",
        "    x_f = x.to(tl.float32)\n",
        "\n",
        "    # Mean/variance (baseline style)\n",
        "    mu = tl.sum(x_f, axis=0) / D\n",
        "    diff = x_f - mu\n",
        "    var = tl.sum(diff * diff, axis=0) / D\n",
        "    inv_std = tl.rsqrt(var + eps)\n",
        "\n",
        "    gamma = tl.load(gamma_ptr + offs, mask=mask, other=1.0).to(tl.float32)\n",
        "    beta  = tl.load(beta_ptr  + offs, mask=mask, other=0.0).to(tl.float32)\n",
        "\n",
        "    y = diff * inv_std * gamma + beta\n",
        "\n",
        "    tl.store(y_ptr + pid * stride_yb + offs * stride_yd, y, mask=mask)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Chunked LayerNorm (TODO): supports D > BLOCK_D\n",
        "# Requirements:\n",
        "# - One program handles one row\n",
        "# - Process the row in chunks of BLOCK_D\n",
        "# - Compute mean/var across full D\n",
        "# - Normalize + affine + store\n",
        "#\n",
        "# Leave blank for you to fill in.\n",
        "# Suggestion:\n",
        "# - Use 2-pass over X: pass1 sum/sumsq, pass2 normalize+store\n",
        "# - Use fp32 accum\n",
        "# ============================================================\n",
        "\n",
        "@triton.jit\n",
        "def layernorm_fwd_chunked_TODO(\n",
        "    x_ptr, gamma_ptr, beta_ptr, y_ptr,\n",
        "    B,\n",
        "    stride_xb, stride_xd,\n",
        "    stride_yb, stride_yd,\n",
        "    eps: tl.constexpr,\n",
        "    D: tl.constexpr,\n",
        "    BLOCK_D: tl.constexpr,\n",
        "):\n",
        "    # TODO: implement chunked layernorm forward\n",
        "    # - pid = tl.program_id(0)\n",
        "    # - offs = tl.arange(0, BLOCK_D)\n",
        "    # - for start in tl.static_range(0, D, BLOCK_D):\n",
        "    #     load chunk, accumulate sum/sumsq in fp32\n",
        "    # - compute mean/var/inv_std\n",
        "    # - second loop over chunks: load x + gamma/beta, normalize+affine, store y\n",
        "    #\n",
        "    # NOTE: Triton kernel cannot \"raise\" nicely; just leave it empty and set\n",
        "    # CHUNK_IMPLEMENTED=False until you fill it.\n",
        "    # pass\n",
        "    pid = tl.program_id(0)\n",
        "    offs = tl.arange(0, BLOCK_D)\n",
        "\n",
        "    # Pass 1: sum/sumsq\n",
        "    sum_x = -float(\"inf\")\n",
        "    sum_sq = -float(\"inf\")\n",
        "\n",
        "    for start in tl.static_range(0, D, BLOCK_D):\n",
        "        cols = start + offs\n",
        "        mask = cols < D\n",
        "\n",
        "        x = tl.load(x_ptr + pid * stride_xb + cols * stride_xd, mask=mask, other=-float(\"inf\"))\n",
        "        x_f32 = x.to(tl.float32)\n",
        "        sum_x += tl.sum(x_f32, axis = 0)\n",
        "        sum_sq += tl.sum(x_f32 * x_f32, axis = 0)\n",
        "\n",
        "        mu = sum_x / D\n",
        "        diff = x_f32 - mu\n",
        "        var = tl.sum(diff * diff, axis=0) / D\n",
        "        inv_std = tl.rsqrt(var + eps)\n",
        "\n",
        "\n",
        "    # Pass2: normalize+store\n",
        "    for start in tl.static_range(0,D, BLOCK_D):\n",
        "        cols = start + offs\n",
        "        mask = cols < D\n",
        "\n",
        "        x = tl.load(x_ptr + pid * stride_xb + cols * stride_xd, mask=mask, other=0.0).to(tl.float32)\n",
        "        gamma = tl.load(gamma_ptr + cols, mask=mask, other=1.0).to(tl.float32)\n",
        "        beta  = tl.load(beta_ptr  + cols, mask=mask, other=0.0).to(tl.float32)\n",
        "\n",
        "        y = (x - mu) * inv_std * gamma + beta\n",
        "\n",
        "    # store\n",
        "    tl.store(\n",
        "        y_ptr + pid * stride_yb + cols * stride_yd,\n",
        "        y,\n",
        "        mask=mask\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Helpers\n",
        "# ============================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def bench_ms(fn, iters=ITERS, warmup=WARMUP):\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    start.record()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    return start.elapsed_time(end) / iters  # ms/iter\n",
        "\n",
        "\n",
        "def max_abs_err(a, b):\n",
        "    return float((a - b).abs().max().item())\n",
        "\n",
        "\n",
        "def gbps(bytes_processed, ms):\n",
        "    # GB/s = bytes / sec / 1e9\n",
        "    return bytes_processed / (ms / 1000.0) / 1e9\n",
        "\n",
        "\n",
        "def bytes_layernorm_baseline(B, D, elem_size):\n",
        "    # Baseline: read X once + read gamma once + read beta once + write Y once\n",
        "    # Approx bytes:\n",
        "    #   X read:      B*D*elem\n",
        "    #   gamma read:  B*D*elem  (each row reads gamma; gamma is reused but from DRAM viewpoint this is an upper bound)\n",
        "    #   beta read:   B*D*elem\n",
        "    #   Y write:     B*D*elem\n",
        "    # Total upper bound: 4 * B*D*elem\n",
        "    #\n",
        "    # If gamma/beta are cached well, real traffic is lower; still useful as a rough metric.\n",
        "    return 4 * B * D * elem_size\n",
        "\n",
        "\n",
        "def bytes_layernorm_chunked_TODO(B, D, elem_size):\n",
        "    # TODO: Fill this based on your implementation.\n",
        "    # Example for 2-pass over X + 1 extra pass over Y would differ from 3-pass.\n",
        "    # Return an approximate upper bound.\n",
        "    # return None\n",
        "\n",
        "    return 5 * B * D * elem_size\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Runner wrappers\n",
        "# ============================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_baseline(x, gamma, beta, block_d, num_warps, num_stages):\n",
        "    B, D = x.shape\n",
        "    y = torch.empty_like(x, dtype=torch.float32)  # baseline writes fp32 here\n",
        "    grid = (B,)\n",
        "    layernorm_fwd_baseline[grid](\n",
        "        x, gamma, beta, y,\n",
        "        B, D,\n",
        "        x.stride(0), x.stride(1),\n",
        "        y.stride(0), y.stride(1),\n",
        "        EPS,\n",
        "        BLOCK_D=block_d,\n",
        "        num_warps=num_warps,\n",
        "        num_stages=num_stages,\n",
        "    )\n",
        "    return y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_chunked_TODO(x, gamma, beta, block_d, num_warps, num_stages):\n",
        "    # D is constexpr in the chunked kernel, so we pass D as tl.constexpr argument.\n",
        "    B, D = x.shape\n",
        "    y = torch.empty_like(x, dtype=torch.float32)\n",
        "    grid = (B,)\n",
        "    layernorm_fwd_chunked_TODO[grid](\n",
        "        x, gamma, beta, y,\n",
        "        B,\n",
        "        x.stride(0), x.stride(1),\n",
        "        y.stride(0), y.stride(1),\n",
        "        eps=EPS,\n",
        "        D=D,\n",
        "        BLOCK_D=block_d,\n",
        "        num_warps=num_warps,\n",
        "        num_stages=num_stages,\n",
        "    )\n",
        "    return y\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Main sweep\n",
        "# ============================================================\n",
        "\n",
        "def main():\n",
        "    torch.manual_seed(0)\n",
        "    assert torch.cuda.is_available()\n",
        "    device = \"cuda\"\n",
        "\n",
        "    # Choose dtype for input; LN is numerically sensitive.\n",
        "    # Start with fp16/bf16 if you want realistic workloads, but fp32 is easiest to debug.\n",
        "    in_dtype = torch.float16\n",
        "\n",
        "    print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Input dtype: {in_dtype}\")\n",
        "    print(f\"EPS: {EPS}\")\n",
        "    print(\"=\" * 140)\n",
        "\n",
        "    for D in Ds:\n",
        "        # Use a reasonable B to avoid too much compilation overhead\n",
        "        B = 4096 if D <= 2048 else 2048\n",
        "\n",
        "        x = torch.randn((B, D), device=device, dtype=in_dtype)\n",
        "        gamma = torch.randn((D,), device=device, dtype=in_dtype)\n",
        "        beta = torch.randn((D,), device=device, dtype=in_dtype)\n",
        "\n",
        "        # Torch reference (compute in fp32 by default internally for some ops, but keep it as reference)\n",
        "        y_ref = torch.nn.functional.layer_norm(x, (D,), gamma, beta, EPS).to(torch.float32)\n",
        "\n",
        "        elem_size = x.element_size()\n",
        "\n",
        "        print(f\"\\n=== Sweep for D={D}, B={B} ===\")\n",
        "        print(f\"{'impl':<10} | {'BLOCK':>5} | {'warps':>5} | {'stages':>6} | {'ms':>9} | {'GB/s':>10} | {'max_err':>10} | note\")\n",
        "        print(\"-\" * 140)\n",
        "\n",
        "        for block_d in BLOCK_D_LIST:\n",
        "            for w in WARPS_LIST:\n",
        "                for s in STAGES_LIST:\n",
        "                    # -------------------------\n",
        "                    # Baseline: only valid when block_d >= D\n",
        "                    # -------------------------\n",
        "                    if block_d >= D:\n",
        "                        def fnA():\n",
        "                            return run_baseline(x, gamma, beta, block_d, w, s)\n",
        "\n",
        "                        yA = fnA()\n",
        "                        errA = max_abs_err(yA, y_ref)\n",
        "                        msA = bench_ms(fnA)\n",
        "\n",
        "                        bytesA = bytes_layernorm_baseline(B, D, elem_size)\n",
        "                        gbA = gbps(bytesA, msA)\n",
        "\n",
        "                        print(f\"{'baseline':<10} | {block_d:5d} | {w:5d} | {s:6d} | {msA:9.4f} | {gbA:10.2f} | {errA:10.3e} | valid\")\n",
        "                    else:\n",
        "                        print(f\"{'baseline':<10} | {block_d:5d} | {w:5d} | {s:6d} | {'-':>9} | {'-':>10} | {'-':>10} | requires BLOCK>=D\")\n",
        "\n",
        "                    # -------------------------\n",
        "                    # Chunked: should be valid for all D, but left TODO\n",
        "                    # -------------------------\n",
        "                    if CHUNK_IMPLEMENTED:\n",
        "                        def fnC():\n",
        "                            return run_chunked_TODO(x, gamma, beta, block_d, w, s)\n",
        "\n",
        "                        yC = fnC()\n",
        "                        errC = max_abs_err(yC, y_ref)\n",
        "                        msC = bench_ms(fnC)\n",
        "\n",
        "                        bytesC = bytes_layernorm_chunked_TODO(B, D, elem_size)\n",
        "                        if bytesC is None:\n",
        "                            gbC = float(\"nan\")\n",
        "                            note = \"fill bytes estimate\"\n",
        "                        else:\n",
        "                            gbC = gbps(bytesC, msC)\n",
        "                            note = \"ok\"\n",
        "\n",
        "                        print(f\"{'chunk':<10} | {block_d:5d} | {w:5d} | {s:6d} | {msC:9.4f} | {gbC:10.2f} | {errC:10.3e} | {note}\")\n",
        "                    else:\n",
        "                        print(f\"{'chunk':<10} | {block_d:5d} | {w:5d} | {s:6d} | {'-':>9} | {'-':>10} | {'-':>10} | TODO: implement\")\n",
        "\n",
        "        print(\"-\" * 140)\n",
        "\n",
        "    print(\"\\nDone.\")\n",
        "    print()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxqVPMjWia-Y",
        "outputId": "dfbbd963-bdb4-4bea-8016-b9bcf8e8cd37"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPU: Tesla T4\n",
            "Input dtype: torch.float16\n",
            "EPS: 1e-05\n",
            "============================================================================================================================================\n",
            "\n",
            "=== Sweep for D=1024, B=4096 ===\n",
            "impl       | BLOCK | warps | stages |        ms |       GB/s |    max_err | note\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "baseline   |   256 |     2 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     2 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     2 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     2 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     2 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     2 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     2 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     2 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     4 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     4 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     4 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     4 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     4 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     4 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     4 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     4 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     8 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     8 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     8 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     8 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     8 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     8 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     8 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     8 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     2 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     2 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     2 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     2 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     2 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     2 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     2 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     2 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     4 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     4 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     4 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     4 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     4 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     4 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     4 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     4 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     8 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     8 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     8 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     8 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     8 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     8 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     8 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     8 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     2 |      1 |    0.1173 |     285.99 |  3.898e-03 | valid\n",
            "chunk      |  1024 |     2 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     2 |      2 |    0.1172 |     286.21 |  3.898e-03 | valid\n",
            "chunk      |  1024 |     2 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     2 |      3 |    0.1171 |     286.51 |  3.898e-03 | valid\n",
            "chunk      |  1024 |     2 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     2 |      4 |    0.1172 |     286.27 |  3.898e-03 | valid\n",
            "chunk      |  1024 |     2 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     4 |      1 |    0.1139 |     294.62 |  3.898e-03 | valid\n",
            "chunk      |  1024 |     4 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     4 |      2 |    0.1137 |     295.23 |  3.898e-03 | valid\n",
            "chunk      |  1024 |     4 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     4 |      3 |    0.1139 |     294.65 |  3.898e-03 | valid\n",
            "chunk      |  1024 |     4 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     4 |      4 |    0.1138 |     294.93 |  3.898e-03 | valid\n",
            "chunk      |  1024 |     4 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     8 |      1 |    0.1123 |     298.81 |  3.898e-03 | valid\n",
            "chunk      |  1024 |     8 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     8 |      2 |    0.1123 |     298.89 |  3.898e-03 | valid\n",
            "chunk      |  1024 |     8 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     8 |      3 |    0.1124 |     298.60 |  3.898e-03 | valid\n",
            "chunk      |  1024 |     8 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     8 |      4 |    0.1123 |     298.67 |  3.898e-03 | valid\n",
            "chunk      |  1024 |     8 |      4 |         - |          - |          - | TODO: implement\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "=== Sweep for D=2048, B=4096 ===\n",
            "impl       | BLOCK | warps | stages |        ms |       GB/s |    max_err | note\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "baseline   |   256 |     2 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     2 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     2 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     2 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     2 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     2 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     2 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     2 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     4 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     4 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     4 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     4 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     4 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     4 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     4 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     4 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     8 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     8 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     8 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     8 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     8 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     8 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     8 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     8 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     2 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     2 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     2 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     2 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     2 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     2 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     2 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     2 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     4 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     4 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     4 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     4 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     4 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     4 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     4 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     4 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     8 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     8 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     8 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     8 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     8 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     8 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     8 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     8 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     2 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     2 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     2 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     2 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     2 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     2 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     2 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     2 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     4 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     4 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     4 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     4 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     4 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     4 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     4 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     4 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     8 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     8 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     8 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     8 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     8 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     8 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     8 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     8 |      4 |         - |          - |          - | TODO: implement\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "=== Sweep for D=4096, B=2048 ===\n",
            "impl       | BLOCK | warps | stages |        ms |       GB/s |    max_err | note\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "baseline   |   256 |     2 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     2 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     2 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     2 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     2 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     2 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     2 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     2 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     4 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     4 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     4 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     4 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     4 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     4 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     4 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     4 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     8 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     8 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     8 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     8 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     8 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     8 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |   256 |     8 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   256 |     8 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     2 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     2 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     2 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     2 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     2 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     2 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     2 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     2 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     4 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     4 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     4 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     4 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     4 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     4 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     4 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     4 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     8 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     8 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     8 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     8 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     8 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     8 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |   512 |     8 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |   512 |     8 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     2 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     2 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     2 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     2 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     2 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     2 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     2 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     2 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     4 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     4 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     4 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     4 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     4 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     4 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     4 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     4 |      4 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     8 |      1 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     8 |      1 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     8 |      2 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     8 |      2 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     8 |      3 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     8 |      3 |         - |          - |          - | TODO: implement\n",
            "baseline   |  1024 |     8 |      4 |         - |          - |          - | requires BLOCK>=D\n",
            "chunk      |  1024 |     8 |      4 |         - |          - |          - | TODO: implement\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Done.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}